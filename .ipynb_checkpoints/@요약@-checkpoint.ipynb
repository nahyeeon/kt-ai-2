{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ec11f1-d725-4f69-9547-6169fc46267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜„ì¥ì—ì„œ ë§ì´ì”€(ì„±ëŠ¥ì´ ì¢‹ê¸°ë•Œë¬¸) => RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6586a9b-b735-473d-b739-6811b33bedc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params \n",
    "KNN { 'n_neighbors' : range(3,51,2), 'metric' : ['euclidean', 'manhattan']  }\n",
    "DecisionTree { 'max_depth':range(2,11), 'min_samples_leaf':range(10,101,10) }\n",
    "RandomForest { 'n_estimators':[20,50,100], 'max_features':range(1,21) }\n",
    "XGBoost {'learning_rate' : np.linspace(0.01,0.2, 20), 'n_estimators':range(60,200,20), 'max_depth':[3,4,5,6]}\n",
    "SVM { 'C' : np.linspace(0.01, 100, 50), 'gamma':[0.001,0.01,.1,1] }\n",
    "\n",
    "GridSearchCV(m, params, cv=5, scoring = 'neg_mean_absolute_error')\n",
    "XGBRegressor(objective = 'reg:squarederror')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21635a-7ffc-458a-8c3f-1153adb17a03",
   "metadata": {},
   "source": [
    "## ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7900e-4296-4361-976c-e65a7c03245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™” í•œê¸€í°íŠ¸ ì„¤ì •\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font=\"NanumGothicCoding\", \n",
    "        rc={\"axes.unicode_minus\":False}, # ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ê¹¨ì§ í˜„ìƒ í•´ê²°\n",
    "        style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b2fbff-8574-435a-8be6-f587fc7a189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¶ˆëŸ¬ì˜¤ì.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ì „ì²˜ë¦¬\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ëª¨ë¸ë§\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import * \n",
    "\n",
    "# ë¹„ì§€ë„ í•™ìŠµ\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c3a6c-841d-4042-bb6e-c7c3520e15f7",
   "metadata": {},
   "source": [
    "## target ë³€ìˆ˜ ì¡°íšŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31aedfe-394b-4ab8-8096-622dad5d30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Creditability'].value_counts())\n",
    "print(data['Creditability'].value_counts()/ data.shape[0])\n",
    "\n",
    "data['Creditability'].value_counts().plot(kind = 'barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcedc36-a3d4-431e-b28b-d4bf36b11b95",
   "metadata": {},
   "source": [
    "## ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f296f100-1f25-458a-9625-b687d7da503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label'] = submission_pred\n",
    "submission\n",
    "\n",
    "submission['label']= submission['label'].replace([0,1],['ham','spam'])\n",
    "\n",
    "submission.to_csv(\"/aihub/data/M2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb77912-4f8d-40f8-850f-56bd72498178",
   "metadata": {},
   "source": [
    "## to_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196b1549-3337-4561-af05-ef18ca0390d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['ê¸°ì¤€ì¼ID'] = df_total['ê¸°ì¤€ì¼ID'].astype('str')\n",
    "\n",
    "air_201['time'] = pd.to_datetime(air_201['time'],format='%Y%m%d%H')\n",
    "air_202['time'] = pd.to_datetime(air_202['time'],format='%Y%m%d%H')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d154d6a-7f80-40dc-aa6e-c437c8391566",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3cdeb5-e440-42eb-9823-186597581100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ë¶„í• 1\n",
    "target = 'Sales'\n",
    "x = data.drop(target, axis=1)\n",
    "y = data.loc[:, target]\n",
    "\n",
    "# ê°€ë³€ìˆ˜í™”\n",
    "dumm_cols = ['ShelveLoc','Education','Urban', 'US']\n",
    "x = pd.get_dummies(x, columns = dumm_cols, drop_first = True)\n",
    "\n",
    "# ë°ì´í„° ë¶„í• 2\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=.2\n",
    "                                                  , random_state = 2022)\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§\n",
    "scaler = MinMaxScaler()\n",
    "x_train_s = scaler.fit_transform(x_train)\n",
    "x_val_s = scaler.transform(x_val)\n",
    "\n",
    "x_train_s = pd.DataFrame(x_train_s, columns=list(x))\n",
    "x_val_s = pd.DataFrame(x_val_s, columns=list(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4c264-ab90-4350-90f1-4e1db257837e",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4aff41-5048-451a-9a9a-73a9fa73a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost\n",
    "!pip install category_encoders\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffca176-b38f-4b10-bb0a-7cdfc4200586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ëª¨ë¸\n",
    "cat_model= CatBoostRegressor()\n",
    "cat_model.fit(train_x, train_y)\n",
    "cat_predict = cat_model.predict(test_x)\n",
    "\n",
    "print(mean_squared_error(test_y, cat_predict, squared=False))\n",
    "print(r2_score(test_y, cat_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c54b6b1-7b82-4168-a091-cba953a17f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì í™”\n",
    "CB = CatBoostRegressor(depth=4,bagging_temperature=2.099,learning_rate=0.02091,subsample=0.2325)\n",
    "CB.fit(train_x, train_y)\n",
    "CB_pred = CB.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1c352-db6f-4b27-9158-fd4a72209927",
   "metadata": {},
   "source": [
    "* íŠœë‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e94a4e-4d3a-4be7-b029-aa082c46ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bayesian-optimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def fit_model(train_x,train_y,test_x,depth,bagging_temperature,learning_rate,subsample):\n",
    "    cat_model = CatBoostRegressor(depth = int(depth),\n",
    "                                  bagging_temperature=bagging_temperature,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  random_state=1339,#\n",
    "                                  verbose=0,#\n",
    "                                  subsample=subsample\n",
    "                                  ).fit(train_x,train_y)\n",
    "    cat_predict = cat_model.predict(test_x)\n",
    "    return cat_predict\n",
    "\n",
    "def CAT_cv(depth,bagging_temperature,learning_rate,subsample):\n",
    "\n",
    "    cat_predict = fit_model(np.array(train_x),np.array(train_y),np.array(test_x),depth,bagging_temperature,learning_rate,subsample)\n",
    "    score = r2_score(test_y,cat_predict)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b0d12-697d-4f0c-a639-4225ff466eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = { 'depth': (3, 15),\n",
    "            'bagging_temperature': (1, 10),\n",
    "            'learning_rate': (0.01, 1),\n",
    "            'subsample' : (0.01,1),\n",
    "            }\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "bo = BayesianOptimization(f = CAT_cv, pbounds = pbounds, random_state = 42,verbose = 2)\n",
    "bo.maximize(init_points = 5, n_iter = 10,acq = 'ei',xi = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736578d2-2961-4060-806f-b5d9e2571627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble model\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "cat1 = CatBoostRegressor(depth=5,bagging_temperature=2.909,learning_rate=0.01,subsample=0.7559)\n",
    "cat2 = CatBoostRegressor(depth=4,bagging_temperature=2.308,learning_rate=0.01,subsample=1)\n",
    "cat3 = CatBoostRegressor(depth=4,bagging_temperature=2.099,learning_rate=0.02091,subsample=0.2325)\n",
    "\n",
    "\n",
    "eclf = VotingRegressor(estimators=[\n",
    "         ('cat1', cat1), ('cat2', cat2),('cat3', cat3)])\n",
    "\n",
    "eclf.fit(train_x, train_y)\n",
    "eclf_predict = eclf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060557b2-7651-4219-8c9d-7f9003118188",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(mean_squared_error(test_y,eclf_predict)))\n",
    "print(r2_score(test_y,eclf_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64255320-fd98-4f20-b828-d87efa0f4db4",
   "metadata": {},
   "source": [
    "## Fitting Graph ->Elbow Method [ëª¨ë¸ê³¼ì í•©]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ec031-30e5-4a94-8c13-8a7602d610b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "* ê°€ì¥ ë‹¨ìˆœí•œ ëª¨ë¸(í‰ê· ëª¨ë¸)\n",
    "** knn : kë¥¼ ìµœëŒ€ë¡œ í¬ê²Œí•˜ë©´ í‰ê·  ëª¨ë¸ì´ ë¨.\n",
    "** kì˜ ìµœëŒ€ê°’ì€ í•™ìŠµ ë°ì´í„°ì˜ í–‰ ìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fe7a64-aa84-4141-8d05-a60747447be9",
   "metadata": {},
   "source": [
    "* Decision Tree\n",
    "** Decision TreeëŠ” ë‚˜ë¬´ì˜ í¬ê¸°ê°€ í´ ìˆ˜ë¡ ë³µì¡í•œ ëª¨ë¸\n",
    "** í¬ê¸°ë¥¼ ê²°ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°ëŠ” max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff7702-02a6-428e-aca3-0b02b207cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ë§\n",
    "n = x_train_s.shape[0]\n",
    "model = KNeighborsRegressor(n_neighbors = n) # train setì˜ í–‰ ìˆ˜\n",
    "model.fit(x_train_s, y_train)\n",
    "pred_train = model.predict(x_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc2d5d-ca76-4a77-aaf1-45bf0b8c5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_train = [] # train setì„ ê°€ì§€ê³  ì˜ˆì¸¡í•œ ê²°ê³¼\n",
    "result_val = [] # val setì„ ê°€ì§€ê³  ì˜ˆì¸¡í•œ ê²°ê³¼\n",
    "k_values = list(range(1,101))\n",
    "\n",
    "# KNN\n",
    "for d in k_values :\n",
    "    model = KNeighborsClassifier(n_neighbors= d)\n",
    "    model.fit(x_train_s, y_train)\n",
    "    pred_tr, pred_val = model.predict(x_train_s), model.predict(x_val_s)\n",
    "    result_train.append(accuracy_score(y_train, pred_tr))\n",
    "    result_val.append(accuracy_score(y_val, pred_val))\n",
    "    \n",
    "result_train = [] # train setì„ ê°€ì§€ê³  ì˜ˆì¸¡í•œ ê²°ê³¼\n",
    "result_val = [] # val setì„ ê°€ì§€ê³  ì˜ˆì¸¡í•œ ê²°ê³¼\n",
    "depth = list(range(1,21))\n",
    "\n",
    "# Decision Tree\n",
    "for d in depth :\n",
    "    model = DecisionTreeClassifier(max_depth = d)\n",
    "    model.fit(x_train, y_train)\n",
    "    pred_tr, pred_val = model.predict(x_train), model.predict(x_val)\n",
    "    result_train.append(accuracy_score(y_train, pred_tr))\n",
    "    result_val.append(accuracy_score(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b42a9-2736-410b-9789-40287fd91b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "plt.plot(k_values, result_train, label = 'train_acc', marker = 'o')\n",
    "plt.plot(k_values, result_val, label = 'val_acc', marker = 'o')\n",
    "\n",
    "plt.xlabel('Complexity')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beeb07e-8f9e-450f-a0c8-592fffca7cff",
   "metadata": {},
   "source": [
    "## Decision Tree[ ë³€ìˆ˜ì¤‘ìš”ë„ ê·¸ë˜í”„ ] ì‹¤ì „ì—ì„œ ë§ì´ ì”€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979c9b5-e881-45d8-91b7-1159b6e0615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ ì´ë¦„ì„ ì´ìš©í•˜ì—¬ ì‹œê°í™”\n",
    "plt.figure(figsize = (20,8)) # ê·¸ë¦¼ ì‚¬ì´ì¦ˆ ì¡°ì ˆ\n",
    "plot_tree(m3.best_estimator_, feature_names = x_train.columns, \n",
    "               class_names= ['Bad', 'Good'], filled = True, fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484790ba-8b17-43e3-bd6a-852f81a151fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(importance, names):\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "    fi_df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.barplot(x='feature_importance', y='feature_names', data = fi_df)\n",
    "\n",
    "    plt.xlabel('FEATURE IMPORTANCE')\n",
    "    plt.ylabel('FEATURE NAMES')\n",
    "    plt.grid()\n",
    "\n",
    "    return fi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59b9a5-05b1-4e0a-9404-6a1de506990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = plot_feature_importance(model.feature_importances_, list(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19d05d-637d-4055-8820-180621d28edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = plot_feature_importance(model.best_estimator_.feature_importances_, list(x_train)) # íŠœë‹í–ˆê¸° ë•Œë¬¸ì—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26fdad3-eb3d-4e64-bca5-4d3429b744a8",
   "metadata": {},
   "source": [
    "## Regessor ì°¨íŠ¸ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672f86a-faa5-4385-85d5-70ad9462e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŠœë‹ ê³¼ì • ë¡œê·¸ë¥¼ dfë¡œ ì €ì¥ í•©ì‹œë‹¤.\n",
    "result = pd.DataFrame(m1_gs.cv_results_)\n",
    "\n",
    "# íŠœë‹ ê²°ê³¼ë¥¼ ê·¸ë˜í”„ë¡œ ê·¸ë ¤ë´…ì‹œë‹¤.\n",
    "plt.figure(figsize = (10,6))\n",
    "sns.lineplot(x='param_max_depth', y='mean_test_score', data = result)  # plt.plot\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620285a-44ce-4fcc-8f12-eec866a79908",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVR(Regressor)\n",
    "# ì´ë¥¼ ì°¨íŠ¸ë¡œ ê·¸ë ¤ë´…ì‹œë‹¤.\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.lineplot(x = 'param_C', y = 'mean_test_score', data = result, hue = 'param_gamma')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4d163-b6ec-4ec9-b50e-b91a5054114b",
   "metadata": {},
   "source": [
    "## XGB ë³€ìˆ˜ì¤‘ìš”ë„ / ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79b9a7-8b55-47a7-86c0-ad406137af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ìœ„ì—êº¼ plot_feature_importance í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cfedd-07e6-484e-b631-9e5fc4a9fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgbì˜ ë³€ìˆ˜ ì¤‘ìš”ë„\n",
    "weight : ëª¨ë¸ ì „ì²´ì—ì„œ í•´ë‹¹ featureê°€ splitë  ë•Œ ì‚¬ìš©ëœ íšŸìˆ˜ì˜ í•©(plot_tree ì—ì„œì˜ ê¸°ë³¸ê°’)\n",
    "gain : featureë³„ í‰ê·  imformation gain.(model.feature_importances_ ì˜ ê¸°ë³¸ê°’)\n",
    "cover : featureê°€ split í• ë•Œì˜ ìƒ˜í”Œ ìˆ˜ì˜ í‰ê· ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf69414-2d92-4b9e-b32d-1d520116b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, plot_tree, plot_importance\n",
    "plt.rcParams['figure.figsize'] = 8, 5\n",
    "plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c8e9e-0bb5-4b42-a041-af290eb0af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = plot_feature_importance(model.feature_importances_, list(x),6) #x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f47772-a1b8-44b5-b798-32b0f78cb161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™”(Treeí•˜ë‚˜ì”© ì—´ì–´ë³¼ ìˆ˜ ìˆìŒ)\n",
    "plt.rcParams['figure.figsize'] = 20,20  # íŒŒì¼ì „ì²´ì— ì˜í–¥ì„ ë¯¸ì¹¨ !!!!\n",
    "plot_tree(model, num_trees = 0) \n",
    "plt.show()\n",
    "\n",
    "# xgboost ìì²´ plot_tree í•¨ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "# plot_tree(model, num_trees = 0)\n",
    "# num_trees : ì „ì²´ íŠ¸ë¦¬ 5ê°œì§œë¦¬ ëª¨ë¸ì´ë¯€ë¡œ ê°ê° 0~4ê¹Œì§€ ì¸ë±ìŠ¤ë¡œ ì¡°íšŒí•´ ë³¼ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f2c63-94d3-4652-982d-d4bbf308f063",
   "metadata": {},
   "source": [
    "## ì„±ëŠ¥ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7fa2d-6358-4b95-b3d1-d030c05b573e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RMSE, MAE, MAPE = [],[],[]\n",
    "model_desc = ['lr_selected', 'lr_all','knn','dt','rf','xgb','svm']\n",
    "pred = [p1, p2, p3, p4, p5, p6,p7]\n",
    "\n",
    "for i, p in enumerate(pred) :\n",
    "    RMSE.append(mean_squared_error(y_val, p, squared=False))\n",
    "    MAE.append(mean_absolute_error(y_val, p))\n",
    "    MAPE.append(mean_absolute_percentage_error(y_val, p))\n",
    "\n",
    "result = pd.DataFrame({'model_desc':model_desc,'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543c4d3-d6f5-415f-abb8-251ab9fec147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_val, pred1))\n",
    "print(classification_report(y_val, pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5921def8-9b58-4f63-94d5-5fd1579763dd",
   "metadata": {},
   "source": [
    "## Decision Tree ì‹œê°í™” ë° ë³€ìˆ˜ì¤‘ìš”ë„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74512328-ac56-4f62-8165-a1b739bd2c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10)) # ê·¸ë¦¼ ì‚¬ì´ì¦ˆ ì¡°ì ˆ\n",
    "plot_tree(m1, feature_names = list(x_train), \n",
    "               class_names= ['Stay', 'Leave'], filled = True, fontsize = 10);  # class_names = [targetì˜ ë‚´ìš©]\n",
    "plt.show()\n",
    "print('-'*88)\n",
    "# ë³€ìˆ˜ ì¤‘ìš”ë„\n",
    "print(list(x_train))\n",
    "print(m1.feature_importances_)\n",
    "print('-'*88)\n",
    "print(classification_report(y_val, p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eaf2ba-8169-49c1-ab4e-c2503d7049fe",
   "metadata": {},
   "source": [
    "## ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ìœ„í•œ ì „ì§„ì„ íƒë²• í•¨ìˆ˜ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f321f-66db-420b-80f0-047e9d7bad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant\n",
    "\n",
    "def forward_stepwise_linear(x_train, y_train):\n",
    "\n",
    "    # ë³€ìˆ˜ëª©ë¡, ì„ íƒëœ ë³€ìˆ˜ ëª©ë¡, ë‹¨ê³„ë³„ ëª¨ë¸ê³¼ AIC ì €ì¥ì†Œ ì •ì˜\n",
    "    features = list(x_train)\n",
    "    selected = []\n",
    "    step_df = pd.DataFrame({ 'step':[], 'feature':[],'aic':[]})\n",
    "\n",
    "    # \n",
    "    for s in range(0, len(features)) :\n",
    "        result =  { 'step':[], 'feature':[],'aic':[]}\n",
    "\n",
    "        # ë³€ìˆ˜ ëª©ë¡ì—ì„œ ë³€ìˆ˜ í•œê°œì”© ë½‘ì•„ì„œ ëª¨ë¸ì— ì¶”ê°€\n",
    "        for f in features :\n",
    "            vars = selected + [f]\n",
    "            x_tr = x_train[vars]\n",
    "            model = OLS(y_train, add_constant(x_tr)).fit(disp=False)\n",
    "            result['step'].append(s+1)\n",
    "            result['feature'].append(vars)\n",
    "            result['aic'].append(model.aic)\n",
    "        \n",
    "        # ëª¨ë¸ë³„ aic ì§‘ê³„\n",
    "        temp = pd.DataFrame(result).sort_values('aic').reset_index(drop = True)\n",
    "\n",
    "        # ë§Œì•½ ì´ì „ aicë³´ë‹¤ ìƒˆë¡œìš´ aic ê°€ í¬ë‹¤ë©´ ë©ˆì¶”ê¸°\n",
    "        if step_df['aic'].min() < temp['aic'].min() :\n",
    "            break\n",
    "        step_df = pd.concat([step_df, temp], axis = 0).reset_index(drop = True)\n",
    "\n",
    "        # ì„ íƒëœ ë³€ìˆ˜ ì œê±°\n",
    "        v = temp.loc[0,'feature'][s]\n",
    "        features.remove(v)\n",
    "\n",
    "        selected.append(v)\n",
    "    \n",
    "    # ì„ íƒëœ ë³€ìˆ˜ì™€ step_df ê²°ê³¼ ë°˜í™˜\n",
    "    return selected, step_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f64df06-f8e2-4d8a-b2ef-97d362534801",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars, result = forward_stepwise_logistic(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0df5b-343a-4271-9877-0ac0cef5ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ íƒëœ ë³€ìˆ˜\n",
    "lr_m1 = LinearRegression()\n",
    "lr_m1.fit(x_train[vars], y_train)\n",
    "p1 = lr_m1.predict(x_val[vars])\n",
    "\n",
    "print('RMSE : ', mean_squared_error(y_val, p1, squared=False))\n",
    "print('MAE  : ', mean_absolute_error(y_val, p1))\n",
    "print('MAPE : ', mean_absolute_percentage_error(y_val, p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf20d7b-664d-4370-8e00-55b8e409fb21",
   "metadata": {},
   "source": [
    "## cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c24999-fa58-4b07-b323-450de6a1323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# train + validation setì„ ì´ìš©í•˜ì—¬ í•™ìŠµ, ì˜ˆì¸¡, í‰ê°€ë¥¼ í•œë²ˆì—. (ì—¬ê¸°ì„œëŠ” .fit ì´ ì•„ë‹˜!)\n",
    "dt_result = cross_val_score(model, x, y, cv=10)\n",
    "print(dt_result)\n",
    "print(dt_result.mean(), dt_result.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0876d-960f-44ea-b0a5-8a2acb251549",
   "metadata": {},
   "source": [
    "## Permutation Feature Importance(ê·¸ ì™¸ ë³€ìˆ˜ì¤‘ìš”ë„ ê·¸ë˜í”„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd55a3-cfe5-4fcb-ace0-1b399f315a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d30d2c-6d7c-4780-9492-1a021fc4c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfi1 = permutation_importance(model4, x_val_s, y_val, n_repeats=10, scoring = 'r2', random_state=2022)\n",
    "# scoring = 'accuracy' default[ë¶„ë¥˜ ëª¨ë¸]\n",
    "# deep learning ëª¨ë¸ì— ëŒ€í•´ì„œëŠ” ëª…ì‹œì ìœ¼ë¡œ scoring = 'r2'ì„ ì§€ì •í•´ í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58147e8f-a372-4b2c-a951-7df3e66ce309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# featureë³„ Score ë¶„í¬\n",
    "plt.figure(figsize = (10,8))\n",
    "for i,vars in enumerate(list(x)) :\n",
    "    sns.kdeplot(pfi1.importances[i], label = vars)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820c5a2-0ff6-4c62-b698-2feac8aa0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = pfi1.importances_mean.argsort()\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.boxplot(pfi1.importances[sorted_idx].T, vert=False, labels=x.columns[sorted_idx])\n",
    "plt.axvline(0, color = 'r')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d3e93-fd33-4813-b864-ad186951fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê· ê°’ìœ¼ë¡œ ë³€ìˆ˜ì¤‘ìš”ë„ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n",
    "pfi1 = permutation_importance(model1, x_val_s, y_val, n_repeats=10, scoring = 'r2', random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc70a2-80be-4adf-a371-f3b0cd35c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€ìˆ˜ ì¤‘ìš”ë„ plot(ê°€ì ¸ì˜¤ê¸°)\n",
    "result = plot_feature_importance(pfi1.importances_mean, list(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa99e68-a6f0-4011-b165-60185b89f33d",
   "metadata": {},
   "source": [
    "## Partial Dependence Plot(ê°œë³„ ë³€ìˆ˜ë³„ ê´€ê³„)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25612daa-29b7-4292-a14c-4f611064b304",
   "metadata": {},
   "source": [
    "* ë³€ìˆ˜ ì¤‘ìš”ë„,PDP : train ë°ì´í„°ë¡œ ì‚´í´ë³´ëŠ” ê²ƒì´ ê¸°ë³¸ì´ë‹¤. ê·¸ë¦¬ê³  ê²°ê³¼ê°€ ìœ ì‚¬í•´ì•¼ í•œë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524504e-98c0-4b99-85f4-ddc30f355d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import plot_partial_dependence, partial_dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b994514-5ba4-44e8-8c6f-d3514006025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°œë³„ ë³€ìˆ˜ ë¶„ì„\n",
    "var = 'MonthlyIncome'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "plot_partial_dependence(model, features = [var], X = x_val)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac85a3e-a9a0-407d-9b5a-5b6c28609b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë°ì´í„°ë¡œ í™•ì¸\n",
    "var = 'MonthlyIncome'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 12, 8\n",
    "plot_partial_dependence(model, features = [var], X = x_val, kind = 'both') #kind = 'average'\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35f900-19bf-48d5-b435-3be786601d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ê°œ ë³€ìˆ˜ ë¹„êµ\n",
    "plot_partial_dependence(model, features = ['rm','lstat'], X = x_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9cea2-ca6a-4260-8783-c65510ac347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ê°œ ë³€ìˆ˜ ë¹„êµ\n",
    "plot_partial_dependence(model2, features = [('CreditAmount','Age')], X = x_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13acb0-6d39-4e2f-a7ad-b98442f5d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŠ¤ì¼€ì¼ë§í•œê±° ë°ì´í„° í”„ë ˆì„ì— ë„£ì–´ì„œ ë½‘ì•„ì•¼í•¨\n",
    "x_train_s = pd.DataFrame(x_train_s, columns = list(x))  # ì¹¼ëŸ¼ì´ë¦„ ì§€ì • í•„ìš”!!\n",
    "x_val_s = pd.DataFrame(x_val_s, columns = list(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd92fd6-e143-48c7-839e-96ab575b40ff",
   "metadata": {},
   "source": [
    "## SHAP ê°’ìœ¼ë¡œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„¤ëª…í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26bd8b4-529c-4568-a7e7-b8167cbef6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ í˜•íšŒê·€ëŠ” íšŒê·€ê³„ìˆ˜ë¡œ ë³€ìˆ˜ ê¸°ì—¬ë„ í•´ì„í•´ë„ ë¬´ë°©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd050132-df09-4291-bc29-658ad30df832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (íšŒê·€ëª¨ë¸)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer1.shap_values(x_train)\n",
    "\n",
    "# (ë¶„ë¥˜ëª¨ë¸)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777c0f6-5481-49e9-a1d9-5dcce0d4076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œëŠ” ì…ë ¥ë°ì´í„°(x)ê°€ 2ì°¨ì›ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "pred = model1.predict(x_train.iloc[0:1,:])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361de6de-deb7-4cf6-8ce8-6f1a3a522cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainì˜ í‰ê· (íšŒê·€)\n",
    "explainer1.expected_value\n",
    "\n",
    "# trainì˜ í‰ê· (ë¶„ë¥˜)\n",
    "explainer1.expected_value[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e9a1ae-2275-452e-b6c6-567d2b419c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ë±ìŠ¤ë³„ ë°ì´í„°\n",
    "shap.initjs() # javascript ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ --> colabì—ì„œëŠ” ëª¨ë“  ì…€ì— í¬í•¨ì‹œì¼œì•¼ í•¨.-> ì•„ë‚˜ì½˜ë‹¤ì—ì„œëŠ” í•œë²ˆë§Œ !\n",
    "index=371\n",
    "# force_plot(ì „ì²´í‰ê· , shapley_values, input)\n",
    "shap.force_plot(explainer1.expected_value, shap_values1[index,:], x.iloc[index,:])\n",
    "\n",
    "# ë¶„ë¥˜ëª¨ë¸\n",
    "shap.initjs() # javascript ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ --> colabì—ì„œëŠ” ëª¨ë“  ì…€ì— í¬í•¨ì‹œì¼œì•¼ í•¨.-> ì•„ë‚˜ì½˜ë‹¤ì—ì„œëŠ” í•œë²ˆë§Œ !\n",
    "index=932\n",
    "# force_plot(ì „ì²´í‰ê· , shapley_values, input)\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][index,:], x.iloc[index,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a1d2a-6bf7-46e5-86da-61fda96261e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ë°ì´í„°\n",
    "shap.initjs() # javascript ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ --> colabì—ì„œëŠ” ëª¨ë“  ì…€ì— í¬í•¨ì‹œì¼œì•¼ í•¨.-> ì•„ë‚˜ì½˜ë‹¤ì—ì„œëŠ” í•œë²ˆë§Œ !\n",
    "\n",
    "# force_plot(ì „ì²´í‰ê· , shapley_values, input)\n",
    "shap.force_plot(explainer1.expected_value, shap_values1[0, :], x_train.iloc[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edf225d-cdd8-4e25-a11e-d96ef4fadca5",
   "metadata": {},
   "source": [
    "## class balanceë¥¼ ë§ì¶”ê¸° ìœ„í•œ resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f098a68-04d1-45a1-9473-e893b5fca852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3a8f7-42da-4cb1-a3b2-a0d0644bf8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a0459-dca1-4b10-9c53-79827e1a9fb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ë¹„ì§€ë„ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b59d6-b431-4063-9d80-7500cf3a167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1,50)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    model = KMeans(n_clusters=k)\n",
    "    model.fit(mobile_x)\n",
    "    inertias.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816bcac3-b88c-456d-b8d6-846b97a4e51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34af48-e397-48a1-aaa7-c27279a6a29a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ë§[KNN]\n",
    "model = KMeans(n_clusters=5)\n",
    "model.fit(mobile_x)\n",
    "pred = model.predict(mobile_x)\n",
    "pred = pd.DataFrame(pred, columns = ['predict'])\n",
    "# ê²°ê³¼ ë³´ê¸°\n",
    "mobile_y.reset_index(inplace=True, drop=True)\n",
    "result = pd.concat([mobile_x, mobile_y, pred], axis =1)\n",
    "result.CHURN = result.CHURN.astype('int')\n",
    "# í´ëŸ¬ìŠ¤í„° ë³„ ê³ ê°ì´íƒˆìœ¨\n",
    "result.groupby('predict')['CHURN'].mean()\n",
    "# ì „ì²´ í‰ê· \n",
    "result['CHURN'].value_counts() / result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e88bc6-48aa-43a0-bf61-6195f7628f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN ëª¨ë¸ì„ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.\n",
    "model = DBSCAN(eps=0.1, min_samples=3)\n",
    "model.fit(x)\n",
    "# fittingí•œ í›„ì— ëª¨ë¸ì˜ labels_ ê°’ì´ ì°¾ì•„ë‚¸ êµ°ì§‘ ì¢…ë¥˜ì…ë‹ˆë‹¤.\n",
    "clusters = model.labels_\n",
    "# êµ°ì§‘ ë²ˆí˜¸ ì¤‘ -1ì€ ì´ìƒì¹˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.(ì–´ëŠ êµ°ì§‘ì—ë„ í¬í•¨ ì•ˆë˜ëŠ” ê°’ë“¤!)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126cc2cd-68ba-4e0c-b45f-2cf050a208b6",
   "metadata": {},
   "source": [
    "## ì‹œê³„ì—´ ë°ì´í„°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359fee4-1fa7-45ce-94c0-0b540f19c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore')\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1b49b-b28b-4008-b77d-f1ef70702d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target ì‹œê³„ì—´ë¡œ í™•ì¸ \n",
    "plt.figure(figsize = (20,8))\n",
    "plt.plot(data['sales'])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "temp = data[-100:]\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.plot(temp['sales'], marker ='o')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa23b2-c179-4c13-a838-d655a9e750e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì”ì°¨ ë¶„ì„\n",
    "def residual_diag(residuals, lags = 20) :\n",
    "\n",
    "    print('* ì •ê·œì„± ê²€ì •(> 0.05) : ', round(spst.shapiro(residuals)[1],5))\n",
    "    print('* ì •ìƒì„± ê²€ì •(< 0.05) : ', round(sm.tsa.stattools.adfuller(residuals)[1],5))\n",
    "    print('* ìê¸°ìƒê´€ì„± í™•ì¸(ACF, PACF)')\n",
    "    fig,ax = plt.subplots(1,2, figsize = (15,5))\n",
    "    plot_acf(residuals, lags = lags, ax = ax[0])\n",
    "    plot_pacf(residuals, lags = lags, ax = ax[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733fa1d-5bec-493d-98c1-78d906b7dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‚ ì§œ íƒ€ì…ìœ¼ë¡œ ë³€ê²½\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "# ë‚ ì§œë¥¼ ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "data['DT'] = data['date']\n",
    "data.set_index('DT', inplace=True)\n",
    "data.head()\n",
    "## ë‚ ì§œë‹¨ìœ„ ì§€ì •í•˜ê¸° : freq / ì¸ë±ìŠ¤ ì¡°íšŒì‹œ, ë§ˆì§€ë§‰ì— ìˆëŠ” freq ì˜µì…˜\n",
    "# ì¼ë‹¨ìœ„\n",
    "data.asfreq('D').head()\n",
    "# ì›”(ë§)ë‹¨ìœ„\n",
    "data.asfreq('M').head()\n",
    "# ì›”ì´ˆ ë‹¨ìœ„\n",
    "data.asfreq('MS').head()\n",
    "#(ì¶”ê°€) ë¹ ì§„ê°’ ì°¾ê¸°\n",
    "temp = data.asfreq('D')\n",
    "temp.isna().sum()\n",
    "# ì±„ìš°ê¸°\n",
    "data.asfreq('D', method = 'ffill')\n",
    "df = data.asfreq('D') # ì¼ë‹¨ìœ„ ë°ì´í„°ì´ë¯€ë¡œ ì´ê±¸ë¡œ ì§€ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c782bfe-33b8-4ed8-ba1e-14cd98bc0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y ë§Œë“¤ê¸°\n",
    "df['y'] = df['sales'].shift(-1)\n",
    "display(df.head())\n",
    "display(df.tail())\n",
    "# ì œì¼ ë§ˆì§€ë§‰ í–‰ì€ ì‚­ì œ\n",
    "df.dropna(axis = 0, inplace = True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077b5ee-bae1-4267-9902-c3c237830513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¶„í• \n",
    "# 1) x, y ë‚˜ëˆ„ê¸°\n",
    "# .values(ë„˜íŒŒì´ ì–´ë ˆì´)ë¡œ ë³€í™˜í•´ì„œ ì €ì¥í•˜ëŠ” ì´ìœ  â¡ ë°ì´í„° ìŠ¤í”Œë¦¿ indexë¥¼ ì ìš©í•´ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ì„œ\n",
    "target = 'y'\n",
    "\n",
    "x = df.drop([target, 'date'], axis = 1)\n",
    "y = df.loc[:, target]\n",
    "\n",
    "# ì‹œê³„ì—´ ë°ì´í„° ë¶„í• \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "x.shape\n",
    "# validation set size\n",
    "val_size = 30\n",
    "nfold = 3\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = nfold, test_size = val_size)\n",
    "tscv\n",
    "\n",
    "#ì°¸ì¡°\n",
    "# .splitì„ ì´ìš©í•˜ì—¬ fold í•˜ë‚˜ì”© ì¸ë±ìŠ¤ë“¤ì„ ë½‘ì•„ ë‚¼ ìˆ˜ ìˆìŒ.\n",
    "for train_index, val_index in tscv.split(x):\n",
    "    print(\"Train:\", train_index, \"Val:\", val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7509d9-02fa-4c4d-9963-d83d88be275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ë§\n",
    "# loop ëŒë©° ëª¨ë¸ë§(cross-validation) ìˆ˜í–‰\n",
    "rmse, mae, mape = [],[],[]\n",
    "residuals = []\n",
    "pred = []\n",
    "model = LinearRegression()\n",
    "\n",
    "for train_index, val_index in tscv.split(x):\n",
    "\n",
    "    # ì¸ë±ìŠ¤ë¡œ ë°ì´í„° ë¶„í• \n",
    "    x_train, y_train = x.iloc[train_index], y.iloc[train_index]\n",
    "    x_val, y_val = x.iloc[val_index], y.iloc[val_index]\n",
    "\n",
    "    # í•™ìŠµ\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # ì˜ˆì¸¡\n",
    "    pr = model.predict(x_val)\n",
    "    pred += list(pr)\n",
    "\n",
    "    # í‰ê°€\n",
    "    rmse.append(mean_squared_error(y_val, pr, squared = False))\n",
    "    mae.append(mean_absolute_error(y_val, pr))\n",
    "    mape.append(mean_absolute_percentage_error(y_val, pr))\n",
    "\n",
    "    # ì”ì°¨ : ê° foldì˜ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€\n",
    "    residuals += list(y_val - pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969db4bd-ac0c-4fbb-adb9-1e023faaf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì¸¡ ê²°ê³¼ í‰ê°€\n",
    "print('RMSE : ',round(np.mean(rmse),4))\n",
    "print('MAE  : ',round(np.mean(mae),4))\n",
    "print('MAPE : ',round(np.mean(mape),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ccda9-2ee3-442d-aa60-215ffc227846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ë¹„êµ\n",
    "n = val_size * nfold\n",
    "pred = pd.Series(pred, index = y[-n:].index)\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.plot(y[:-n], label = 'train')\n",
    "plt.plot(y[-n:], label = 'val')\n",
    "plt.plot(pred, label = 'predicted')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.plot(y[-n:], label = 'val')\n",
    "plt.plot(pred, label = 'predicted')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb6b063-297d-4b19-90ca-b535597d6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€: ì”ì°¨ë¶„ì„\n",
    "## ì‹œê°í™”\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.plot(residuals)\n",
    "plt.axhline(0, color = 'r', ls = '--')\n",
    "plt.axhline(np.mean(residuals), color = 'g', ls = '--')\n",
    "plt.show()\n",
    "\n",
    "## ACF, PACF(ìê¸°ìƒê´€ì„± ì—¬ë¶€ í™•ì¸)\n",
    "lags = 20\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize = (15,5))\n",
    "plot_acf(residuals, lags = lags, ax = ax[0])\n",
    "plot_pacf(residuals, lags = lags, ax = ax[1])\n",
    "plt.show()\n",
    "\n",
    "## ê²€ì •\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "stats.shapiro(residuals)[1]  # ì •ê·œì„± ê²€ì • : Shapiro-Wilk ê²€ì •\n",
    "sm.tsa.stattools.adfuller(residuals)[1]  # ì •ìƒì„± ê²€ì • : ADF ê²€ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d21e45a-b164-4278-96bb-e2922bb8410f",
   "metadata": {},
   "source": [
    "## ì‹œê³„ì—´ ëª¨ë¸ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a079f9-4e0d-49d0-b774-c97f26ab228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì‹œê°í™”\n",
    "def plot_model_result(y_train, y_val, pred) :\n",
    "    pred = pd.Series(pred, index = y_val.index)\n",
    "\n",
    "    # ì „ì²´ ì‹œê°í™”\n",
    "    plt.figure(figsize = (20,12))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(y_train, label = 'train')\n",
    "    plt.plot(y_val, label = 'val')\n",
    "    plt.plot(pred, label = 'pred')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(y_val, label = 'val')\n",
    "    plt.plot(pred, label = 'pred')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9739f4e-d804-4514-9beb-94847e7120d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yê°’ ì‚´í´ë³´ê¸°\n",
    "residual_diag(y_train, lags = 30)\n",
    "\n",
    "# ARIMA ëª¨ë¸ë§\n",
    "m1_1 = sm.tsa.SARIMAX(y_train, order=(1,0,1)).fit() # ARMA\n",
    "m1_2 = sm.tsa.SARIMAX(y_train, order=(1,1,1)).fit() # ARIMA\n",
    "\n",
    "# SARIMA ëª¨ë¸ë§ : P, D, Q, m = 1,1,1,7 ë¡œ ëª¨ë¸ì„ ìƒì„±í•©ì‹œë‹¤.\n",
    "m2_1 = sm.tsa.SARIMAX(y_train, order=(5,1,4), seasonal_order=(1,1,1,7)).fit()\n",
    "\n",
    "m3_1 = sm.tsa.SARIMAX(y_train, order=(5,1,4), seasonal_order=(1,1,1,7), exog=x_train).fit()\n",
    "\n",
    "# í‰ê°€[ì”ì°¨ ì§„ë‹¨]\n",
    "residuals = m1_1.resid  # y_trainê³¼ ì˜ˆì¸¡ê°’ ì°¨ì´\n",
    "residual_diag(residuals)\n",
    "# í‰ê°€[AIC] ->ì„ í˜• ëª¨ë¸ì—ì„œì˜ ì í•©ë„ì™€, featureê°€ ê³¼ë„í•˜ê²Œ ëŠ˜ì–´ë‚˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ë„ë¡ ì„¤ê³„ëœ í†µê³„ëŸ‰ì´ AIC ì…ë‹ˆë‹¤.\n",
    "# ê°’ì´ ì‘ì„ ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸\n",
    "# ê³µì‹ : ğ´ğ¼ğ¶=âˆ’2 lnâ¡(ğ¿)+2ğ‘˜ â¡ - ëª¨ë¸ì˜ ì í•©ë„ + ë³€ìˆ˜ì˜ ê°¯ìˆ˜\n",
    "print('model1 AIC :', m1_1.aic)\n",
    "# í‰ê°€[Validation]\n",
    "pred = m1_1.forecast(30)   # SARIMAX ëª¨ë¸ì„ ìƒì„±í•˜ê³ , ì˜ˆì¸¡í•  ë•ŒëŠ” exog=x_val ì˜µì…˜ì´ ë“¤ì–´ê°€ì•¼ í•¨. \n",
    "print('MAE :', mean_absolute_error(y_val, pred))\n",
    "print('MAPE:', mean_absolute_percentage_error(y_val, pred))\n",
    "# í‰ê°€[ê²°ê³¼ ì‹œê°í™”]\n",
    "plot_model_result(y_train, y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc011a5-ea27-4a08-bce6-dd151e82c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\n",
    "\n",
    "# í•™ìŠµ\n",
    "from itertools import product\n",
    "# product í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ê°’ì˜ ì¡°í•©ì„ êµ¬ì„±\n",
    "p = [1,2,3,4,5]\n",
    "q = [1,2,3,4,5]\n",
    "d = [1]\n",
    "iter = list(product(p,d,q))\n",
    "iter\n",
    "\n",
    "# íŠœë‹ \n",
    "mae, aic = [],[]\n",
    "for i in iter :\n",
    "    model_fit = sm.tsa.SARIMAX(y_train, order=(i[0],i[1],i[2])).fit()\n",
    "    pred = model_fit.forecast(30)\n",
    "    mae.append( mean_absolute_error(y_val, pred))\n",
    "    aic.append(model_fit.aic)\n",
    "    print(i)\n",
    "    \n",
    "result = pd.DataFrame({'params(p,d,q)' : iter, 'mae' : mae, 'aic':aic})\n",
    "\n",
    "display(result.loc[result['mae'] == result.mae.min()])\n",
    "display(result.loc[result['aic'] == result.aic.min()])\n",
    "\n",
    "# ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ p, d, q ê°’ìœ¼ë¡œ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "m1_3 = sm.tsa.SARIMAX(y_train, order=(5,1,4)).fit()\n",
    "\n",
    "# í‰ê°€[ì”ì°¨ì§„ë‹¨]\n",
    "residuals = m1_3.resid\n",
    "residual_diag(residuals) # seasonallity \n",
    "# í‰ê°€[AIC]\n",
    "print('model2 AIC :', m1_3.aic)\n",
    "# í‰ê°€[validation]\n",
    "p1 = m1_3.forecast(30)\n",
    "print('MAE :', mean_absolute_error(y_val, p1))\n",
    "print('MAPE:', mean_absolute_percentage_error(y_val, p1))\n",
    "# ê²°ê³¼ ì‹œê°í™”\n",
    "plot_model_result(y_train, y_val, p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841415ba-a113-49b3-a213-0f2df244b0af",
   "metadata": {},
   "source": [
    "## ë”¥ëŸ¬ë‹[keras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63aa4c-1211-43fd-a84b-c6ffb57254d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ad5b2-0eec-4277-8908-5af216a9dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape , y.shape  # í™•ì¸í•´ë³¼ê²ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9d68f-a6f4-4763-8fef-6d406055849b",
   "metadata": {},
   "source": [
    "* Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56367fdb-b8cf-4b4b-b174-7834b14485f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ [Linear Regression]\n",
    "# í˜¹ì‹œ ì´ë¯¸ ê·¸ë ¤ë‘” ê·¸ë˜í”„ê°€ ìˆë‹¤ë©´ ë‚ ë ¤ì¤˜!\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# ë ˆì´ì–´ë“¤ì„ ì‚¬ìŠ¬ë¡œ ì—°ê²°í•˜ ë“¯ì´ ì—°ê²°!\n",
    "input_layer = keras.layers.Input(shape=(1,))\n",
    "output_layer = keras.layers.Dense(1)(input_layer)\n",
    "\n",
    "# ëª¨ë¸ì˜ ì‹œì‘ê³¼ ëì„ ì§€ì •\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "model.fit(x, y, epochs=10, verbose=1)\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(model.predict(x).reshape(-1,) )\n",
    "\n",
    "@ [Logistic Regression]\n",
    "keras.backend.clear_session()\n",
    "input_layer = keras.layers.Input(shape=(1,))\n",
    "output_layer = keras.layers.Dense(1, activation='sigmoid')(input_layer)\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "model.fit(x, y, epochs=10, verbose=1)\n",
    "print(y)\n",
    "print(model.predict(x).reshape(-1,) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb0919-cb10-4b52-b673-e043cccdd6c2",
   "metadata": {},
   "source": [
    "* Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67fd5e0-82fb-42c9-a314-9715ab7552b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ [Linear Regression]\n",
    "# 1ë²ˆ ì²­ì†Œ : ì´ë¯¸ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì´ ìˆë‹¤ë©´ ê·¸ ëª¨ë¸ì„ ì—†ì• ì¤˜\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# 2ë²ˆ ëª¨ë¸ ì„ ì–¸\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# 3ë²ˆ ëª¨ë¸ ë¸”ë¡ ì¡°ë¦½\n",
    "model.add( keras.layers.Input(shape=(1,)) )\n",
    "model.add( keras.layers.Dense(1) )\n",
    "\n",
    "## ì˜¤ë¦¬ì§€ë„ Sequential API\n",
    "# model.add( keras.layers.Dense(1, input_shape=(1,)) )\n",
    "\n",
    "# 4ë²ˆ ì»´íŒŒì¼ \n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.fit(x[:15], y[:15], epochs=10, verbose=1)\n",
    "print(y[15:])\n",
    "print(model.predict(x[15:]))\n",
    "\n",
    "@ [Logistic Regression]\n",
    "# í˜¹ì‹œ ì´ë¯¸ ê·¸ë ¤ë‘” ê·¸ë˜í”„ê°€ ìˆë‹¤ë©´ ë‚ ë ¤ì¤˜!\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# modelì— ìˆœì°¨ì ìœ¼ë¡œ ë ˆì´ì–´ë¥¼ ìŒ“ì•„ê°€ê² ë‹¤ëŠ” ì˜ë„!\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# modelì— ì¸í’‹ ê°’ì„ ë°›ëŠ” ë ˆì´ì–´ë¥¼ ë„£ìŒ\n",
    "model.add( keras.layers.Input(shape=(1,)) )\n",
    "# modelì— Dense ë ˆì´ì–´ë¥¼ ë„£ì„ê±°ì•¼ (ìµœì´ˆì˜ ë ˆì´ì–´) : weightë¥¼ ê³±í•˜ê³ , biasë¥¼ ë”í•´ì£¼ëŠ” ê³¼ì •\n",
    "model.add( keras.layers.Dense(1, activation='sigmoid') )\n",
    "\n",
    "# ì˜¤ë¦¬ì§€ë„ Sequential API\n",
    "# model.add( keras.layers.Dense(1, input_shape=(1,), activation='sigmoid') )\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics=['accuracy'])\n",
    "# keras.losses.binary_crossentropy ì´ê±¸ë¡œë„ ê°€ëŠ¥\n",
    "\n",
    "model.fit(x[:15], y[:15], epochs=10, verbose=1)\n",
    "print(y[15:])\n",
    "print(model.predict(x[15:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91289ec3-9fbe-42a4-bb08-76d33084e753",
   "metadata": {},
   "source": [
    "* ë©€í‹°í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60420175-9abb-4a4d-aa65-a20f60569a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¯¸ë˜ ë°ì´í„°ë¥¼ ë¨¼ì œ ë–¼ì–´ë‚´ì•¼í•˜ê¸° ë•Œë¬¸ì— ë¨¼ì € ë¶„ë¦¬í•´ì¤€ë‹¤.(ë¯¸ë˜ë°ì´í„°ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠëŠ”ë‹¤)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.2, random_state=2022)\n",
    "x_train.shape, y_train.shape\n",
    "# One-Hot Encoding  (get_dummies= xì—ë§Œ ì ìš©)\n",
    "class_n = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, class_n)\n",
    "y_test = to_categorical(y_test, class_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e62134-a6df-4a34-bd32-92d177f7835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ y í™•ì¸í•˜ê¸°\n",
    "iris.target_names\n",
    "\n",
    "# One-Hot Encoding  (get_dummies= xì—ë§Œ ì ìš©)\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "y = to_categorical(y, 3) # ë°˜ë³µ ì‹¤í–‰ ì£¼ì˜!!(ê³„ì† ìƒì„±ë¨)\n",
    "x.shape, y.shape\n",
    "\n",
    "@ [Sequential]\n",
    "keras.backend.clear_session()\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input(shape=(4,)))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs=10,verbose=1)\n",
    "model.predict(x_test).reshape(-1)\n",
    "y.argmax(axis=1)\n",
    "\n",
    "@ [Functional]\n",
    "keras.backend.clear_session()\n",
    "il = keras.layers.Input(shape=(4,))\n",
    "ol = keras.layers.Dense(3,activation='softmax')(il)\n",
    "model = keras.models.Model(il,ol)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "model.fit(x_train,y_train,epochs=10, verbose=1)\n",
    "pred = model.predict(x_test)\n",
    "pred[:5] # í•©ì¹˜ë©´ í™•ë¥ ê°’ =1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b4936-56b0-4659-a35c-f24f016b555f",
   "metadata": {},
   "source": [
    "* Hidden layer ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab5d1f-3c3d-4be5-80ae-f6c1a261c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ [Sequential API]\n",
    "\n",
    "keras.backend.clear_session()\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input(shape=(13,)))\n",
    "model.add(keras.layers.Dense(32,activation='relu'))\n",
    "model.add(keras.layers.Dense(32,activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "@ [Functional]\n",
    "keras.backend.clear_session()\n",
    "il = keras.layers.Input(shape=(30,))\n",
    "hl = keras.layers.Dense(32, activation='relu', name='hidden1')(il)\n",
    "hl = keras.layers.Dense(32, activation='relu', name='hidden2')(hl)\n",
    "ol = keras.layers.Dense(1,activation='sigmoid')(hl)\n",
    "model = keras.models.Model(il,ol)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "@ [multi-Functional]\n",
    "keras.backend.clear_session()\n",
    "il = keras.layers.Input(shape=(4,))\n",
    "hl = keras.layers.Dense(32, activation='relu', name='hidden1')(il)\n",
    "hl = keras.layers.Dense(32, activation='relu', name='hidden2')(hl)  # ë³€ìˆ˜ëª… ê°™ì•„ë„ ìƒê´€ì—†ìŒ\n",
    "ol = keras.layers.Dense(3,activation='softmax')(hl)\n",
    "model = keras.models.Model(il,ol)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " # optimizer=keras.optimizers.Adam(0.01)-> eta\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8066a064-fb8b-457c-93af-792304ffbdb9",
   "metadata": {},
   "source": [
    "## ë”¥ëŸ¬ë‹[ANN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6361238-7234-4dd4-937c-62c28c1ad4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì°¸ì¡°ì½”ë“œ\n",
    "'''\n",
    "matplolib inline ëª…ë ¹ì–´ë¥¼ í†µí•´ì„œ\n",
    "matplotìœ¼ë¡œ ê·¸ë¦¬ëŠ” í”Œë¡¯ë“¤ì„ ì£¼í”¼í„° ë…¸íŠ¸ë¶ ë‚´ì—ì„œ ë³¼ ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.\n",
    "í¬ë§·ì„ retinaë¡œ ë°”ê¾¸ë©´ ê·¸ë˜í”„ì˜ í™”ì§ˆì´ í›¨ì”¬ ì¢‹ì•„ì§„ë‹¤.\n",
    "'''\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20445e-b94f-47b6-b20a-c81a42dedc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) ì „ì²˜ë¦¬\n",
    "x = wine.data\n",
    "y = wine.target\n",
    "x.shape, y.shape\n",
    "data.target_names\n",
    "\n",
    "## reshape -> flattení•˜ë©´ í•„ìš”ì—†ìŒ\n",
    "train_x.shape\n",
    "train_x = train_x.reshape([train_x.shape[0],-1])\n",
    "test_x = test_x.reshape([test_x.shape[0],-1])\n",
    "train_x.shape # 28*28\n",
    "## min-max scaling\n",
    "max_n, min_n = train_x.max(), train_x.min()\n",
    "max_n, min_n\n",
    "train_x = (train_x - min_n) / (max_n - min_n)\n",
    "test_x = (test_x - min_n) / (max_n - min_n)\n",
    "print(f'max : {train_x.max()} / min : {train_x.min()}')\n",
    "## target feature : One-hot Encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "len_y = len(set(train_y))\n",
    "train_y = to_categorical(train_y, len_y)\n",
    "test_y = to_categorical(test_y, len_y)\n",
    "\n",
    "2) ëª¨ë¸ë§\n",
    "train_x.shape, train_y.shape\n",
    "keras.backend.clear_session()\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input(train_x.shape[1]))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(0.01),metrics='accuracy')\n",
    "model.summary()\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss',  # ë¬´ì–¼ ê´€ì°°í• ì§€(ê´€ì¸¡ëŒ€ìƒ)\n",
    "                   min_delta=0,         # ìµœì†Œí•œ ë‚˜ë¹ ì§€ì§€ ì•Šìœ¼ë©´ ê´œì°®ì•„\n",
    "                   patience=5,          # ì¤‘ìš”! ëª‡ë²ˆì´ë‚˜ ì°¸ì„ì§€(ê±°ê¸°ê¹Œì§€ ê°œì„  ì•ˆë˜ë©´ ë©ˆì¶œê±°ì•¼)\n",
    "                   verbose=1,            \n",
    "                   restore_best_weights=True)  # (ë°˜ë“œì‹œ ì‚¬ìš©)í•™ìŠµì´ ë©ˆì·„ì„ ë•Œ, ìµœì ì˜ ê°€ì¤‘ì¹˜ë¡œ ì „í™˜í•´ì¤Œ\n",
    "model.fit(train_x, train_y, \n",
    "          validation_split=0.2,  # Train dataì˜ 20%ë¥¼ Validation dataë¡œ!\n",
    "          callbacks=[es],        # Early Stopping ì ìš©\n",
    "          verbose=1, epochs=50)\n",
    "\n",
    "3) ì˜ˆì¸¡\n",
    "pred_train = model.predict(train_x)\n",
    "pred_test = model.predict(test_x)\n",
    "single_pred_train = pred_train.argmax(axis=1)\n",
    "single_pred_test = pred_test.argmax(axis=1)\n",
    "logi_train_accuracy = accuracy_score(train_y.argmax(axis=1), single_pred_train)\n",
    "logi_test_accuracy = accuracy_score(test_y.argmax(axis=1), single_pred_test)\n",
    "print('íŠ¸ë ˆì´ë‹ ì •í™•ë„ : {:.2f}%'.format(logi_train_accuracy*100))\n",
    "print('í…ŒìŠ¤íŠ¸ ì •í™•ë„ : {:.2f}%'.format(logi_test_accuracy*100))\n",
    "\n",
    "4) í™•ì¸\n",
    "mnist_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "figure = plt.figure(figsize=(20, 10))\n",
    "for i, index in enumerate(np.random.choice(test_x.shape[0], size=15, replace=False)):\n",
    "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    # Display each image\n",
    "    ax.imshow(np.squeeze(test_x[index].reshape([28,-1])), cmap='gray' )\n",
    "    \n",
    "    predict_index = pred_test[index].argmax(axis=0)\n",
    "    true_index = test_y[index].argmax(axis=0)\n",
    "    # Set the title for each image\n",
    "    ax.set_title(f\"{mnist_labels[predict_index]} ({mnist_labels[true_index]})\",\n",
    "                 color=(\"green\" if predict_index == true_index else \"red\"))\n",
    "    \n",
    "5) ì˜¤ë‹µí™•ì¸\n",
    "true_false = (test_y.argmax(axis=1) == single_pred_test)\n",
    "f_id = np.where(true_false == False)[0]\n",
    "f_n = len(f_id)\n",
    "id = f_id[rd.randrange(0,f_n)]\n",
    "print(f'id = {id}' )\n",
    "print(f'ë‹¤ìŒ ê·¸ë¦¼ì€ ìˆ«ì {test_y.argmax(axis=1)[id]} ì…ë‹ˆë‹¤.')\n",
    "print(f'ëª¨ë¸ì˜ ì˜ˆì¸¡ : {single_pred_test[id]}')\n",
    "print(f'ëª¨ë¸ì˜ ì¹´í…Œê³ ë¦¬ë³„ í™•ë¥  : {np.floor(pred_test[id]*100)}')\n",
    "if test_y.argmax(axis=1)[id] == single_pred_test[id] :\n",
    "    print('===============')\n",
    "    print('ì •ë‹µì…ë‹ˆë‹¤')\n",
    "    print('===============')\n",
    "else : \n",
    "    print('===============')\n",
    "    print('í‹€ë ¸ì–´ìš”')\n",
    "    print('===============')\n",
    "plt.imshow(test_x[id].reshape([28,-1]), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "6) í‰ê°€\n",
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe819b5d-4d0f-4b45-af16-e768a4fb3bff",
   "metadata": {},
   "source": [
    "## Modeling : multi-input & Concatenate layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7af6d6-ab4f-49b4-b2fa-3bf9e6b15d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "print(data.DESCR) \n",
    "df_x = pd.DataFrame(x, columns=iris.feature_names)\n",
    "# nullê°’ í™•ì¸ í•„ìš”\n",
    "iris.target_names\n",
    "\n",
    "2) train set, test set êµ¬ë¶„í•˜ê¸°\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(df_x, y, test_size=0.1, random_state=2022)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape\n",
    "\n",
    "ì¶”ê°€) Scaling (ë°ì´í„° ìˆ˜ì¹˜->ë„ˆë¬´ í¸ì°¨ê°€ ì»¤ì„œ(min_max í•„ìš”))\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm_scaler = MinMaxScaler()\n",
    "train_x = mm_scaler.fit_transform(train_x)\n",
    "test_x = mm_scaler.transform(test_x)\n",
    "pd.DataFrame(train_x, columns=iris.feature_names).describe()\n",
    "train_x = pd.DataFrame(train_x, columns=iris.feature_names)\n",
    "test_x = pd.DataFrame(test_x, columns=iris.feature_names)\n",
    "\n",
    "3) lengthë¼ë¦¬, widthë¼ë¦¬\n",
    "print(df_x.columns)\n",
    "tr_x_l = train_x.loc[:, ['sepal length (cm)', 'petal length (cm)'] ]\n",
    "tr_x_w = train_x.loc[:, ['sepal width (cm)', 'petal width (cm)'] ]\n",
    "tr_x_l.shape, tr_x_w.shape\n",
    "te_x_l = test_x.loc[:, ['sepal length (cm)', 'petal length (cm)'] ]\n",
    "te_x_w = test_x.loc[:, ['sepal width (cm)', 'petal width (cm)'] ]\n",
    "\n",
    "4) One-hot Encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "train_y = to_categorical(train_y, 3)\n",
    "test_y = to_categorical(test_y, 3)\n",
    "train_y.shape\n",
    "\n",
    "5-1) Modeling : multi-input & Concatenate layer\n",
    "@ Functional APIë§Œ ê°€ëŠ¥!!!!\n",
    "# 1. ì„¸ì…˜ í´ë¦¬ì–´\n",
    "clear_session()\n",
    "# 2. ë ˆì´ì–´ ì‚¬ìŠ¬ì²˜ëŸ¼ ì—®ê¸° : input 2ê°œ!\n",
    "il_l = Input( shape=(2,) )\n",
    "hl_l = Dense(2, activation=relu)(il_l)\n",
    "il_w = Input( shape=(2,) )\n",
    "hl_w = Dense(2, activation=relu)(il_w)\n",
    "cl = Concatenate()([hl_l, hl_w])\n",
    "ol = Dense(3, activation=softmax)(cl)\n",
    "# 3. ëª¨ë¸ ì‹œì‘ê³¼ ë ì§€ì •\n",
    "model = Model([il_l, il_w], ol)\n",
    "# 4. ëª¨ë¸ ì»´íŒŒì¼\n",
    "model.compile(loss=categorical_crossentropy, metrics=['accuracy'],\n",
    "              optimizer=Adam())\n",
    "model.summary()\n",
    "\n",
    "5-2) Modeling : multi-input & Add layer\n",
    "tr_x_p.shape, train_y.shape\n",
    "keras.backend.clear_session()\n",
    "il_s = keras.layers.Input(shape=(2,))\n",
    "hl_s = keras.layers.Dense(6,activation='swish')(il_s)\n",
    "il_p = keras.layers.Input(shape=(2,))\n",
    "hl_p = keras.layers.Dense(6,activation='swish')(il_p)\n",
    "add_l = keras.layers.Add()([hl_s, hl_p])\n",
    "ol = keras.layers.Dense(3,activation='softmax')(add_l)\n",
    "model = keras.models.Model([il_s,il_p],ol)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "6) ëª¨ë¸ ì‹œê°í™”\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)\n",
    "\n",
    "7) í•™ìŠµ\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n",
    "model.fit([tr_x_l, tr_x_w], train_y, validation_split=0.1, epochs=1000, verbose=1, callbacks=[es])\n",
    "        # ëª¨ë¸ì— ë¶“ëŠ” ìˆœì„œ ì§€ì¼œì•¼ë¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c88908-5b03-4c91-a051-dc4506ec43ed",
   "metadata": {},
   "source": [
    "## ì‹œê°ì§€ëŠ¥ ë”¥ëŸ¬ë‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ddd25f-c347-485b-ab54-d37c107c3a0c",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bd33f-550a-4886-a70d-99e29451d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = io.loadmat(\"notMNIST_small.mat\")\n",
    "\n",
    "# transform data\n",
    "X = data['images']\n",
    "y = data['labels']\n",
    "resolution = 28\n",
    "classes = 10\n",
    "\n",
    "X = np.transpose(X, (2, 0, 1))\n",
    "\n",
    "y = y.astype('int32')\n",
    "X = X.astype('float32') #/ 255.\n",
    "\n",
    "# shape: (sample, x, y, channel)\n",
    "X = X.reshape((-1, resolution, resolution, 1))\n",
    "\n",
    "# looking at data; some fonts are strange\n",
    "i = np.random.randint(0, 18723)\n",
    "print(i)\n",
    "plt.imshow( X[i,:,:,0] )\n",
    "plt.title( \"ABCDEFGHIJ\"[y[i]] )\n",
    "\n",
    "# random letters\n",
    "rows = 6\n",
    "fig, axs = plt.subplots(rows, classes, figsize=(classes, rows))\n",
    "\n",
    "for letter_id in range(10):\n",
    "    letters = X[y == letter_id]\n",
    "    for i in range(rows):\n",
    "        ax = axs[i, letter_id]\n",
    "        ax.imshow(letters[np.random.randint(len(letters)),:,:,0],\n",
    "                  cmap='Greys', interpolation='none')\n",
    "        ax.axis('off')\n",
    "        \n",
    "# splitting data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2022)\n",
    "\n",
    "x_train.shape, y_train.shape\n",
    "\n",
    "max_n, min_n = x_train.max(),x_train.min()\n",
    "\n",
    "x_train = (x_train - min_n)/ (max_n - min_n)\n",
    "x_test = (x_test - min_n)/ (max_n - min_n)\n",
    "\n",
    "len(np.unique(y_train))\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test,10)\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324770b9-89db-4637-81b7-f9369f2567ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "il = keras.layers.Input(shape=(28,28,1,))\n",
    "hl = keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu',)(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "\n",
    "hl = keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu')(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "\n",
    "hl = keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2))(hl)\n",
    "hl = keras.layers.Dropout(.25)(hl)\n",
    "\n",
    "hl = keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "\n",
    "hl = keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "\n",
    "hl = keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2))(hl)\n",
    "hl = keras.layers.Dropout(.25)(ml)\n",
    "\n",
    "hl = keras.layers.Flatten()(hl)\n",
    "hl = keras.layers.Dense(512,)(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl) \n",
    "ol = keras.layers.Dense(10, activation='softmax')(hl)\n",
    "\n",
    "model = keras.models.Model(il,ol)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4a828-c809-4b9f-9b7e-409f10f1c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "il = keras.layers.Input(shape=(28,28,1))\n",
    "hl = keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(il)\n",
    "hl = keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(hl)\n",
    "hl = keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2))(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "hl = keras.layers.Dropout(.25)(hl)\n",
    "\n",
    "hl = keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(hl)\n",
    "hl = keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(hl)\n",
    "hl = keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2))(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "hl = keras.layers.Dropout(.25)(hl)\n",
    "\n",
    "hl = keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(hl)\n",
    "hl = keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(hl)\n",
    "hl = keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(hl)\n",
    "hl = keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2))(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "hl = keras.layers.Dropout(.25)(hl)\n",
    "\n",
    "hl = keras.layers.Flatten()(hl)\n",
    "hl = keras.layers.Dense(1024, activation='relu')(hl)\n",
    "hl = keras.layers.Dense(1024, activation='relu')(hl)\n",
    "hl = keras.layers.Dense(1024, activation='relu')(hl)\n",
    "ol = keras.layers.Dense(10, activation='softmax')(hl)\n",
    "\n",
    "model = keras.models.Model(il ,ol)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(0.01), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3af6c7-0138-4488-acee-7d699c10745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True)\n",
    "model.fit(x_train, y_train,, verbose=1, validation_split=.2, batch_size=1024, epochs=100, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e329d13-50dd-461f-8122-7937a58416e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "pred_array = np.zeros(shape=(y_pred.shape[0], y_pred.shape[1]))\n",
    "idx = 0\n",
    "\n",
    "for arr_val in y_pred :\n",
    "    # print(arr_val)\n",
    "    pred_array[idx][arr_val.argmax()] = 1\n",
    "    idx += 1\n",
    "    \n",
    "pred_array.shape\n",
    "\n",
    "@ ì„±ëŠ¥í‰ê°€\n",
    "from sklearn.metrics import accuracy_score\n",
    "print( f'{accuracy_score(y_test, pred_array):.4f}' )\n",
    "\n",
    "@ ë¬¸ì ì´ë¯¸ì§€ ì‹œê°í™”\n",
    "import random as rd\n",
    "character = {0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:'F', 6:'G', 7:'H', 8:'I', 9:'J'}\n",
    "\n",
    "rand_n = rd.randrange(0, 3744)\n",
    "\n",
    "print(f'id = {rand_n}')\n",
    "print(f'ì‹¤ì œ ë¬¸ì : {character[y_test[rand_n].argmax()]}')\n",
    "print(f'ëª¨ë¸ì˜ ë¬¸ì ì˜ˆì¸¡ : {character[y_pred[rand_n].argmax()]}' )\n",
    "print(f'ëª¨ë¸ì˜ ë¬¸ìë³„ ì˜ˆì¸¡ í™•ë¥  : {np.round(y_pred[rand_n]*100)}')\n",
    "# print(f'ëª¨ë¸ì˜ ë¬¸ìë“¤ ì´ í™•ë¥  : {sum(np.round(y_pred[rand_n]*100))}')\n",
    "\n",
    "print('====================================================')\n",
    "\n",
    "if y_test[rand_n].argmax() == y_pred[rand_n].argmax() :\n",
    "    print('ì •ë‹µ')\n",
    "else :\n",
    "    print('ì˜¤ë‹µ')\n",
    "\n",
    "print('====================================================')\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(x_test[rand_n].reshape(28, 28), cmap='gray')\n",
    "plt.title(\"ABCDEFGHIJ\"[y_test[rand_n].argmax()] )\n",
    "plt.show()\n",
    "\n",
    "# í‹€ë¦° ë¬¸ìë§Œ í™•ì¸\n",
    "t_f = ( y_test.argmax(axis=1) == y_pred.argmax(axis=1) )\n",
    "false_id = np.where(t_f==False)[0]\n",
    "false_n = len(false_id)\n",
    "\n",
    "id = false_id[rd.randrange(0, false_n)]\n",
    "\n",
    "print(f'id = {id}')\n",
    "print(f'ì‹¤ì œ ë¬¸ì : {character[y_test[id].argmax()]}')\n",
    "print(f'ëª¨ë¸ì˜ ë¬¸ì ì˜ˆì¸¡ : {character[y_pred[id].argmax()]}' )\n",
    "print(f'ëª¨ë¸ì˜ ë¬¸ìë³„ ì˜ˆì¸¡ í™•ë¥  : {np.round(y_pred[id]*100)}')\n",
    "# print(f'ëª¨ë¸ì˜ ë¬¸ìë“¤ ì´ í™•ë¥  : {sum(np.round(y_pred[rand_n]*100))}')\n",
    "\n",
    "print('====================================================')\n",
    "\n",
    "if y_test[id].argmax() == y_pred[id].argmax() :\n",
    "    print('ì •ë‹µ')\n",
    "else :\n",
    "    print('ì˜¤ë‹µ')\n",
    "\n",
    "print('====================================================')\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(x_test[id].reshape(28, 28), cmap='gray')\n",
    "plt.title(\"ABCDEFGHIJ\"[y_pred[id].argmax()] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc456f-3a99-42de-b0e2-7f01c39f4370",
   "metadata": {},
   "source": [
    "### ë™ì˜ìƒCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838adee-514a-4c66-b0f9-0fda84315f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°.\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839bd74c-adb0-424b-ab8a-f7c80192c633",
   "metadata": {},
   "source": [
    "1) ì½”ë© ì‚¬ìš© ì‹œ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03339166-1cd2-43d1-b971-52c49af0379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd1a77-c415-4c7d-83d2-ffd6e2eeebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(TUTORIAL_PATH + \"/tutorial.mp4\")\n",
    "video.isOpened()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec18179-c804-4f34-9963-371c1ce99650",
   "metadata": {},
   "source": [
    "2. ê²½ë¡œ í™•ì¸í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a44879-e174-405d-8a08-abe6f7239bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT_PATH í™•ì¸ \n",
    "import os\n",
    "\n",
    "# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë‚´ í”„ë¡œì íŠ¸ ì••ì¶•í•´ì œëœ ì˜ì—­ (êµ¬ê¸€ ë“œë¼ì´ë¸Œ ìµœìƒìœ„ì— ì••ì¶•í•´ì œ ì‹œ ê·¸ëŒ€ë¡œ ì‹¤í–‰ ìˆ˜ì • X)\n",
    "WORK_SPACE = \"\"\n",
    "\n",
    "if os.getcwd() == '/content' :\n",
    "  # êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì‚¬ìš© ì‹œ \n",
    "  ROOT_PATH = \"/content/drive/MyDrive/\"+WORK_SPACE+\"/AIVLE3rd_individual\"\n",
    "else :\n",
    "  ROOT_PATH = os.path.abspath('..')\n",
    "\n",
    "# ëª¨ë¸ ì˜ˆì¸¡ì„ ìœ„í•œ test ë°ì´í„°ê°€ ì €ì¥ë˜ëŠ” ê²½ë¡œ\n",
    "TEST_PATH = ROOT_PATH + \"/test\"\n",
    "# í…ŒìŠ¤íŠ¸ ì˜ìƒ test.mp4 ê²½ë¡œ \n",
    "\n",
    "TEST_VIDEO = TEST_PATH + \"/test.mp4\"\n",
    "\n",
    "# ëª¨ë¸ ì˜ˆì¸¡ì„ ìœ„í•´ í…ŒìŠ¤íŠ¸ ì˜ìƒì„ í”„ë ˆì„ ì´ë¯¸ì§€ë¡œ ìë¥´ê³  ì €ì¥í•˜ëŠ” ê²½ë¡œ (testGenerator ìƒì„± ì‹œ PATH )   \n",
    "TEST_IMAGE = TEST_PATH + \"/image\"\n",
    "\n",
    "# ëª¨ë¸(.h5) íŒŒì¼ì´ ì €ì¥ëœ ê²½ë¡œ (ë³¸ì¸ì´ ìƒì„±í•œ ëª¨ë¸)\n",
    "MODEL_PATH = ROOT_PATH + \"/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ffa93-58c1-45a5-8ec1-13048331848c",
   "metadata": {},
   "source": [
    "3) test data ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee20513-07ba-465a-a4fb-fa525112f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import gdown\n",
    "import shutil\n",
    "import zipfile\n",
    " \n",
    "google_path = 'https://drive.google.com/uc?id='\n",
    "file_id = \"10iKCHLPx-YFgkJcDqCgJPl83ZKYHvXmr\"\n",
    "output_name = 'test.zip'\n",
    "\n",
    "# íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "gdown.download(google_path+file_id,output_name,quiet=False)\n",
    "\n",
    "# íŒŒì¼ ìœ„ì¹˜ ì´ë™\n",
    "shutil.move(\"./\" + output_name, ROOT_PATH)\n",
    "\n",
    "zip_file = ROOT_PATH + \"/\" + output_name\n",
    "\n",
    "# ì••ì¶•í•´ì œ \n",
    "with zipfile.ZipFile(zip_file) as z:\n",
    "    z.extractall(ROOT_PATH)\n",
    "\n",
    "os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f4b0f2-2b64-4fe9-bcb1-b323e1b0b0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™˜ê²½ í™•ì¸í•˜ê¸°\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee88abf-0502-41ee-9f70-be30d66f6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_width = video.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "video_height = video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "video_length = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "video_fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(\"ê°€ë¡œ :\", video_width)\n",
    "print(\"ì„¸ë¡œ :\", video_height)\n",
    "print(\"ì´ í”„ë ˆì„ ìˆ˜ :\" , video_length)\n",
    "print(\"FPS :\", video_fps)\n",
    "print(\"ì˜ìƒ ê¸¸ì´ : %d ì´ˆ :\"  %round(video_length/video_fps))\n",
    "\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd0bc8-39df-4714-8181-435200064a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì°¸ì¡° \n",
    "for x_data, t_data in train_generator:\n",
    "\n",
    "    print(x_data.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb449ef2-80f9-4486-a803-091a7ce703ba",
   "metadata": {},
   "source": [
    "* ì´ë¯¸ì§€ ë§Œë“¤ê¸°(CROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b24df3-31c9-473f-b862-a3e6a6564528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ìŠµí•´ë³´ì„¸ìš”.\n",
    "test_IMAGE = \"/content/drive/MyDrive/AIVLE3rd_individual/train/samang\"\n",
    "\n",
    "TIME_MEASUERMENT_UNIT = 1 #TIME MEASUREMENT UNITì„ í†µí•´ ëª‡ ì´ˆ ë‹¨ìœ„ë¡œ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ì§€ ì„ íƒ\n",
    "\n",
    "if not os.path.exists(test_IMAGE):\n",
    "  os.mkdir(test_IMAGE)\n",
    "\n",
    "video = cv2.VideoCapture(\"/content/drive/MyDrive/AIVLE3rd_individual/video/train/220206_samang.mp4\")\n",
    "\n",
    "while video.isOpened():\n",
    "  ret,frame = video.read()\n",
    "  if ret:\n",
    "    # í˜„ì¬ í”„ë ˆì„ ìœ„ì¹˜ (msec) \n",
    "    frame_sec = video.get(cv2.CAP_PROP_POS_MSEC)/1000\n",
    "    print(frame_sec%1)\n",
    "    # if (frame_sec % TIME_MEASUERMENT_UNIT == 0.6796875):\n",
    "    #  filename = test_IMAGE + \"/\" + str(round(frame_sec)) + \".jpg\"\n",
    "    #  cv2.imwrite(filename, frame) \n",
    "  else:\n",
    "    break\n",
    "\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba312c-9857-4099-8bd8-76d18226f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_IMAGE = \"/content/drive/MyDrive/AIVLE3rd_individual/train/boss\"\n",
    "\n",
    "# TIME_MEASUERMENT_UNIT = 1 #TIME MEASUREMENT UNITì„ í†µí•´ ëª‡ ì´ˆ ë‹¨ìœ„ë¡œ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ì§€ ì„ íƒ\n",
    "\n",
    "if not os.path.exists(test_IMAGE):\n",
    "  os.mkdir(test_IMAGE)\n",
    "\n",
    "video = cv2.VideoCapture(\"/content/drive/MyDrive/AIVLE3rd_individual/train/06boss\")\n",
    "\n",
    "count = 0\n",
    "\n",
    "while video.isOpened():\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    if ret:\n",
    "        if(int(video.get(1)) % video.get(cv2.CAP_PROP_FPS) == 0):\n",
    "            count += 1\n",
    "            # filename = test_IMAGE + \"/\" + str(count) + \".jpg\"\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # í‘ë°± ë³€í™˜\n",
    "            cv2.imwrite(filename, frame[55:80, 35:165]) # ì½”ë„ˆ ë¡œê³ ë§Œ í¬ë¡­í•´ì„œ ì €ì¥\n",
    "            # cv2.imwrite(filename, frame)\n",
    "    else: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1875e6-c30a-4c8f-ab14-314d54f6667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_IMAGE = \"/content/drive/MyDrive/AIVLE3rd_individual/train/20samang\"\n",
    "\n",
    "TIME_MEASUERMENT_UNIT = 1 #TIME MEASUREMENT UNITì„ í†µí•´ ëª‡ ì´ˆ ë‹¨ìœ„ë¡œ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ì§€ ì„ íƒ\n",
    "\n",
    "if not os.path.exists(test_IMAGE):\n",
    "  os.mkdir(test_IMAGE)\n",
    "\n",
    "video = cv2.VideoCapture(\"/content/drive/MyDrive/AIVLE3rd_individual/video/train/220220_samang.mp4\")\n",
    "\n",
    "count = 0\n",
    "\n",
    "while video.isOpened():\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    if ret:\n",
    "        if(round(int(video.get(1)) % video.get(cv2.CAP_PROP_FPS)) == 0):\n",
    "            count += 1\n",
    "            filename = test_IMAGE + \"/\" + str(count) + \".jpg\"\n",
    "            # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # í‘ë°± ë³€í™˜\n",
    "            # cv2.imwrite(filename, frame[55:80, 35:165]) # ì½”ë„ˆ ë¡œê³ ë§Œ í¬ë¡­í•´ì„œ ì €ì¥\n",
    "            cv2.imwrite(filename, frame)\n",
    "    else: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb2717-f177-4980-9849-20b6e28841e0",
   "metadata": {},
   "source": [
    "### CROP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a91c9-1968-45fa-8c41-9acf6c60f912",
   "metadata": {},
   "source": [
    "* â‘ 'ë§¤ë‰´ì–¼í•˜ê²Œ ì¼ì¼ì´ í™•ì¸í•˜ëŠ” ë°©ë²•'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ae076-b09d-4450-a591-43becea6812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ìŠµí•´ë³´ì„¸ìš”.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class_map = {\n",
    "    0: 'jung',\n",
    "    1: 'park',    \n",
    "    2: 'sam',\n",
    "    3: 'sayuri',\n",
    "}\n",
    "\n",
    "image_class = glob.glob(IMAGE_PATH + \"*/*\")\n",
    "image_class.sort()\n",
    "\n",
    "fig, axes = plt.subplots(4, 5,figsize=(16, 9))\n",
    "class_count = 0 \n",
    "for c in image_class:  \n",
    "  for i in range(5):\n",
    "    original_image = cv2.imread(c + \"/\" + str(i*200) + \".jpg\")\n",
    "    rgb_image = cv2.cvtColor(original_image,cv2.COLOR_BGR2RGB)\n",
    "    # íŠ¹ì • ì˜ì—­ë§Œ ì˜ë¼ì„œ ì´ë¯¸ì§€ ë¹„êµ\n",
    "    cropped_image = rgb_image[0:96, 0:320]\n",
    "    axes[class_count, i].imshow(cropped_image)\n",
    "    axes[class_count, i].set_title(class_map[class_count] , fontsize=15)    \n",
    "    axes[class_count, i].axis('off')\n",
    "  class_count += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f01bb7-bcae-4650-ab05-57600f0c64fe",
   "metadata": {},
   "source": [
    "* â‘¡'ì´ë¯¸ì§€ ì† ë™ì¼í•œ ê·œì¹™ì„ ì°¾ì•„ë‚´ëŠ” ë°©ë²•'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c4d9a-c42a-4400-b60d-966eb5d4cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Tip] \n",
    "# ì´ë¯¸ì§€ ì „ì²´ë¥¼ í•™ìŠµí•  í•„ìš”ê°€ ì—†ë‹¤. \n",
    "# ì½”ë„ˆë¥¼ êµ¬ë¶„í•˜ëŠ”ë° í•„ìš”í•œ ì˜ì—­ì€ ì¢Œì¸¡ ìƒë‹¨ì— ê³ ì •ë˜ì–´ ìˆìŒìœ¼ë¡œ í•´ë‹¹ ì˜ì—­ë§Œ ì˜ë¼ë‚´ì„œ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥\n",
    "\n",
    "def croppedImage(IMAGE_PATH):\n",
    "  #ì´ë¯¸ì§€ë¥¼ ì˜ë¼ë‚¼ ì˜ì—­ \n",
    "  bounding_box = {\n",
    "      'start_x' : 0, \n",
    "      'start_y' : 0,\n",
    "      'end_x' : 320,    \n",
    "      'end_y' : 96\n",
    "  }\n",
    "  from tqdm.auto import tqdm, trange\n",
    "  image_list = glob.glob(IMAGE_PATH + '/*/*.jpg')\n",
    "  for image in tqdm(image_list, desc=\"IMAGE CROP PROGRESS\"):\n",
    "    original_image = cv2.imread(image)\n",
    "    cropped_image = original_image[bounding_box['start_y']:bounding_box['end_y'], bounding_box['start_x']:bounding_box['end_x']]    \n",
    "    cv2.imwrite(image, cropped_image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc2481-4b46-4fc9-95a2-470d9db0f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "croppedImage(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a373b77-419a-4b9d-91e2-0eaec85bfe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = ROOT_PATH + \"/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df711c9c-24bf-408c-b46c-2043e0d407e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copytree(IMAGE_PATH, TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72fb6c4-e9e1-42c0-8320-3646609c4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë§Œì•½ logo ë¶€ë¶„ë§Œ cropí•˜ì‹ ë‹¤ë©´ êµ³ì´ flipì„ ì´ìš©í•´ì„œ ë°ì´í„°ë¥¼ ì¦ê°•í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243eaa7c-cbf7-4a6e-9a8b-86609b38c00d",
   "metadata": {},
   "source": [
    "4) ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c9bc9e-cf34-4b03-bd4f-02a0e9d29b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 96\n",
    "img_width = 320\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. /255,\n",
    "    validation_split=0.2,\n",
    "    # rotation_range=40,\n",
    "    # width_shift_range=0.2,\n",
    "    # height_shift_range=0.2,\n",
    "    # zoom_range=0.2,\n",
    "    # horizontal_flip=True,\n",
    "    # fill_mode='nearest'\n",
    ")\n",
    "# train_genrator ìƒì„±\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    batch_size=16,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "   )\n",
    "\n",
    "# validation_generator ìƒì„± (trainì™€ ë™ì¼í•˜ê²Œ í•´ì•¼í•¨)\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    batch_size= batch_size,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c6919-874d-4fff-80ea-8c7f1d82c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preprocessing(TEST_VIDEO):\n",
    "  from tensorflow import keras\n",
    "  from keras.preprocessing.image import ImageDataGenerator\n",
    "  import os\n",
    "  import cv2\n",
    "\n",
    "  TEST_IMAGE = TEST_PATH + \"/image\"\n",
    "\n",
    "  TIME_MEASUERMENT_UNIT =1 #TIME MEASUREMENT UNITì„ í†µí•´ ëª‡ ì´ˆ ë‹¨ìœ„ë¡œ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ì§€ ì„ íƒ\n",
    "\n",
    "  if not os.path.exists(TEST_IMAGE):\n",
    "    os.mkdir(TEST_IMAGE)\n",
    "\n",
    "  video = cv2.VideoCapture(TEST_PATH + \"/test.mp4\")\n",
    "\n",
    "  while video.isOpened():\n",
    "    ret,frame = video.read()\n",
    "    if ret:\n",
    "      # í˜„ì¬ í”„ë ˆì„ ìœ„ì¹˜ (msec) \n",
    "      frame_sec = video.get(cv2.CAP_PROP_POS_MSEC)/1000\n",
    "      if frame_sec.is_integer():\n",
    "        if (frame_sec % TIME_MEASUERMENT_UNIT == 0):\n",
    "          filename = TEST_IMAGE + \"/\" + str(round(frame_sec)) + \".jpg\"\n",
    "          cv2.imwrite(filename, frame) \n",
    "    else:\n",
    "      break\n",
    "\n",
    "  video.release()\n",
    "\n",
    "  batch_size=8\n",
    "  img_height=240\n",
    "  img_width=427\n",
    "\n",
    "  test_datagen = ImageDataGenerator(\n",
    "    rescale=1. /255,)\n",
    "  \n",
    "# test_genrator ìƒì„±\n",
    "  test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_PATH,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode='categorical',\n",
    "    subset='test',\n",
    "    )      \n",
    "\n",
    "  return test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e92e06-f8ba-4e36-b606-02a583212958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ í™•ì¸ (ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”.) - Found 1220 images belonging to 1 classes. ì™€ ê°™ì€ ë©”ì‹œì§€ê°€ ì¶œë ¥ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. \n",
    "test_generator = my_preprocessing(TEST_VIDEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942ce55-3a9f-4a73-b2b5-ea8e7b2cc4ce",
   "metadata": {},
   "source": [
    "5) ëª¨ë¸ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81450e10-cc92-416e-9a2f-fa01d29fae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapeì°ì–´ë³´ê¸°\n",
    "from PIL import Image\n",
    "\n",
    "imagel = Image.open('/content/drive/MyDrive/AIVLE3rd_individual/train/park/1000.jpg')\n",
    "imagel.show()\n",
    "imagl_size = imagel.size\n",
    "print(imagl_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17673475-52bf-4a63-9e54-143ec127578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2D, MaxPooling2D ì¡°í•©ìœ¼ë¡œ ì¸µì„ ìŒ“ìŠµë‹ˆë‹¤. ì²«ë²ˆì§¸ ì…ë ¥ì¸µì˜ input_shapeì€ imageDataGenerator target sizeë¡œ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(img_height, img_width,3,)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu',))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(.25))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100,))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16597587-2290-4976-9035-35bdf5f06db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[img_height, img_width,3,]))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Adding convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Step 4 - Full Connection\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "# Step 5 - Output Layer\n",
    "cnn.add(tf.keras.layers.Dense(units=8, activation='softmax'))\n",
    "\n",
    "# Compiling the CNN\n",
    "cnn.compile(optimizer = 'adam', \n",
    "            loss = 'categorical_crossentropy', \n",
    "            metrics = ['accuracy'])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e54641-1d04-4cb2-bdee-97fe54188108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "MODEL_PATH = '/content/drive/MyDrive/AIVLE3rd_individual/weights'\n",
    "# early_stopping \n",
    "cp = ModelCheckpoint(filepath= MODEL_PATH, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "\n",
    "# early_stopping\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=20,\n",
    "                    epochs=200,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=10,\n",
    "                    callbacks=[cp,es], \n",
    "                    batch_size=10,)\n",
    "\n",
    "@ ëª¨ë¸ì €ì¥í•˜ê¸°\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('/content/drive/MyDrive/AIVLE3rd_individual/model/[ê°œì¸]ë¯¸ë‹ˆí”„ë¡œì íŠ¸3ì°¨_A021087.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56d1b0-3078-4449-9c90-1233860fe5ad",
   "metadata": {},
   "source": [
    "* ëª¨ë¸ SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad56627-fd47-4d79-a8f3-726387f095b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "name = 'ê¹€ë‚˜í˜„'\n",
    "model.save(MODEL_PATH + '/ë¯¸ë‹ˆí”„ë¡œì íŠ¸3ì°¨_' + name + '.h5')\n",
    "print(MODEL_PATH + '/ë¯¸ë‹ˆí”„ë¡œì íŠ¸3ì°¨_' + name + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a5665-9eba-4ab7-9f50-cb2b30bd8393",
   "metadata": {},
   "source": [
    "6) ëª¨ë¸ì ìš© ì˜ˆì¸¡ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da179aab-91a4-4a19-861e-b70bbd25f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model_predict(test_generator, model):\n",
    "  import datetime as df\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "\n",
    "  class_map = {\n",
    "    0: 'jung',\n",
    "    1: 'park',    \n",
    "    2: 'sam',\n",
    "    3: 'sayuri',\n",
    "  }\n",
    "\n",
    "  keys = ['predict_v1','predict_v2','predict_v3']\n",
    "  predict_verList = dict.fromkeys(keys)\n",
    "\n",
    "  predict = model.predict(test_generator)\n",
    "\n",
    "  np.set_printoptions(formatter={'float':lambda x:\"{0:0.3f}\".format(x)})\n",
    "  predict_v1 = pd.DataFrame({\n",
    "      'file' : test_generator.filenames,\n",
    "      class_map[0] : predict[:,0],\n",
    "      class_map[1] : predict[:,1],\n",
    "      class_map[2] : predict[:,2],\n",
    "      class_map[3] : predict[:,3],\n",
    "      })\n",
    "  \n",
    "  pd.options.display.float_format = '{:.2f}'.format\n",
    "  predict_verList['predict_v1'] = predict_v1\n",
    "\n",
    "\n",
    "  return predict_verList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55c6fd-37a9-4645-ba80-8c76c6f51f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('/content/drive/MyDrive/AIVLE3rd_individual/model/[á„€á…¢á„‹á…µá†«]á„†á…µá„‚á…µá„‘á…³á„…á…©á„Œá…¦á†¨á„á…³3á„á…¡_A021087.h5', \n",
    "                                custom_objects=None, compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732517c9-ff74-4c51-a7b1-21ba7385e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_verList = my_model_predict(test_generator, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5b0fe-1215-42c9-b4de-ba20fc750ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ëª¨ë¸ê²°ê³¼_v1')\n",
    "predict_verList['predict_v1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede200a-525c-41ed-a79d-26350e318f38",
   "metadata": {},
   "source": [
    "7) ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53f474-0ac5-4048-8381-5e54157c00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pred = model.predict(validation_generator)\n",
    "\n",
    "pred = np.argmax(pred, axis = 1 )\n",
    "\n",
    "f1 = f1_score(validation_generator.labels, pred, average = None)\n",
    "print('f1 score :', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b43e5-6962-40e3-bb9b-544dcd3cb59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "if not isinstance(history, dict):\n",
    "    history = history.history\n",
    "\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.title('Accuracy : Training vs Validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "if not isinstance(history, dict):\n",
    "    history = history.history\n",
    "\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('Loss : Training vs Validation')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b244a83-691d-4558-9251-8e9552c8c508",
   "metadata": {},
   "source": [
    "### UltraLytics YOLO v3 Image Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d32db-9414-4d3c-bbfc-1086f5e495d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) UltraLytics gitì—ì„œ ë³µì‚¬í•˜ê¸°\n",
    "!git clone https://github.com/ultralytics/yolov3.git\n",
    "\n",
    "2) yolov3 í´ë” ì´ë™ ë° requirements.txt ë‚´ë¶€ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!cd yolov3; pip install -r /content/yolov3/requirements.txt\n",
    "\n",
    "3) Image Detection\n",
    "3-1) ì˜ˆì œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ => !wget -O [ì €ì¥í•  íŒŒì¼ëª…] [íŒŒì¼ ì£¼ì†Œ]\n",
    "!wget -O /content/yolov3/data/images/14th_street.jpg\\\n",
    "https://raw.githubusercontent.com/DrKAI/image/main/14th_Street_2005.jpg\n",
    "\n",
    "3-2) COCO Dataset(ì„±ëŠ¥ì§€í‘œ)ìœ¼ë¡œ pretrained ëœ weights ë‹¤ìš´ë¡œë“œ [weightsê°€ ì—†ìœ¼ë©´ ìë™ ë‹¤ìš´ë¡œë“œ] \n",
    "=> !mkdir [ê²½ë¡œ/ë””ë ‰í† ë¦¬ ëª…]\n",
    "=> pretrained weights ë‹¤ìš´ë¡œë“œ\n",
    "!mkdir /content/yolov3/pretrained\n",
    "!wget -O /content/yolov3/pretrained/yolov3-tiny.pt\\\n",
    "https://github.com/ultralytics/yolov3/releases/download/v9.6.0/yolov3-tiny.pt\n",
    "\n",
    "3-3) detect.pyë¥¼ pythonìœ¼ë¡œ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ìˆ˜í–‰ => ëª…ë ¹ì–´ ë„ì›€ë§ : !cd yolo3; python detect.py -h\n",
    "!cd yolov3; python detect.py -h\n",
    "!cd yolov3; python detect.py \\\n",
    "    --weights '/content/yolov3/pretrained/yolov3.pt' \\\n",
    "    --source '/content/yolov3/data/images' \\\n",
    "    --project '/content/yolov3/detected' \\\n",
    "    --name 'images' \\\n",
    "    --img 640 \\\n",
    "    --conf-thres 0.1 \\\n",
    "    --iou-thres 0.4 \\\n",
    "    --line-thickness 2 \\\n",
    "    --exist-ok \\# ë®ì–´ì“°ê¸°\n",
    "    --device CPU\n",
    "\n",
    "@ Detect Image ì‚´í´ë³´ê¸°\n",
    "from IPython.display import Image\n",
    "from google.colab import files\n",
    "\n",
    "# Image(filename=[íŒŒì¼ ê²½ë¡œ])\n",
    "Image(filename='/content/yolov3/detected/images/14th_street.jpg', width=850)\n",
    "\n",
    "# files.download(filename=[íŒŒì¼ ê²½ë¡œ])\n",
    "files.download(filename='/content/yolov3/detected/images/14th_street.jpg')\n",
    "\n",
    "# zip\n",
    "!zip -r /content/detected_image.zip /content/yolov3/detected/images2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b633ee0-0881-4d27-b266-086b66b5e510",
   "metadata": {},
   "source": [
    "### UltraLytics_YOLOv3_VideoDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5675720-bd1b-4f39-83fa-f24b59aae4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov3.git\n",
    "\n",
    "!cd yolov3; pip install -r /content/yolov3/requirements.txt\n",
    "\n",
    "!mkdir /content/yolov3/data/videos\n",
    "!wget -O /content/yolov3/data/videos/noway.mp4\\\n",
    "https://github.com/DrKAI/image/raw/main/No_Way_This_Happened.mp4\n",
    "\n",
    "!mkdir /content/yolov3/pretrained\n",
    "!wget -O /content/yolov3/pretrained/yolov3.pt\\\n",
    "https://github.com/ultralytics/yolov3/releases/download/v9.6.0/yolov3.pt\n",
    "\n",
    "# !cd yolov3; python detect.py -h\n",
    "!cd yolov3; python detect.py \\\n",
    "    --weights '/content/yolov3/pretrained/yolov3.pt' \\\n",
    "    --source '/content/yolov3/data/videos/' \\\n",
    "    --project '/content/yolov3/detected' \\\n",
    "    --name 'videos' \\\n",
    "    --img 640 \\\n",
    "    --conf-thres 0.5 \\\n",
    "    --iou-thres 0.4 \\\n",
    "    --line-thickness 2 \\\n",
    "    --exist-ok\n",
    "    # --device CPU\n",
    "    \n",
    "from google.colab import files\n",
    "## colabì€ ë©€í‹° ë‹¤ìš´ë¡œë“œ ì§€ì›X\n",
    "## í´ë” ì••ì¶•í•˜ì—¬ íŒŒì¼ í•˜ë‚˜ë¡œ ë§Œë“¤ê³  ë‹¤ìš´ë¡œë“œ\n",
    "!zip -r /content/detected_videos.zip /content/yolov3/detected/videos/\n",
    "\n",
    "files.download(filename='/content/yolov3/detected/videos/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f69e8-44c7-408a-b75b-1453c5f2fb68",
   "metadata": {},
   "source": [
    "### UltraLytics_YOLOv5_CustomData_ImageDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d2333-3ea6-476f-9ec0-068da8150e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5\n",
    "    \n",
    "!cd yolov5; pip install -r requirements.txt\n",
    "\n",
    "@ Image Detection\n",
    "1) ì‚¬ì „ ì‘ì—…ëœ CustomData yaml ë‹¤ìš´ë¡œë“œ\n",
    "!wget -O /content/yolov5/data/street.yaml\\\n",
    "https://raw.githubusercontent.com/DrKAI/CV/main/street_example.yaml\n",
    "\n",
    "2) pretrained ëœ weights ë‹¤ìš´ë¡œë“œ => weightsê°€ ì—†ìœ¼ë©´ ìë™ ë‹¤ìš´ë¡œë“œ\n",
    "!mkdir /content/yolov5/pretrained\n",
    "!wget -O /content/yolov5/pretrained/yolov5s.pt\\\n",
    "https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt\n",
    "\n",
    "3) trainìš© ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ\n",
    "!mkdir /content/datasets; mkdir /content/datasets/street\n",
    "!mkdir /content/datasets/street/images; mkdir /content/datasets/street/images/train\n",
    "!mkdir /content/datasets/street/labels; mkdir /content/datasets/street/labels/train\n",
    "\n",
    "!wget -O /content/street_images1.zip https://github.com/DrKAI/CV/raw/main/street_images1.zip\n",
    "!wget -O /content/street_images2.zip https://github.com/DrKAI/CV/raw/main/street_images2.zip\n",
    "!wget -O /content/street_images3.zip https://github.com/DrKAI/CV/raw/main/street_images3.zip\n",
    "\n",
    "!wget -O /content/street_labels1.zip https://github.com/DrKAI/CV/raw/main/street_labels1.zip\n",
    "!wget -O /content/street_labels2.zip https://github.com/DrKAI/CV/raw/main/street_labels2.zip\n",
    "!wget -O /content/street_labels3.zip https://github.com/DrKAI/CV/raw/main/street_labels3.zip\n",
    "    \n",
    "!unzip /content/street_images1.zip -d /content/datasets/street/images/train\n",
    "!unzip /content/street_images2.zip -d /content/datasets/street/images/train\n",
    "!unzip /content/street_images3.zip -d /content/datasets/street/images/train\n",
    "\n",
    "!unzip /content/street_labels1.zip -d /content/datasets/street/labels/train\n",
    "!unzip /content/street_labels2.zip -d /content/datasets/street/labels/train\n",
    "!unzip /content/street_labels3.zip -d /content/datasets/street/labels/train\n",
    "\n",
    "4) train.py ì‹¤í–‰\n",
    "!cd yolov5; python train.py -h\n",
    "!cd yolov5; python train.py \\\n",
    "    --data '/content/yolov5/data/street.yaml' \\\n",
    "    --cfg '/content/yolov5/models/yolov5s.yaml' \\\n",
    "    --weights '/content/yolov5/pretrained/yolov5s.pt' \\\n",
    "    --epochs 1000 \\\n",
    "    --patience 7 \\\n",
    "    --img 640 \\\n",
    "    --project 'trained' \\\n",
    "    --name 'train_street' \\\n",
    "    --exist-ok\n",
    "    # --device cpu\n",
    "\n",
    "5) detectìš© ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ\n",
    "!wget -O /content/yolov5/data/images/street01.jpeg https://github.com/DrKAI/image/raw/main/001.jpeg\n",
    "!wget -O /content/yolov5/data/images/street02.jpg https://github.com/DrKAI/image/raw/main/14th_Street_2005.jpg\n",
    "!wget -O /content/yolov5/data/images/street03.jpg https://github.com/DrKAI/image/raw/main/street02.jpg\n",
    "!wget -O /content/yolov5/data/images/street04.jpg https://github.com/DrKAI/image/raw/main/street03.jpg\n",
    "!wget -O /content/yolov5/data/images/street05.jpg https://github.com/DrKAI/image/raw/main/street04.jpg\n",
    "!wget -O /content/yolov5/data/images/street06.jpg https://github.com/DrKAI/image/raw/main/street05.jpg\n",
    "\n",
    "\n",
    "6) detect.py ì‹¤í–‰\n",
    "!cd yolov5; python detect.py -h\n",
    "\n",
    "!cd yolov5; python detect.py \\\n",
    "    --weights '/content/yolov5/trained/train_street/weights/best.pt' \\\n",
    "    --source '/content/yolov5/data/images/' \\\n",
    "    --project '/content/yolov5/detected' \\\n",
    "    --name 'images' \\\n",
    "    --img 640 \\\n",
    "    --conf-thres 0.25 \\\n",
    "    --iou-thres 0.5 \\\n",
    "    --line-thickness 2 \\\n",
    "    --exist-ok \n",
    "    # --device CPU\n",
    "\n",
    "7) Detect Image ì‚´í´ë³´ê¸°\n",
    "from IPython.display import Image\n",
    "from google.colab import files\n",
    "\n",
    "Image(filename='/content/yolov5/detected/images/street01.jpeg', width=640)\n",
    "\n",
    "## colabì€ ë©€í‹° ë‹¤ìš´ë¡œë“œë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤\n",
    "## í´ë”ë¥¼ ì••ì¶•í•˜ì—¬ íŒŒì¼ í•˜ë‚˜ë¡œ ë§Œë“¤ê³  ë‹¤ìš´ë¡œë“œ í•œë‹¤\n",
    "\n",
    "!zip -r /content/detected_images.zip /content/yolov5/detected/images\n",
    "\n",
    "files.download(filename='/content/detected_images.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0706d8f3-326a-4946-891b-a470b77cfac7",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57fddd-9cc4-40fe-b90a-47dd69c00c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests ì´ìš©\n",
    "ë°›ì•„ì˜¤ëŠ” ë¬¸ìì—´ì— ë”°ë¼ ë‘ê°€ì§€ ë°©ë²•ìœ¼ë¡œ êµ¬ë¶„\n",
    "json ë¬¸ìì—´ë¡œ ë°›ì•„ì„œ íŒŒì‹±í•˜ëŠ” ë°©ë²• : ì£¼ë¡œ ë™ì  í˜ì´ì§€ í¬ë¡¤ë§í• ë•Œ ì‚¬ìš©\n",
    "html ë¬¸ìì—´ë¡œ ë°›ì•„ì„œ íŒŒì‹±í•˜ëŠ” ë°©ë²• : ì£¼ë¡œ ì •ì  í˜ì´ì§€ í¬ë¡¤ë§í• ë•Œ ì‚¬ìš©\n",
    "selenium ì´ìš©\n",
    "ë¸Œë¼ìš°ì ¸ë¥¼ ì§ì ‘ ì—´ì–´ì„œ ë°ì´í„°ë¥¼ ë°›ëŠ” ë°©ë²•\n",
    "í¬ë¡¤ë§ ë°©ë²•ì— ë”°ë¥¸ ì†ë„\n",
    "requests json > requests html > selenium\n",
    "# summary\n",
    "# web : server-client : url\n",
    "# request, response : get,post\n",
    "# ì›¹ì„œë¹„ìŠ¤ì˜ êµ¬ì¡°\n",
    "# ì›¹í˜ì´ì§€ì˜ ì¢…ë¥˜\n",
    "# - ë™ì í˜ì´ì§€ : URL ë³€ê²½ X > ë°ì´í„° ìˆ˜ì • : JSON : API\n",
    "# - ì •ì í˜ì´ì§€ : URL ë³€ê²½ O > ë°ì´í„° ìˆ˜ì • : HTML : css selector > BeautifulSoup : select(), select_one()\n",
    "\n",
    "\n",
    "# html : ì›¹í˜ì´ì§€ì—ì„œ ë ˆì´ì•„ì›ƒ, í…ìŠ¤íŠ¸ ë“±ì˜ ë°ì´í„°ë¥¼ ì‘ì„±\n",
    "# êµ¬ì„±ìš”ì†Œ: document, element, tag, attr, text\n",
    "# element ê³„ì¸µì  êµ¬ì¡°\n",
    "# tag ì¢…ë¥˜ : p, span, ul, li, table, a, img, iframe, div\n",
    "\n",
    "# css selector : htmlì˜ elementì— styleì„ ì ìš©ì‹œí‚¬ë•Œ elementë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•\n",
    "# element ì„ íƒ : tag(span), class(.), id(#), attr([value=\"no1\"])\n",
    "# në²ˆì§¸ element ì„ íƒ : .py:nth-child(2) : 2ë²ˆì§¸ ì—˜ë¦¬ë¨¼íŠ¸ì¤‘ì— í´ë˜ìŠ¤ê°€ pyì¸ ì—˜ë¦¬ë¨¼íŠ¸\n",
    "# ê³„ì¸µì  element ì„ íƒ : ëª¨ë“  í•˜ìœ„ ì—˜ë¦¬ë¨¼íŠ¸ ì„ íƒ(.wrap p), í•œë‹¨ê³„ í•˜ìœ„ ì—˜ë¦¬ë¨¼íŠ¸ ì„ íƒ(.wrap > p), ì—¬ëŸ¬ê°œ ì„ íƒ(.no1, .no2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49620ab6-88c5-4ddc-8858-69529f3ea12a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Selenium\n",
    "- ë¸Œë¼ìš°ì ¸ì˜ ìë™í™” ëª©ì ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ë‹¤ì–‘í•œ ë¸Œë¼ìš°ì ¸ì™€ ì–¸ì–´ë¥¼ ì§€ì›í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "- ë¸Œë¼ìš°ì ¸ë¥¼ íŒŒì´ì¬ ì½”ë“œë¡œ ì»¨íŠ¸ë¡¤ í•´ì„œ ë¸Œë¼ìš°ì ¸ì— ìˆëŠ” ë°ì´í„°ë¥¼ ìˆ˜ì§‘\n",
    "\n",
    "#### í¬ë¡¤ë§ ë°©ë²•\n",
    "- 1. requests : json : ì›¹í˜ì´ì§€ì˜ API íŠ¸ë˜í”½ì„ ë¶„ì„í•´ì„œ ë°ì´í„° ìˆ˜ì§‘ : naver stock\n",
    "- 2. requests : json : ê³µì‹ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” APIë¥¼ application key ë°›ì•„ì„œ ë°ì´í„° ìˆ˜ì§‘ : naver api(papago, trend)\n",
    "- 3. requests : html, BeautifulSoup(css selector) : ì›¹í˜ì´ì§€ì˜ html ì½”ë“œ ë°›ì•„ì„œ ë°ì´í„° ìˆ˜ì§‘ : gmarket\n",
    "- 4. selenium : browser - python : ë¸Œë¼ìš°ì ¸ë¥¼ íŒŒì´ì¬ ì½”ë“œë¡œ ì»¨íŠ¸ë¡¤ í•´ì„œ ë°ì´í„° ìˆ˜ì§‘ : ted\n",
    "- í¬ë¡¤ë§í• ë•Œ ì¢‹ì€ ìˆœì„œ : 2 > 1 > 3 > 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6973869f-8ad4-408b-86f5-05fb2098994b",
   "metadata": {},
   "source": [
    "### ë™ì í˜ì´ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0ee8b-075f-48c3-b624-7f1eb7f9f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë™ì  í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤\n",
    "# 1. ì›¹ì„œë¹„ìŠ¤ ë¶„ì„(ê°œë°œìë„êµ¬) : URL\n",
    "# 2. request(url, params, header) > response(json) : JSON(str)\n",
    "# 3. JSON(str) > list, dict > DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be0f089-4c10-41a8-8129-205f475a03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "1) pc ì›¹í˜ì´ì§€ê°€ ë³µì¡í•˜ë©´ mobile ì›¹í˜ì´ì§€ì—ì„œ ìˆ˜ì§‘\n",
    "2) ì„œë²„ì— ë°ì´í„° ìš”ì²­ : request(url) > response : json(str)\n",
    "- responseì˜ status codeê°€ 200ì´ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸\n",
    "- 403ì´ë‚˜ 500ì´ ë‚˜ì˜¤ë©´ requestê°€ ì˜ëª»ë˜ê±°ë‚˜ web serverì—ì„œ ìˆ˜ì§‘ì´ ì•ˆë˜ë„ë¡ ì„¤ì •ì´ ëœê²ƒì„\n",
    "  -> header ì„¤ì • ë˜ëŠ” selenium ì‚¬ìš©\n",
    "- 200ì´ ë‚˜ì˜¤ë”ë¼ë„ response ì•ˆì— ìˆëŠ” ë‚´ìš©ì„ í™•ì¸ > í™•ì¸í•˜ëŠ” ë°©ë²• : response.text[:200]\n",
    "3) ì„œë²„ì—ì„œ ë°›ì€ ë°ì´í„° íŒŒì‹±(ë°ì´í„° í˜•íƒœë¥¼ ë³€ê²½) : json(str) > list, dict > DataFrame\n",
    "4) í•¨ìˆ˜ë¡œ ë§Œë“¤ê¸°\n",
    "\n",
    "def stock_price(pagesize, page, code=\"KOSPI\"):\n",
    "    \"\"\"This function is crawling stock price form naver webpage.\n",
    "    Params\n",
    "    ------\n",
    "    pagesize : int : one page size\n",
    "    page : int : page number\n",
    "    code : str : KOSPI or KOSDAQ\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    type : DataFrame : display date, price columns\n",
    "    \"\"\"\n",
    "    url = f\"https://m.stock.naver.com/api/index/{code}/price?pageSize={pagesize}&page={page}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return pd.DataFrame(data)[[\"localTradedAt\",\"closePrice\"]]\n",
    "\n",
    "kospi = stock_price(60,1,\"KOSPI\")\n",
    "kosdaq = stock_price(60,1,\"KOSDAQ\")\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬\n",
    "df = kospi.copy()\n",
    "df[\"kosdaq\"]=kosdaq[\"closePrice\"]\n",
    "df[\"usd\"] = usd['closePrice']\n",
    "df = df.rename(columns={\"closePrice\":\"kospi\"})\n",
    "df\n",
    "\n",
    "# docstring[\"\"\"\"\"\"] : í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë¬¸ìì—´ë¡œ ì‘ì„± \n",
    "#help()ì´ìš©, Shift + tab\n",
    "help(stock_price)\n",
    "\n",
    "df = pd.DataFrame([{'age': 23},{'age': 36},{'age': 27}])  # ë°ì´í„°í”„ë ˆì„ ë§Œë“¤ê¸°\n",
    "df\n",
    "\n",
    "@ ì‹œê°í™”\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ë°ì´í„° ìˆ˜ì§‘\n",
    "page_size = 60\n",
    "kospi_df = stock_price(\"KOSPI\", page_size=page_size)\n",
    "kosdaq_df = stock_price(\"KOSDAQ\", page_size=page_size)\n",
    "usd_df = exchage_rate(\"FX_USDKRW\", page_size=page_size)\n",
    "eur_df = exchage_rate(\"FX_EURKRW\", page_size=page_size)\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ 1 : ë°ì´í„° íƒ€ì… ë³€ê²½\n",
    "print(kospi_df.dtypes)\n",
    "kospi_df[\"kospi\"] = kospi_df[\"closePrice\"].apply(lambda data: float(data.replace(\",\", \"\")))\n",
    "kospi_df = kospi_df.drop(columns=[\"closePrice\"])\n",
    "print(kospi_df.dtypes)\n",
    "\n",
    "kosdaq_df[\"kosdaq\"] = kosdaq_df[\"closePrice\"].apply(lambda data: float(data.replace(\",\", \"\")))\n",
    "usd_df[\"usd\"] = usd_df[\"closePrice\"].apply(lambda data: float(data.replace(\",\", \"\")))\n",
    "eur_df[\"eur\"] = eur_df[\"closePrice\"].apply(lambda data: float(data.replace(\",\", \"\")))\n",
    "\n",
    "kosdaq_df = kosdaq_df.drop(columns=[\"closePrice\"])\n",
    "usd_df = usd_df.drop(columns=[\"closePrice\"])\n",
    "eur_df = eur_df.drop(columns=[\"closePrice\"])\n",
    "\n",
    "# ë°ì´í„° ì „ì²˜ë¦¬ 2 : ë‚ ì§œ ë°ì´í„° ë§ì¶”ê¸° : merge\n",
    "merge_df_1 = pd.merge(kospi_df, kosdaq_df, on=\"localTradedAt\")\n",
    "merge_df_2 = pd.merge(merge_df_1, usd_df, on=\"localTradedAt\")\n",
    "merge_df_3 = pd.merge(merge_df_2, eur_df, on=\"localTradedAt\")\n",
    "merge_df = merge_df_3.copy()\n",
    "merge_df.tail(2)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "plt.figure(figsize=(20, 5))\n",
    "columns = merge_df.columns[1:]\n",
    "for column in columns:\n",
    "    plt.plot(merge_df[\"localTradedAt\"], merge_df[column], label=column)\n",
    "\n",
    "xticks_count = 11\n",
    "plt.xticks(merge_df[\"localTradedAt\"][::int(len(merge_df) // xticks_count) + 1])\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "\n",
    "corr_df = merge_df[merge_df.columns[1:]].corr()\n",
    "corr_df\n",
    "\n",
    "# ê²°ì •ê³„ìˆ˜ : r-squared \n",
    "# 1ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ ê°•í•œ ê´€ê³„, 0ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ ì•½í•œ ê´€ê³„\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.heatmap(corr_df**2, cmap=\"YlGnBu\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92a181-e776-4208-91a2-032e56cf19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì… ë³€ê²½ : str > float\n",
    "# df[column].apply() : ëª¨ë“  ë°ì´í„°ë¥¼ í•¨ìˆ˜ì— ëŒ€ì…í•œ ê²°ê³¼ë¥¼ ì¶œë ¥\n",
    "# apply(func) : ëª¨ë“  ë°ì´í„°ë¥¼ funcì„ ì ìš©ì‹œí‚¨ ê²°ê³¼ ì¶œë ¥\n",
    "df.dtypes\n",
    "\n",
    "# í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ : df.corr()\n",
    "# 1ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ë¥¼ ê°–ëŠ”ë‹¤.\n",
    "# -1ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ ê°•í•œ ìŒì˜ ìƒê´€ê´€ê³„ë¥¼ ê°–ëŠ”ë‹¤.\n",
    "# 0ê³¼ ê°€ê¹Œìš¸ìˆ˜ë¡ ê´€ê³„ê°€ ì—†ë‹¤.\n",
    "df[['kospi','kosdaq','usd']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b41e3-a071-43ec-9c16-9f4c6503ddc6",
   "metadata": {},
   "source": [
    "### ì¹´ì¹´ì˜¤ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbb599-8ffc-487d-8cec-629122da977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "REST_API_KEY = '9a2d4ed549676750779492f1f9f89956'\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://dapi.kakao.com/v2/local/geo/coord2regioncode.json\"\n",
    "#params = {query': '}\n",
    "params = {'x' : '127.177375','y' : '37.240666'}\n",
    "headers = {'Authorization': f'KakaoAK {REST_API_KEY}'}\n",
    "\n",
    "\n",
    "\n",
    "response = requests.get(url, params = params, headers=headers)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58329ff-519b-4c70-bfbd-262752375141",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b48f4b-7dc6-485e-82b1-45e3e360447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "application programing interface\n",
    "api ë¥¼ ì‚¬ìš©í•´ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ”ê²ƒì€ ì„œë¹„ìŠ¤ì— ë°ì´í„°ë¥¼ ì œê³µí•œëŠ” ê³µì‹ì ì¸ ë°©ë²•ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1f72e-7afc-42ce-a3ec-d487dbdb901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APIë¥¼ ì´ìš©í•œ ë°ì´í„° ìˆ˜ì§‘\n",
    "# 1. APP ë“±ë¡ : application key \n",
    "# 2. API ë¬¸ì„œ : URL\n",
    "# 3. request(url, params, header(application key)) > response(json) : JSON(str)\n",
    "# 4. JSON(str) > list, dict > DataFrame or Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c258821c-0a5d-4f7f-88f2-523a3a5139de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68d416-17b4-40ce-9268-a47074cd698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID, CLIENT_SECRET = \"KE2M4YcO4Sv9P7HrFXJ1\", \"9X0zM51ZRf\"\n",
    "\n",
    "# json.dumps() : ì¸í„°ë„· íŠ¸ë˜í”½ì—ì„œëŠ” ì˜ë¬¸, ìˆ«ì, íŠ¹ìˆ˜ë¬¸ìë§Œ ì‚¬ìš©ê°€ëŠ¥\n",
    "# í•œê¸€ê³¼ ê°™ì€ ë¬¸ìë¥¼ ì¸ì½”ë”©(ì˜ë¬¸,ìˆ«ì,íŠ¹ìˆ˜ë¬¸ì)\n",
    "txt= \"íŒŒì´ì¬ì€ ì¬ë¯¸ìˆìŠµë‹ˆë‹¤.\"\n",
    "url = \"https://openapi.naver.com/v1/papago/n2mt\" \n",
    "params = {\"source\": \"ko\", \"target\": \"en\", \"text\": txt}\n",
    "headers = {\"Content-Type\": \"application/json\", \n",
    "           \"X-Naver-Client-Id\": CLIENT_ID,\n",
    "           \"X-Naver-Client-Secret\": CLIENT_SECRET}\n",
    "\n",
    "response = requests.post(url, json.dumps(params),headers=headers)\n",
    "response\n",
    "\n",
    "response.text\n",
    "\n",
    "txt_en = response.json()[\"message\"][\"result\"][\"translatedText\"]\n",
    "txt_en\n",
    "\n",
    "# ì´ë ‡ê²Œ ê°€ì ¸ì˜¬ìˆ˜ ìˆë‹¤[ë‹¤ìŒ í™˜ìœ¨]\n",
    "datas = response.json()[\"data\"]\n",
    "df = pd.DataFrame(datas)\n",
    "df.head(1)\n",
    "\n",
    "def translate(txt):\n",
    "    CLIENT_ID, CLIENT_SECRET = \"KE2M4YcO4Sv9P7HrFXJ1\",\"9X0zM51ZRf\" \n",
    "    url = \"https://openapi.naver.com/v1/papago/n2mt\"\n",
    "    params = {\"source\": \"ko\", \"target\": \"en\", \"text\": txt}\n",
    "    headers = {\"Content-Type\": \"application/json\", \n",
    "           \"X-Naver-Client-Id\": CLIENT_ID,\n",
    "           \"X-Naver-Client-Secret\": CLIENT_SECRET}\n",
    "    response = requests.post(url, json.dumps(params), headers=headers)\n",
    "    txt_en = response.json()[\"message\"][\"result\"][\"translatedText\"]\n",
    "    return txt_en\n",
    "\n",
    "txt= \"ì›¹í¬ë¡¤ë§ì€ ì¬ë¯¸ìˆìŠµë‹ˆë‹¤.\"\n",
    "txt_en = translate(txt)\n",
    "txt_en\n",
    "\n",
    "@ í•œê¸€ excel íŒŒì¼ì„ ì˜ë¬¸ excel íŒŒì¼ë¡œ ë³€ê²½\n",
    "\n",
    "%ls\n",
    "\n",
    "covid = pd.read_excel(\"covid.xlsx\")[[\"category\",\"title\"]]\n",
    "covid.tail(2)\n",
    "\n",
    "covid_en = covid[\"title\"].apply(translate)\n",
    "\n",
    "covid[\"title_en\"] = covid_en\n",
    "covid\n",
    "\n",
    "# utf-8-sig : excelì—ì„œ ì‚¬ìš©í•˜ëŠ” ì¸ì½”ë”© ë°©ì‹ê³¼ í˜¸í™˜ì´ ë˜ëŠ” utf-8ì¸ ì¸ì½”ë”© ë°©ì‹ \n",
    "covid.to_excel(\"covid_en.xlsx\", index = False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d8888-7d77-4ea1-9b61-173c3796e550",
   "metadata": {},
   "source": [
    "### í†µí•©ê²€ìƒ‰ì–´ íŠ¸ë Œë“œ api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ee8a0-4758-428d-a301-f2dbe2437d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. URL\n",
    "url = \"https://openapi.naver.com/v1/datalab/search\"\n",
    "\n",
    "# 2. request > response\n",
    "params = {\n",
    "    \"startDate\": \"2018-01-01\",\n",
    "    \"endDate\": \"2022-01-31\",\n",
    "    \"timeUnit\": \"month\",\n",
    "    \"keywordGroups\": [\n",
    "        {\"groupName\": \"íŠ¸ìœ„í„°\", \"keywords\": [\"íŠ¸ìœ„í„°\", \"íŠ¸ìœ—\"]},\n",
    "        {\"groupName\": \"í˜ì´ìŠ¤ë¶\", \"keywords\": [\"í˜ì´ìŠ¤ë¶\", \"í˜ë¶\"]},\n",
    "        {\"groupName\": \"ì¸ìŠ¤íƒ€ê·¸ë¨\", \"keywords\": [\"ì¸ìŠ¤íƒ€ê·¸ë¨\", \"ì¸ìŠ¤íƒ€\"]},\n",
    "    ]\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"X-Naver-Client-Id\": CLIENT_ID,\n",
    "    \"X-Naver-Client-Secret\": CLIENT_SECRET,    \n",
    "}\n",
    "\n",
    "response = requests.post(url, data=json.dumps(params), headers=headers)\n",
    "response\n",
    "\n",
    "# 3. parsing\n",
    "datas = response.json()[\"results\"]\n",
    "\n",
    "# 4. preprocessing\n",
    "result_df = pd.concat(dfs, ignore_index=True)\n",
    "result_df.tail(2)\n",
    "\n",
    "pivot_df = result_df.pivot(\"period\", \"title\", \"ratio\")\n",
    "pivot_df.columns = [\"instagram\", \"twitter\", \"facebook\"]\n",
    "pivot_df.tail(2)\n",
    "\n",
    "# 5. visualization\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pivot_df.plot(figsize=(20, 5))\n",
    "plt.legend(loc=0)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8271533-a62e-415f-9a5b-7ac6fd1aa285",
   "metadata": {},
   "source": [
    "### ìœ„ë„ ê²½ë„ë¡œ ì°¾ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a584b0-0446-4bd9-a872-034d0a336b16",
   "metadata": {},
   "source": [
    "- ì ˆì°¨\n",
    "    - ë™ì´ë¦„ìœ¼ë¡œ ìœ„ë„ ê²½ë„ êµ¬í•˜ê¸°\n",
    "    - ìœ„ë„ ê²½ë„ë¡œ geohash ì•Œì•„ë‚´ê¸°\n",
    "    - geohashë¡œ ë§¤ë¬¼ ì•„ì´ë”” ê°€ì ¸ì˜¤ê¸°\n",
    "    - ë§¤ë¬¼ ì•„ì´ë””ë¡œ ë§¤ë¬¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784bbc48-ffad-4cfd-9dd9-94e3a2c8a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ë™ì´ë¦„ìœ¼ë¡œ ìœ„ë„ ê²½ë„ êµ¬í•˜ê¸°\n",
    "addr = \"ë§ì›ë™\"\n",
    "url = f\"https://apis.zigbang.com/v2/search?leaseYn=N&q={addr}&serviceType=ì›ë£¸\"\n",
    "response = requests.get(url)\n",
    "data = response.json()[\"items\"][0]\n",
    "lat, lng = data[\"lat\"], data[\"lng\"]\n",
    "lat, lng\n",
    "\n",
    "# 2. ìœ„ë„ ê²½ë„ë¡œ geohash ì•Œì•„ë‚´ê¸°\n",
    "# install geohash2\n",
    "# !pip install geohash2\n",
    "import geohash2\n",
    "\n",
    "# precisionì´ ì»¤ì§ˆìˆ˜ë¡ ì˜ì—­ì´ ì‘ì•„ì§\n",
    "geohash = geohash2.encode(lat, lng, precision=5)\n",
    "geohash\n",
    "\n",
    "# 3. geohashë¡œ ë§¤ë¬¼ ì•„ì´ë”” ê°€ì ¸ì˜¤ê¸°\n",
    "url = f\"https://apis.zigbang.com/v2/items?deposit_gteq=0&domain=zigbang\\\n",
    "&geohash={geohash}&needHasNoFiltered=true&rent_gteq=0&sales_type_in=ì „ì„¸|ì›”ì„¸\\\n",
    "&service_type_eq=ì›ë£¸\"\n",
    "response = requests.get(url)\n",
    "datas = response.json()[\"items\"]\n",
    "# len(datas), datas[0]\n",
    "ids = [data[\"item_id\"] for data in datas]\n",
    "len(ids), ids[:5]\n",
    "\n",
    "# 4. ë§¤ë¬¼ ì•„ì´ë””ë¡œ ë§¤ë¬¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "# 1000ê°œ ë„˜ì–´ê°€ë©´ ë‚˜ëˆ ì„œ ìˆ˜ì§‘í•´ì•¼ í•¨\n",
    "url = \"https://apis.zigbang.com/v2/items/list\"\n",
    "params = {\n",
    "    \"domain\": \"zigbang\",\n",
    "    \"withCoalition\": \"true\",\n",
    "    \"item_ids\": ids\n",
    "}\n",
    "response = requests.post(url, params)\n",
    "response\n",
    "\n",
    "datas = response.json()[\"items\"]\n",
    "df = pd.DataFrame(datas)\n",
    "df.tail(2)\n",
    "\n",
    "# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ í•„í„°ë§\n",
    "columns = [\"item_id\", \"sales_type\", \"deposit\", \"rent\", \"size_m2\", \"floor\", \"building_floor\",\n",
    "           \"address1\", \"manage_cost\"]\n",
    "filtered_column_df = df[columns]\n",
    "filtered_column_df.tail(2)\n",
    "\n",
    "# ì£¼ì†Œì— ë§ì›ë™ì´ ìˆëŠ” ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "result_df = filtered_column_df[filtered_column_df[\"address1\"].str.contains(\"ë§ì›ë™\")]\n",
    "result_df = result_df.reset_index(drop=True)\n",
    "result_df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f877474-7e76-4bf1-aa09-127db92a7c4b",
   "metadata": {},
   "source": [
    "### ì´ë¯¸ì§€ í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b357db-f6fb-4011-adf9-5c7f01eb8734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests, os\n",
    "\n",
    "# 1. ë””ë ‰í† ë¦¬ ìƒì„± : data\n",
    "path = \"data\"\n",
    "if not os.path.exists(\"data\"): # ë””ë ‰í† ë¦¬ ì¡´ì¬ ìœ ë¬´ í™•ì¸\n",
    "    os.makedirs(path)\n",
    "    \n",
    "%ls\n",
    "# 2. csv íŒŒì¼ì„ ë¡œë“œ : image link\n",
    "df = pd.read_csv(\"gmarket.csv\")\n",
    "df.tail(2)\n",
    "\n",
    "img_link = df.loc[0, 'img']\n",
    "img_link\n",
    "\n",
    "# 3. download images : requests\n",
    "response = requests.get(img_link)\n",
    "response\n",
    "\n",
    "with open(f'{path}/test.png', \"wb\") as file: # íŒŒì¼ì €ì¥ wb, ì½ê¸°ëŠ” rd\n",
    "    file.write(response.content)\n",
    "    \n",
    "%ls data\n",
    "\n",
    "# 4. display image : pillow\n",
    "from PIL import Image as pil\n",
    "pil.open(f'{path}/test.png') \n",
    "\n",
    "# 5. ì—¬ëŸ¬ê°œì˜ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ\n",
    "df[:3]\n",
    "for idx, data in df[:5].iterrows():\n",
    "    filename = '0' * (3 - len(str(idx))) + str(idx) + '.png'\n",
    "    print(idx, filename, data['img'])\n",
    "    response = requests.get(data['img'])\n",
    "    with open(f'{path}/{filename}', \"wb\") as file: # íŒŒì¼ì €ì¥ wb, ì½ê¸°ëŠ” rd\n",
    "        file.write(response.content)\n",
    "\n",
    "pil.open(f'{path}/004.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ed5d2-a022-4cd4-ba4a-2c5291398087",
   "metadata": {},
   "source": [
    "## ì–¸ì–´ì§€ëŠ¥ ë”¥ëŸ¬ë‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65aa6aa-c623-4d87-9823-048c4ec57f27",
   "metadata": {},
   "source": [
    "### MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f23eb8-7d2a-4838-9529-e3894a6db0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<MeCab = í˜•íƒœì†Œ ë¶„ì„ê¸°>\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# MeCab ì„¤ì¹˜í•˜ê¸°\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "    \n",
    "!pwd\n",
    "%cd Mecab-ko-for-Google-Colab\n",
    "\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh \n",
    "\n",
    "# ì‹¤í–‰ í…ŒìŠ¤íŠ¸\n",
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "\n",
    "sentence = 'ì½”ë¡œë‚˜ ê°ì—¼ ë’¤ 4ì£¼ ì´ìƒ í›„ìœ ì¦ì´ ì´ì–´ì§€ëŠ” í˜„ìƒì„ ë¡±ì½”ë¹„ë“œë¼ê³  ë¶€ë¥¸ë‹¤.'\n",
    "print (tagger.parse(sentence))\n",
    "\n",
    "# ì„¤ì¹˜ í›„ Tagger ì—ëŸ¬ê°€ ë°œìƒí•œ ê²½ìš°\n",
    "# https://github.com/konlpy/konlpy/issues/144\n",
    "#!git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git\n",
    "#import os\n",
    "#os.chdir('mecab-python-0.996')\n",
    "#!python setup.py build\n",
    "#!python setup.py install\n",
    "\n",
    "!pwd\n",
    "\n",
    "@ MeCab ì‚¬ìš©ì ì‚¬ì „ ì¶”ê°€í•˜ê¸°\n",
    "\n",
    "# Mecabì˜ ì‚¬ì „ ë””ë ‰í† ë¦¬\n",
    "%cd /content/mecab-ko-dic-2.1.1-20180720/\n",
    "ls\n",
    "\n",
    "!ls user-dic/\n",
    "\n",
    "# ì‚¬ìš©ì ì‚¬ì „ì— ë¯¸ë¦¬ ë“±ë¡ë˜ì–´ìˆëŠ” ê³ ìœ ëª…ì‚¬\n",
    "!cat user-dic/nnp.csv\n",
    "\n",
    "1.Colab ì•ˆì—ì„œ ì§ì ‘ ë‹¨ì–´ ì¶”ê°€í•˜ê¸°\n",
    "# ì‚¬ì „ì— ë“±ë¡í•  ë‹¨ì–´ ì¶”ê°€\n",
    "!echo \"ì½”ë¡œë‚˜,,,,NNP,*,T,ì½”ë¡œë‚˜,*,*,*,*,*\" >> user-dic/nnp.csv\n",
    "!cat user-dic/nnp.csv\n",
    "\n",
    "mv user-dic/nnp.csv /content/gdrive/'My Drive'/'aivle'\n",
    "\n",
    "2.í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë¡œì»¬ì—ì„œ ìˆ˜ì • í›„ ì¶”ê°€í•˜ê¸°\n",
    "# nnp.csv íŒŒì¼ ì´ë™\n",
    "!mv /content/mecab-ko-dic-2.1.1-20180720/user-dic/nnp.csv /content/gdrive/'My Drive'/'aivle'\n",
    "\n",
    "ë¡œì»¬ì—ì„œ nnp.csv íŒŒì¼ì— ë‹¨ì–´ë¥¼ ì¶”ê°€í•œ í›„ì— êµ¬ê¸€ë“œë¼ì´ë¸Œì— ì—…ë¡œë“œ\n",
    "# ë¡œì»¬ì—ì„œ íŒŒì¼ ìˆ˜ì • í›„ ì¬ ì—…ë¡œë“œ\n",
    "#!cat /content/gdrive/'My Drive'/'Colab Notebooks'/nng.csv\n",
    "#!cat /content/gdrive/'My Drive'/'Colab Notebooks'/nnp.csv\n",
    "!cat /content/gdrive/'My Drive'/'Colab Notebooks'/aivle/DAY1/nng.csv\n",
    "!cat /content/gdrive/'My Drive'/'Colab Notebooks'/aivle/DAY1/nnp.csv\n",
    "\n",
    "sentence = 'í† íŠ¸ë„˜ì´ ë¦¬ê·¸ 4ìœ„ë¡œ 21-22 í”„ë¦¬ë¯¸ì–´ë¦¬ê·¸ ì‹œì¦Œì„ ë§ˆë¬´ë¦¬í•´ì„œ UCLì— ì§„ì¶œí–ˆë‹¤.'\n",
    "print (tagger.parse(sentence))\n",
    "\n",
    "# ì—…ë¡œë“œëœ ìˆ˜ì • íŒŒì¼ ì´ë™\n",
    "!mv /content/gdrive/'My Drive'/'Colab Notebooks'/aivle/DAY1/nng.csv /content/mecab-ko-dic-2.1.1-20180720/user-dic/\n",
    "!mv /content/gdrive/'My Drive'/'Colab Notebooks'/aivle/DAY1/nnp.csv /content/mecab-ko-dic-2.1.1-20180720/user-dic/\n",
    "!ls user-dic/\n",
    "\n",
    "!cat /content/mecab-ko-dic-2.1.1-20180720/user-dic/nnp.csv\n",
    "!cat /content/mecab-ko-dic-2.1.1-20180720/user-dic/nng.csv\n",
    "\n",
    "# ì‚¬ìš©ìì‚¬ì „ ì—…ë°ì´íŠ¸\n",
    "!bash ./tools/add-userdic.sh\n",
    "\n",
    "# ì‚¬ì „ ë¦¬ë¹Œë“œ\n",
    "!sudo make install\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "# ì‚¬ìš©ì ì‚¬ì „ ì—…ë°ì´íŠ¸ í™•ì¸\n",
    "sentence = 'ì½”ë¡œë‚˜ ê°ì—¼ ë’¤ 4ì£¼ ì´ìƒ í›„ìœ ì¦ì´ ì´ì–´ì§€ëŠ” í˜„ìƒì„ ë¡±ì½”ë¹„ë“œë¼ê³  ë¶€ë¥¸ë‹¤.'\n",
    "print (tagger.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdeefec-81da-4f24-a235-01eadb8a90f8",
   "metadata": {},
   "source": [
    "### word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12574e-2655-46fa-99f8-66c8f0241808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab í™˜ê²½ì— Nanum í°íŠ¸ë¥¼ ì„¤ì¹˜\n",
    "!sudo apt-get install -y fonts-nanum\n",
    "# scans the font directories and build font cache\n",
    "!sudo fc-cache -fv\n",
    "# matplotlibì˜ font cacheë¥¼ clear\n",
    "!rm ~/.cache/matplotlib -rf\n",
    "# ì‹¤í–‰ í›„ ëŸ°íƒ€ì„ì„ ë‹¤ì‹œ ì‹œì‘í•˜ì„¸ìš”\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "!ls /content/gdrive/'My Drive'/aivle/data/\n",
    "\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "    \n",
    "#!cd Mecab-ko-for-Google-Colab\n",
    "!pwd\n",
    "%cd Mecab-ko-for-Google-Colab\n",
    "\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh\n",
    "\n",
    "import MeCab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "with open('/content/gdrive/My Drive/aivle/data/extreme_job_review.txt','r',encoding='utf-8') as f:\n",
    "    my_sentence=f.read()\n",
    "my_sentence\n",
    "\n",
    "print(tagger.parse(my_sentence))\n",
    "\n",
    "# Mecab í˜•íƒœì†Œë¶„ì„ ê²°ê³¼ì—ì„œ ë‹¨ì–´ë¶€ë¶„(ìŠ¤íŠ¸ë§)ì™€ í’ˆì‚¬íƒœê·¸ ë¶€ë¶„ì„ ë¶„ë¦¬í•´ì„œ ë°˜í™˜í•´ì£¼ëŠ” í•¨ìˆ˜\n",
    "# ì˜ˆ) ì˜í™”ë°°ìš°/NNG -> ('ì˜í™”ë°°ìš°', 'NNG')\n",
    "def mecabsplit(mecab_tagger,inputs, pos):\n",
    "    r=[]\n",
    "    inputs = mecab_tagger.parse(inputs)\n",
    "    t = inputs.split('\\n')[:-2]\n",
    "    for i in t:\n",
    "        field = i.split('\\t')\n",
    "        if field[1].split(',')[-1] is not '*':\n",
    "            r.extend( [ (x.split('/')[0],x.split('/')[1]) for x in field[1].split(',')[-1].split('+') ] )\n",
    "        else:\n",
    "            r.append( (field[0],field[1].split(',')[0]) )\n",
    "    if pos:\n",
    "        return r\n",
    "    else:\n",
    "        return [ x[0] for x in r ]\n",
    "    \n",
    "# tagê°€ NNG(ì¼ë°˜ ëª…ì‚¬)ì¸ ë‹¨ì–´ (í† í°, í˜•íƒœì†Œ) ë¦¬ìŠ¤íŠ¸\n",
    "nng = [word for word, tag in mecabsplit(tagger, my_sentence, True) if tag=='NNG']\n",
    "print(nng)\n",
    "\n",
    "# tagê°€ NNP(ê³ ìœ  ëª…ì‚¬)ì¸ ë‹¨ì–´ (í† í°, í˜•íƒœì†Œ) ë¦¬ìŠ¤íŠ¸\n",
    "nnp = [word for word, tag in mecabsplit(tagger, my_sentence, True) if tag=='NNP']\n",
    "print(nnp)\n",
    "\n",
    "from collections import Counter,OrderedDict\n",
    "\n",
    "# matplotlibì˜ í°íŠ¸ë¥¼ Nanum í°íŠ¸ë¡œ ì§€ì •\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "\n",
    "count_list=Counter(nng)\n",
    "print(len(count_list))\n",
    "\n",
    "# counterì•ˆì˜ valueë¥¼ sortí•´ì¤Œ\n",
    "sorted_list=count_list.most_common(20)\n",
    "# ë‹¤ì‹œ dictionary í˜•íƒœë¡œ ë³€í™˜\n",
    "sorted_list=OrderedDict(sorted_list)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot(list(sorted_list.keys()), list(sorted_list.values()))\n",
    "plt.title(\"ì¼ë°˜ ëª…ì‚¬ ë¹ˆë„ìˆ˜\")\n",
    "plt.ylabel(\"# of words\")\n",
    "plt.xlabel(\"word\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd05d9fe-4069-488f-bb65-4da18b7d81aa",
   "metadata": {},
   "source": [
    "### sentiment_BoW[Naive Bayes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da065920-2153-4f2a-b67f-832c0dd374ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeCab ì„¤ì¹˜\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "    \n",
    "%cd Mecab-ko-for-Google-Colab/\n",
    "\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh \n",
    "\n",
    "#colab ì„ ì´ìš©í•œ ì‹¤í–‰ì‹œ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# Mecab í˜•íƒœì†Œë¶„ì„ ê²°ê³¼ì—ì„œ ë‹¨ì–´ë¶€ë¶„(ìŠ¤íŠ¸ë§)ì™€ í’ˆì‚¬íƒœê·¸ ë¶€ë¶„ì„ ë¶„ë¦¬í•´ì„œ ë°˜í™˜í•´ì£¼ëŠ” í•¨ìˆ˜\n",
    "# ì˜ˆ) ì˜í™”ë°°ìš°/NNG -> ('ì˜í™”ë°°ìš°', 'NNG')\n",
    "def mecabsplit(mecab_tagger,inputs, pos):\n",
    "    r=[]\n",
    "    inputs = mecab_tagger.parse(inputs)\n",
    "    t = inputs.split('\\n')[:-2]\n",
    "    for i in t:\n",
    "        field = i.split('\\t')\n",
    "        if field[1].split(',')[-1] is not '*':\n",
    "            r.extend( [ (x.split('/')[0],x.split('/')[1]) for x in field[1].split(',')[-1].split('+') ] )\n",
    "        else:\n",
    "            r.append( (field[0],field[1].split(',')[0]) )\n",
    "    if pos:\n",
    "        return r\n",
    "    else:\n",
    "        return [ x[0] for x in r ]\n",
    "    \n",
    "# utf-8 ì¸ì½”ë”©ìœ¼ë¡œ ëœ í•œê¸€ íŒŒì¼ì„ ì½ì–´ë“¤ì„\n",
    "import codecs\n",
    "def read_data(filename):\n",
    "    with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]\n",
    "    return data\n",
    "\n",
    "# ë°ì´í„° íŒŒì¼ ìœ„ì¹˜ í™•ì¸í•˜ì„¸ìš”\n",
    "train_data = read_data('/content/gdrive/My Drive/aivle/data/nsm/small_ratings_train.txt')\n",
    "test_data = read_data('/content/gdrive/My Drive/aivle/data/nsm/small_ratings_test.txt')\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print (train_data[0])\n",
    "print (test_data[0])\n",
    "\n",
    "\n",
    "%%time\n",
    "# í˜•íƒœì†Œ ë¶„ì„ê¸° Mecab import\n",
    "import MeCab\n",
    "tagger = MeCab.Tagger()\n",
    "\n",
    "# Mecabì˜ ì¶œë ¥ ìŠ¤íŠ¸ë§ì„ token+space+token+space ... í¬ë§·ìœ¼ë¡œ ë³€í™˜í•´ì¤Œ (pos=Falseì¼ ë•Œ)\n",
    "def tokenize(doc):\n",
    "    return ' '.join(mecabsplit(tagger, doc, False))\n",
    "\n",
    "# train dataì˜ ì²«ë²ˆì§¸ ì¤„\n",
    "print (train_data[0])\n",
    "# test dataì˜ ì²«ë²ˆì§¸ ì¤„\n",
    "print (test_data[0])\n",
    "\n",
    "# train dataì˜ document ë¶€ë¶„\n",
    "train_docs_X = [tokenize(row[1]) for row in train_data]\n",
    "# train dataì˜ label ë¶€ë¶„\n",
    "train_Y = [row[2] for row in train_data]\n",
    "\n",
    "test_docs_X = [tokenize(row[1]) for row in test_data]\n",
    "test_Y = [row[2] for row in test_data]\n",
    "\n",
    "# train data í™•ì¸\n",
    "print(train_docs_X[0])\n",
    "print(train_Y[0])\n",
    "print(train_docs_X[1])\n",
    "print(train_Y[1])\n",
    "print('\\n')\n",
    "# test data í™•ì¸\n",
    "print(test_docs_X[0])\n",
    "print(test_Y[0])\n",
    "print(test_docs_X[1])\n",
    "print(test_Y[1])\n",
    "print('\\n')\n",
    "\n",
    "@ CountVectorizer ë³€í™˜\n",
    "ë°ì´í„° : ['I am happy', 'I am sad']\n",
    "ë‹¨ì–´ ëª¨ìŒ : ['I', 'am', 'happy', 'sad']\n",
    "ë³€í™˜ ê²°ê³¼ : [[1, 1, 1, 0], [1, 1, 0, 1]]\n",
    "    \n",
    "# CountVectorizer ê¸°ëŠ¥: \n",
    "# 1. ë¬¸ì„œë¥¼ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œë‹¤\n",
    "# 2. ê° ë¬¸ì„œì—ì„œ í† í°ì˜ ë¹ˆë„ë¥¼ ì„¼ë‹¤\n",
    "# 3. ê° ë¬¸ì„œë¥¼ BoW ë²¡í„°ë¡œ ë³€í™˜í•œë‹¤.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# max_features : ì „ì²´ ë¬¸ì„œì—ì„œ ë¹ˆë„ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ top max_features ê°¯ìˆ˜ì˜ ë‹¨ì–´ë§Œ í¬í•¨í•˜ì—¬\n",
    "# fit : vocabularyë¥¼ ìƒì„±í•œë‹¤\n",
    "vec = CountVectorizer(max_features = 1000).fit(train_docs_X)\n",
    "\n",
    "# í•™ìŠµë°ì´í„°(train_docs_X), í…ŒìŠ¤íŠ¸ë°ì´í„°(test_docs_X)ë¥¼ BoW ë²¡í„°ë¡œ ë³€í™˜\n",
    "train_X = vec.transform(train_docs_X).toarray()\n",
    "test_X = vec.transform(test_docs_X).toarray()\n",
    "\n",
    "# ì²«ë²ˆì§¸ ë¬¸ì„œ(ë¦¬ë·° ë¬¸ì¥)ì˜ ë²¡í„°\n",
    "print(train_X[0])\n",
    "# í•™ìŠµë°ì´í„° ë¬¸ì„œ(ë¦¬ë·° ë¬¸ì¥) ê°¯ìˆ˜\n",
    "print(len(train_X))\n",
    "\n",
    "\n",
    "@ Naive Bayes ëª¨ë¸ ì´ìš©í•˜ê¸°\n",
    "%%time\n",
    "\n",
    "# sklearn íŒ©í‚¤ì§€ì—ì„œ GaussianNB ëª¨ë¸ì„ import\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# GaussianNB ê°ì²´ë¥¼ ìƒì„±\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# train_X: í•™ìŠµë°ì´í„°, train_Y: ì •ë‹µë ˆì´ë¸”ì„ ë°›ì•„ì„œ í•™ìŠµ\n",
    "gnb.fit(train_X, train_Y)\n",
    "\n",
    "# score(test_data, true_labels) returns the mean accuracy on the given test data and labels.\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gnb.score(train_X, train_Y)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gnb.score(test_X, test_Y)))\n",
    "\n",
    "\n",
    "pos = [ [test_docs_X[i], test_Y[i], gnb.predict([test_X[i]]) ] for i in range(len(test_Y)) if test_Y[i] == '1' ]\n",
    "neg = [ [test_docs_X[i], test_Y[i], gnb.predict([test_X[i]]) ] for i in range(len(test_Y)) if test_Y[i] == '0' ]\n",
    "#crr= []\n",
    "#for i in range(len(test_Y)):\n",
    "#    if test_Y[i] == '1':\n",
    "#        crr.append([test_docs_X[i], test_Y[i], gnb_predict([test_X[i]]) ])\n",
    "#print (crr)\n",
    "print ('>ê¸ì • ë¦¬ë·°ì— ëŒ€í•œ ì˜ˆì¸¡:')\n",
    "for i in pos[:5]:\n",
    "    print ('ì…ë ¥:', i[0])\n",
    "    print ('ì •ë‹µ:', i[1])\n",
    "    print ('ì˜ˆì¸¡:', i[2])\n",
    "print ('\\n>ë¶€ì • ë¦¬ë·°ì— ëŒ€í•œ ì˜ˆì¸¡:')\n",
    "for i in neg[:5]:\n",
    "    print ('ì…ë ¥:', i[0])\n",
    "    print ('ì •ë‹µ:', i[1])\n",
    "    print ('ì˜ˆì¸¡:', i[2])\n",
    "\n",
    "    \n",
    "@  KNN ëª¨ë¸ ì´ìš©í•˜ê¸°\n",
    "# (ì—°ìŠµ 1) KNNìœ¼ë¡œ ê¸ë¶€ì • ë¶„ë¥˜ë¥¼ í•˜ì„¸ìš”. (sklearn ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” KNeighborsClassifierë¥¼ ì‚¬ìš©)\n",
    "# write code here\n",
    "# \n",
    "#\n",
    "#\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(train_X, train_Y)\n",
    "\n",
    "# (ì—°ìŠµ 2) score í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë°ì´í„°ì˜ ì •í™•ë„, í…ŒìŠ¤íŠ¸ë°ì´í„°ì˜ ì •í™•ë„ë¥¼ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "# ì†Œìˆ˜ì  ì´í•˜ 3ìë¦¬ê¹Œì§€ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "#\n",
    "#\n",
    "print(\"Accuracy on training set: {:.3f}\".format(neigh.score(train_X, train_Y)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(neigh.score(test_X, test_Y)))\n",
    "\n",
    "# KNNìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê¸°\n",
    "pos = [ [test_docs_X[i], test_Y[i], neigh.predict([test_X[i]]) ] for i in range(len(test_Y)) if test_Y[i] == '1' ]\n",
    "neg = [ [test_docs_X[i], test_Y[i], neigh.predict([test_X[i]]) ] for i in range(len(test_Y)) if test_Y[i] == '0' ]\n",
    "\n",
    "print ('>ê¸ì • ë¦¬ë·°ì— ëŒ€í•œ ì˜ˆì¸¡:')\n",
    "for i in pos[:5]:\n",
    "    print ('\\nì…ë ¥:', i[0])\n",
    "    print ('ì •ë‹µ:', i[1])\n",
    "    print ('ì˜ˆì¸¡:', i[2])\n",
    "print ('\\n>ë¶€ì • ë¦¬ë·°ì— ëŒ€í•œ ì˜ˆì¸¡:')\n",
    "for i in neg[:5]:\n",
    "    print ('\\nì…ë ¥:', i[0])\n",
    "    print ('ì •ë‹µ:', i[1])\n",
    "    print ('ì˜ˆì¸¡:', i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982313d8-02c0-4511-ab77-b88f813fd10b",
   "metadata": {},
   "source": [
    "### sentiment_vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec1f13-82ec-41ac-8696-60b104be3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# ì‚¬ì „ ê¸°ë°˜ ê°ì„±ë¶„ì„ íˆ´ vaderSentiment ì„¤ì¹˜\n",
    "!pip install vaderSentiment\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(score)))\n",
    "    \n",
    "sentiment_analyzer_scores(\"The phone is super cool.\")\n",
    "\n",
    "# ì•„ë§ˆì¡´ ìƒí’ˆë¦¬ë·°\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/aivle/data/data_sentiment.csv',encoding='utf-8')\n",
    "print(df.head())\n",
    "\n",
    "# vader ê²°ê³¼\n",
    "vaderresult=[]\n",
    "# ìƒí’ˆë¦¬ë·° ì ìˆ˜ì™€ ê°™ì€ ê²½ìš°\n",
    "truecount = 0\n",
    "\n",
    "for i in range(df.id.count()):\n",
    "    # compoundê²°ê³¼ê°€ 0.05ë³´ë‹¤ í¬ë©´ ê¸ì •, -0.05ë³´ë‹¤ ì‘ìœ¼ë©´ ë¶€ì •\n",
    "    if analyser.polarity_scores(df.textcontent[i]).get(\"compound\")>0.05:\n",
    "        vaderresult.append(1)\n",
    "    elif analyser.polarity_scores(df.textcontent[i]).get(\"compound\")< -0.05:\n",
    "        vaderresult.append(0)\n",
    "    else:\n",
    "        vaderresult.append(3)\n",
    "\n",
    "    print(df.textcontent[i], df.reviewrating[i], vaderresult[i])\n",
    "\n",
    "    #ìì„¸íˆ ë³´ê³  ì‹¶ë‹¤ë©´..\n",
    "    #print(df.textcontent[i],analyser.polarity_scores(df.textcontent[i]))\n",
    "    \n",
    "for i in range(df.id.count()):\n",
    "    if vaderresult[i]==df.reviewrating[i]:\n",
    "        truecount=truecount+1\n",
    "# ì •í™•ë„\n",
    "print(\"ì •í™•ë„ : \", truecount/(df.id.count()))\n",
    "print(\"ì •í™•ë„ : \", format(truecount/(df.id.count()), \".3f\"))\n",
    "#print(\"ì •í™•ë„ : \", round(truecount/(df.id.count()), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e512c3a-a49b-4c65-99b6-00c053e06f20",
   "metadata": {},
   "source": [
    "### clustering_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5333c20-b76c-43c2-b375-870477c27d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#êµ¬ê¸€ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„° ìœ„ì¹˜\n",
    "Path = '/content/drive/MyDrive/aivle/data/clustering/' \n",
    "\n",
    "# ê° ì£¼ì œë³„ 100ê±´ì˜ ë‰´ìŠ¤ ë°ì´í„°\n",
    "# Read an Excel file into a pandas DataFrame\n",
    "# DataFrame : 2ì°¨ì› ë°ì´í„°êµ¬ì¡°. Row, Column, Series(ê° Columnì— ìˆëŠ” ë°ì´í„°ë“¤)ë¡œ êµ¬ì„±\n",
    "climate = pd.read_excel(Path+'news_climate change.xlsx')\n",
    "mobility = pd.read_excel(Path+'news_mobility.xlsx')\n",
    "probiotics = pd.read_excel(Path+'news_probiotics.xlsx')\n",
    "\n",
    "# ì£¼ì œ columnì„ ì¶”ê°€\n",
    "climate['ì£¼ì œ'] = 'ê¸°í›„ìœ„ê¸°'\n",
    "mobility['ì£¼ì œ'] = 'ëª¨ë¹Œë¦¬í‹°'\n",
    "probiotics['ì£¼ì œ'] = 'ìœ ì‚°ê· '\n",
    "\n",
    "# ê° ì£¼ì œë³„ 70ê±´ì˜ ë‰´ìŠ¤ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì…ë ¥ìš© ë°ì´í„°ë¥¼ ìƒì„±\n",
    "# pandas.concat(objs, axis=0, ignore_index=Faslse, ...)\n",
    "# axis=0, 0: ìœ„+ì•„ë˜ë¡œ í•©ì¹˜ê¸°, 1: ì™¼ìª½+ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•©ì¹˜ê¸°\n",
    "# ignore_index=True : ê¸°ì¡´ indexë¥¼ ë¬´ì‹œ. The resulting axis will be labeled 0, â€¦, n - 1. \n",
    "data = pd.concat([climate[:70], mobility[:70], probiotics[:70]], ignore_index=True)\n",
    "\n",
    "# ë‰´ìŠ¤ê¸°ì‚¬ body, titleì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µì„ ì œê±°\n",
    "# DataFrame.drop_duplicates : Returns DataFrame with duplicate rows removed.\n",
    "data_unique = data.drop_duplicates(['body']).drop_duplicates(['title'])\n",
    "\n",
    "# ì¤‘ë³µ ì œê±°ëœ ë°ì´í„° ê±´ìˆ˜ë¥¼ í™•ì¸\n",
    "# DataFrame.shape : Returns a tuple representing the dimensionality of the DataFrame.\n",
    "data.shape, data_unique.shape\n",
    "\n",
    "@ ì¤‘ë³µìœ¼ë¡œ ë“¤ì–´ìˆëŠ” ê¸°ì‚¬ë¥¼ ì œê±°í•˜ê³  ê¸°ì‚¬ë“¤ì˜ ìˆœì„œë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ìŠµë‹ˆë‹¤.\n",
    "\n",
    "# ë°ì´í„° ìˆœì„œë¥¼ ì…”í”Œë§\n",
    "# frac: ëœë¤ ì¶”ì¶œí•  ë¹„ìœ¨ (1 == ì „ì²´ ë°ì´í„°ë¥¼ ì…”í”Œë§)\n",
    "# random_state: ëœë¤ ì¶”ì¶œí•  ê°’ì— seed ì„¤ì •í•˜ë©´, í•­ìƒ ê°™ì€ ê²°ê³¼ë¥¼ ìƒì„±\n",
    "# reset_index: Reset the index. Use drop parameter to avoid the old index being added as a column\n",
    "data_shuffled = data_unique.sample(frac=1, random_state=16).reset_index(drop=True) \n",
    "data_shuffled.tail(5)\n",
    "\n",
    "# DataFrame.iloc : integer-location based indexing for selection by position\n",
    "data_shuffled.iloc[0]['title']\n",
    "data_shuffled.iloc[0]['body']\n",
    "\n",
    "# í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ” íŒŒì´ì¬ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install konlpy\n",
    "\n",
    "< ë‰´ìŠ¤ ê¸°ì‚¬ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ >\n",
    "\n",
    "# ë°ì´í„°ì—ì„œ ì²«ë²ˆì§¸ ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()  # ì¹´ì´ìŠ¤íŠ¸ ì—°êµ¬ì‹¤ì—ì„œ ê°œë°œì—ì„œ ì˜¤í”ˆ \n",
    "\n",
    "temp = hannanum.nouns(data_shuffled.loc[0]['body'])  # nouns-> ì¼ë°˜ëª…ì‚¬/ ê³ ìœ ëª…ì‚¬ë§Œ ì¶”ì¶œ\n",
    "print(temp)\n",
    "\n",
    "# ëª…ì‚¬ë§Œ ì¶”ì¶œëœ ë¦¬ìŠ¤íŠ¸\n",
    "word_list=temp\n",
    "\n",
    "# ë¬¸ì„œì— ì¶œí˜„í•œ ê° ëª…ì‚¬ë¡œë¶€í„° Series ê°ì²´ë¥¼ ìƒì„± (indexëŠ” 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì •ìˆ˜ê°’)\n",
    "# 0 ì°¨ëŸ‰\n",
    "# 1 ê³ ì¥\n",
    "# 2 ì œë™\n",
    "# 3 ì°¨ëŸ‰\n",
    "# 4 ê³ ì¥\n",
    "# 5 ...\n",
    "word_list=pd.Series([x for x in word_list if len(x)>1])  # ê°ì²´ìƒì„±\n",
    "\n",
    "# pd.Series.value_counts: Returns a Series containing counts of unique values.\n",
    "# The resulting object will be in descending order so that the first element is the most frequently-occurring element.  \n",
    "word_list.value_counts().head(10), data_shuffled.iloc[0]['ì£¼ì œ']\n",
    "\n",
    "< DBSCAN ì˜ˆì œ : ê¸°ì‚¬ ë¶„ì„í•˜ê¸° >\n",
    "    \n",
    "# ê¸°ì‚¬ ë‚´ìš©ì¤‘ ëª…ì‚¬ë§Œì„ ì¶”ì¶œí•˜ì—¬ docs ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "docs = []\n",
    "for i in data_shuffled['body']:\n",
    "    docs.append(hannanum.nouns(i))\n",
    "\n",
    "# ê³µë°± ë¬¸ìë¥¼ ë„£ì–´ì„œ ê° ë‹¨ì–´ë¥¼ í•©ì¹©ë‹ˆë‹¤.\n",
    "for i in range(len(docs)):\n",
    "    docs[i] = ' '.join(docs[i])\n",
    "\n",
    "docs[0]\n",
    "\n",
    "# ì²˜ìŒ í•œë‚˜ëˆ” ë¶„ì„ê¸°ë¥¼ ì´ìš©í•´ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œí•œ docsì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì´ì¤‘ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "# [[ë¬¸ì¥1], [ë¬¸ì¥2], ... , [ë¬¸ì¥70]]\n",
    "# ë‘ë²ˆì§¸ forë¬¸ì„ ì´ìš©í•˜ë©´ docsëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•ë˜ê³ \n",
    "\n",
    "# [ë¬¸ì¥1, ë¬¸ì¥2, ... , ë¬¸ì¥70]\n",
    "# ê° ë¦¬ìŠ¤íŠ¸ì˜ ì›ì†Œê°€ ëª…ì‚¬ë“¤ ì‚¬ì´ì— ê³µë°±ì´ ì‚½ì…ëœ í…ìŠ¤íŠ¸ë¡œ ë°”ë€ë‹ˆë‹¤.\n",
    "\n",
    "# sklearn.feature_extraction.text.TfidfVectorizer :\n",
    "# Converts a collection of raw documents to a matrix of TF-IDF features (document-term matrix)\n",
    "# ngram_range(min_n, max_n): all values of n such that min_n <= n <= max_n will be used. \n",
    "# min_df : ignore terms that have a document frequency lower than the given threshold.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# n(1 to 5)gram ì„ ì‚¬ìš©í•˜ì—¬ ìœ„ ê¸°ì‚¬ì˜ ëª…ì‚¬ë“¤ì„ tfidf vectorë¡œ ë³€í™˜\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 3, ngram_range=(1,5))\n",
    "tfidf_vectorizer.fit(docs)\n",
    "vector = tfidf_vectorizer.transform(docs).toarray()\n",
    "print(vector.shape) # num_of_documents X vocab   DTM (Document -Term Matrix)\n",
    "\n",
    "# sklearn.cluster.DBSCAN: Performs DBSCAN clustering from vector array\n",
    "# eps: maximum distance between two samples (default 0.5)\n",
    "# min_samples: The number of samples in a neighborhood for a point to be considered as a core point (default 5) \n",
    "# metric: The metric to use when calculating distance between instances in a feature array. (default 'euclidean')\n",
    "# 'cosine' distance == (1 - cosine similarity)\n",
    "# ex) If 2 vectors are perfectly the same then the similarity is 1 (angle=0 hence ğ‘ğ‘œğ‘ (ğœƒ)=1), and the distance is 0 (1â€“1=0).\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "vector = np.array(vector)\n",
    "model = DBSCAN(eps=0.6, min_samples=3, metric = \"cosine\") # ì–˜ë„¤ë¥¼ ë°”ê¿”ì„œ í•˜ë©´ ë¨ (ë…¸ì´ì¦ˆ ì¤„ì–´ë“¦)\n",
    "result = model.fit_predict(vector) # Computes clusters from a data and predict labels.\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ -1ì—ëŠ” ë…¸ì´ì¦ˆ ë°ì´í„°ë¡œ íŒë³„ë˜ì–´ í´ëŸ¬ìŠ¤í„°ë§ì´ ì•ˆëœ ë¬¸ì„œë“¤ì´ ë“¤ì–´ ìˆìŒ\n",
    "print('í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸: í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ê¸°ì‚¬ì˜ ìˆ˜')\n",
    "result_dict = {str(i):list(result).count(i) for i in set(result)} # resultì˜ ê° í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ê¸°ì‚¬ìˆ˜ë¥¼ ì¹´ìš´íŠ¸\n",
    "result_dict # ë§ˆì§€ë§‰ 159 = ë…¸ì´ì¦ˆ\n",
    "\n",
    "# í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ë¥¼ columnìœ¼ë¡œ ì¶”ê°€\n",
    "data_shuffled['result'] = result\n",
    "\n",
    "sentences = []\n",
    "max_cluster_num = 0\n",
    "for cluster_num in set(result):\n",
    "    sentence = ''\n",
    "\n",
    "    # -1ì€ ë…¸ì´ì¦ˆ íŒë³„ì´ ë‚¬ê±°ë‚˜ í´ëŸ¬ìŠ¤í„°ë§ì´ ì•ˆëœ ê²½ìš°\n",
    "    if(cluster_num == -1): \n",
    "        continue\n",
    "    else:\n",
    "        max_cluster_num = cluster_num         # ìƒì„±ëœ í´ëŸ¬ìŠ¤í„°ì˜ ê°¯ìˆ˜ë¥¼ ì¹´ìš´íŠ¸\n",
    "        temp_df = data_shuffled[data_shuffled['result'] == cluster_num] # cluster num ë³„ë¡œ ì¡°íšŒ\n",
    "\n",
    "        print(\"cluster num : {}\".format(cluster_num))\n",
    "        # zip: ë™ì¼í•œ ê°œìˆ˜ë¡œ ì´ë£¨ì–´ì§„ ìë£Œí˜•ì„ ë¬¶ì–´ ì¤€ë‹¤\n",
    "        for data in zip(temp_df['title'], temp_df['body'], temp_df.iloc): \n",
    "            title, body, my_data = data\n",
    "            print(my_data['ì£¼ì œ'], ':', title) # ì£¼ì œ, ê¸°ì‚¬ ì œëª©ì„ ì¶œë ¥\n",
    "            sentence += body + ' '\n",
    "        print()\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "# ê° í´ëŸ¬ìŠ¤í„° ë§ˆë‹¤ ë°ì´í„° ì¶œë ¥í•´ì£¼ê¸°\n",
    "for i in range(max_cluster_num+1):\n",
    "  data_cluster = data_shuffled[data_shuffled.result==i]\n",
    "  data_num = len(data_cluster)\n",
    "\n",
    "  if len(data_cluster) > num_to_display:\n",
    "    data_cluster = data_cluster[0:num_to_display]\n",
    "\n",
    "  print('----- í´ëŸ¬ìŠ¤í„° %i -----' %i)\n",
    "  print('ì´ ë°ì´í„° ê°œìˆ˜: %i' % data_num)\n",
    "  print(data_cluster)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f382540-a136-4fc7-8ffd-89da9679e737",
   "metadata": {},
   "source": [
    "### Word2Vec_Wiki_Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4525c00-27a7-4b1d-a3f0-a949c0a34f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "<ëª¨ë¸!>\n",
    "(ì£¼ì˜) word2vec ëª¨ë¸ í•™ìŠµì— ì¥ì‹œê°„ì´ ì†Œìš”ë˜ë¯€ë¡œ ê°•ì˜ ì¤‘ì— ì‹¤í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import codecs\n",
    "import os\n",
    "import gensim\n",
    "\n",
    "%%time\n",
    "def read_data(filename):\n",
    "    with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "        data = [line.split(' ') for line in f.read().splitlines()]\n",
    "    return data\n",
    "        \n",
    "data = read_data('/content/gdrive/My Drive/aivle/data/wiki/wiki.txt')\n",
    "\n",
    "print(len(data))\n",
    "print(data[0])\n",
    "\n",
    "%%time\n",
    "import gensim\n",
    "# Word2Vec ìƒì„±\n",
    "# sg=1ì´ë©´ skip-gram 0ì´ë©´ cbow ë°©ì‹ ì‚¬ìš©, min_count ë‹¨ì–´ ë¹ˆë„ìˆ˜, size ë²¡í„°ì°¨ì›\n",
    "model = gensim.models.Word2Vec(data, min_count=1, size=100, window=5, sg=1, seed=10)\n",
    "model.save('/content/gdrive/My Drive/Colab Notebooks/aivle/data/wiki/wiki.w2v')\n",
    "\n",
    "%%time\n",
    "model = gensim.models.Word2Vec.load(os.path.join('/content/gdrive/My Drive/aivle/data/wiki', 'wiki.w2v'))\n",
    "\n",
    "model.wv['ê³ ì–‘ì´']\n",
    "\n",
    "result = model.wv.accuracy(os.path.join('/content/gdrive/My Drive/aivle/data/wiki','word_analogy_korean.txt')) \n",
    "for r in result:\n",
    "    print('%s\\t%s\\t%s' % (r['section'], len(r['correct']), len(r['incorrect'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756fd9d8-ef14-4d1e-9671-b4fc05fa4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# gensimì€ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë° í•„ìš”í•œ í•¨ìˆ˜ë“¤ ì œê³µ\n",
    "# gensim is an open-source library for unsupervised topic modeling, document indexing, \n",
    "# retrieval by similarity, and other natural language processing functionalities. \n",
    "import os\n",
    "import gensim\n",
    "import codecs\n",
    "\n",
    "model = gensim.models.Word2Vec.load(os.path.join('/content/gdrive/My Drive/aivle/data/wiki', 'wiki.w2v'))\n",
    "\n",
    "model.wv[\"ê°•ì•„ì§€\"]\n",
    "\n",
    "# Word Analogy Test (ì„ë² ë”©ëœ ë‹¨ì–´ë²¡í„°ë“¤ì´ ì˜ë¯¸ë¡ ì /ë¬¸ë²•ì  ê´€ê³„ë¥¼ ì˜ ë°˜ì˜í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸)\n",
    "# Find the top-N most similar words. Positive words contribute positively towards the similarity, negative words negatively.\n",
    "for t in model.wv.most_similar(positive=[\"íŒŒë¦¬\", \"ì¼ë³¸\"], negative=[\"ë„ì¿„\"], topn=10):\n",
    "    print('%s\\t%f' % (t[0], t[1]))\n",
    "    \n",
    "result = model.wv.accuracy(os.path.join('/content/gdrive/My Drive/Colab Notebooks/aivle/data/wiki','word_analogy_korean.txt'))\n",
    "for r in result:\n",
    "    print('%s\\t%s\\t%s' % (r['section'], len(r['correct']), len(r['incorrect'])))\n",
    "    \n",
    "# most_similar í•¨ìˆ˜ top-Nê°œ ìœ ì‚¬ ë‹¨ì–´ë¥¼ ë°˜í™˜\n",
    "for t in model.wv.most_similar(\"ê°•ì•„ì§€\", topn=5):\n",
    "    print(t[0])\n",
    "    \n",
    "model.wv.similarity(\"í˜¸ë‘ì´\", \"í‘œë²”\") # ë‘ ë‹¨ì–´ì˜ similarì ìˆ˜ ë½‘ì•„ì¤Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d803e-f42c-4c71-bc29-a0ea419d732b",
   "metadata": {},
   "source": [
    "### PyTorch_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e366a8d-eeac-47df-a741-c719643badb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The torch package contains data structures for multi-dimensional tensors and \n",
    "# defines mathematical operations over these tensors. \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "@ í…ì„œì´ˆê¸°í™” í•˜ê¸°\n",
    "# The torch package contains data structures for multi-dimensional tensors and \n",
    "# defines mathematical operations over these tensors. \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ëœë¤ê°’ìœ¼ë¡œ í…ì„œ ìƒì„±. ì •ìˆ˜ê°’ìœ¼ë¡œ í…ì„œì˜ ì°¨ì›ì„ ì „ë‹¬\n",
    "# ëœë¤ê°’ìœ¼ë¡œ ì±„ì›Œì§„ (3, 4) ì°¨ì›ì˜ í…ì„œë¥¼ ìƒì„± \n",
    "tensor = torch.rand(3, 4)   \n",
    "print (\"tensor :\", tensor)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")     \n",
    "print(f\"Datatype of tensor: {tensor.dtype}\") \n",
    "print(f\"Device tensor is stored on: {tensor.device}\") \n",
    "\n",
    "# Returns a tensor filled with the scalar value 1,with the shape defined by the argument size.\n",
    "tensor = torch.ones(4, 4)\n",
    "print(tensor)\n",
    "# ëª¨ë“  í–‰ì— ëŒ€í•´ì„œ 1ë²ˆì—´ì— 0ê°’ì„ ëŒ€ì…\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n",
    "\n",
    "# Concatenate : í…ì„œë¥¼ ì—°ê²°í•˜ê¸°\n",
    "# ë”¥ëŸ¬ë‹ì—ì„œëŠ” ëª¨ë¸ì˜ ì…ë ¥ ë˜ëŠ” ì¤‘ê°„ ì—°ì‚° ë‹¨ê³„ì—ì„œ ë‘ ê°œì˜ í…ì„œë¥¼ ì—°ê²°í•˜ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "# ë‘ í…ì„œë¥¼ ì—°ê²°í•´ì„œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë‘ í…ì„œì— ë‹´ê¸´ ì •ë³´ë¥¼ ëª¨ë‘ ì‚¬ìš©í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. \n",
    "# dim : í…ì„œë¥¼ ì—°ê²°í•˜ì—¬ ì–´ëŠ ì°¨ì›ì„ ëŠ˜ë¦´ ê²ƒì¸ì§€ë¥¼ í‘œì‹œ\n",
    "t1 = torch.cat([tensor, tensor], dim=0) # ë‘ í…ì„œë¥¼ ì—°ê²°í•˜ì—¬ 0ë²ˆì§¸ ì°¨ì›ì„ ëŠ˜ë¦¬ë¼ëŠ” ì˜ë¯¸\n",
    "print(\"tensor shape:\", tensor.shape)\n",
    "print(\"->\", t1)\n",
    "print(\"t1 shape:\", t1.shape) # 0ë²ˆì§¸ dimensionì´ ëŠ˜ì–´ë‚œ ê²ƒì„ í™•ì¸\n",
    "print(\"----------------------------\")\n",
    "t2 = torch.cat([tensor, tensor], dim=1) # ë‘ í…ì„œë¥¼ ì—°ê²°í•˜ì—¬ 1ë²ˆì§¸ ì°¨ì›ì„ ëŠ˜ë¦¬ë¼ëŠ” ì˜ë¯¸\n",
    "print(\"tensor shape:\", tensor.shape)\n",
    "print(\"->\", t2)\n",
    "print(\"t2 shape:\", t2.shape) # 1ë²ˆì§¸ dimensionì´ ëŠ˜ì–´ë‚œ ê²ƒì„ í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3e03a-1cec-473f-ac38-870e0d86ac9b",
   "metadata": {},
   "source": [
    "* ì‹ ê²½ë§ ëª¨ë¸ ìƒì„±í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ede84-1241-4450-b79c-692d4b468206",
   "metadata": {},
   "outputs": [],
   "source": [
    "ëª¨ë“  ì‹ ê²½ë§ í´ë˜ìŠ¤ëŠ” torch.nn íŒ¨í‚¤ì§€ë¥¼ í†µí•´ì„œ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "torch.nn.Moduleì€ PyTorchì˜ ëª¨ë“  ì‹ ê²½ë§ì˜ Base Classì´ë©° ìƒˆë¡œìš´ ì‹ ê²½ë§ ëª¨ë¸ì€ torch.nn.Module í´ë˜ìŠ¤ë¥¼ ìƒì†í•˜ì—¬ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "ìƒˆë¡œìš´ í´ë˜ìŠ¤ ë‚´ì—ì„œ __init()__í•¨ìˆ˜ì™€ forward()í•¨ìˆ˜ë¥¼ ë°˜ë“œì‹œ override í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "__init()__í•¨ìˆ˜ì—ì„œëŠ” ëª¨ë¸ì—ì„œ ì‚¬ìš©ë  module(nn.Linear, nn.Conv2d), activation function(ReLU ë“±)ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "forward()ì—ì„œëŠ” ëª¨ë¸ì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼ í•˜ëŠ” ì—°ì‚°ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "backward ì—°ì‚°ì€ backward() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ PyTorchê°€ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•˜ë¯€ë¡œ forward()ë§Œ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "forward()ì—ì„œëŠ” input ë°ì´í„°ì— ëŒ€í•´ ì–´ë–¤ ì—°ì‚°ì„ ì§„í–‰í•˜ì—¬ outputì´ ë‚˜ì˜¬ì§€ë¥¼ ì •ì˜í•´ ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e782de5-10a3-46db-bc0a-12043b20c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn           # ì‹ ê²½ë§ êµ¬í˜„ì„ ìœ„í•œ ë°ì´í„° êµ¬ì¡°, ì‹ ê²½ë§ ë ˆì´ì–´ ë“±ì´ ì •ì˜ë˜ì–´ ìˆìŒ \n",
    "import torch.nn.functional as F # Convolution, Pooling, Activation, Linear í•¨ìˆ˜ ë“±ì´ ì •ì˜ë˜ì–´ ìˆìŒ\n",
    "\n",
    "# torch.nn.Moduleì€ PyTorchì˜ ëª¨ë“  ì‹ ê²½ë§ì˜ Base Class\n",
    "# __init()__ê³¼ forward()ë¥¼ ë°˜ë“œì‹œ override í•´ì•¼ í•œë‹¤.\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    # ì…ë ¥ ì´ë¯¸ì§€ ì±„ë„ 1ê°œ, ì¶œë ¥ ì±„ë„ 6ê°œ, 5x5ì˜ ì •ì‚¬ê° ì»¨ë³¼ë£¨ì…˜ í–‰ë ¬\n",
    "    # ì»¨ë³¼ë£¨ì…˜ ì»¤ë„ ì •ì˜\n",
    "    # nn.Conv2d(in_channels, out_channels, kernel_size, stride=1)\n",
    "    self.conv1 = nn.Conv2d(1, 6, 5)   # ì…ë ¥ ì±„ë„ í¬ê¸°, ì¶œë ¥ ì±„ë„ í¬ê¸°, ì»¤ë„ í¬ê¸°\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)  # ì…ë ¥ ì±„ë„ í¬ê¸°, ì¶œë ¥ ì±„ë„ í¬ê¸°, ì»¤ë„ í¬ê¸°\n",
    "    \n",
    "    # Fully Connected Layer: y = Wx + b\n",
    "    self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5ì€ ì´ë¯¸ì§€ ì°¨ì›ì— í•´ë‹¹\n",
    "    self.fc2 = nn.Linear(120, 60)\n",
    "    self.fc3 = nn.Linear(60, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # convolutionì„ ê±°ì¹˜ê²Œ ë˜ë©´ ì´ë¯¸ì§€ì˜ í¬ê¸°ëŠ” kernel - 1ë§Œí¼ ê°ì†Œí•¨\n",
    "    # í˜„ì¬ ì…ë ¥ë˜ëŠ” ì´ë¯¸ì§€ì˜ í¬ê¸°ê°€ 32 X 32\n",
    "    # conv1ì„ í†µí•´ ì¶œë ¥ë˜ëŠ” ì´ë¯¸ì§€ì˜ í¬ê¸°ëŠ” (32 - 5 + 1) X (32 - 5 + 1) = 6 X 28 X 28\n",
    "    # (2, 2) í¬ê¸° ìœˆë„ìš°ì— ëŒ€í•´ ë§¥ìŠ¤ í’€ë§ -> ì´ë¯¸ì§€ì˜ í¬ê¸°ëŠ” 2ë¶„ì˜ 1ì´ ë˜ë¯€ë¡œ -> ìµœì¢… ì¶œë ¥ ì´ë¯¸ì§€ í¬ê¸°ëŠ” 14 X 14\n",
    "    # torch.nn.functional.max_pool2d(input, kernel_size)\n",
    "    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # [batch size, ì±„ë„ í¬ê¸°, ì´ë¯¸ì§€ ê°€ë¡œ í¬ê¸°, ì´ë¯¸ì§€ ì„¸ë¡œ í¬ê¸°] == [1, 6, 14, 14] \n",
    "\n",
    "    # conv2ì„ í†µí•´ ì¶œë ¥ë˜ëŠ” ì´ë¯¸ì§€ì˜ í¬ê¸°ëŠ” (14 - 5 + 1) X (14 - 5 + 1) = 10 X 10\n",
    "    # (2, 2) í¬ê¸° ìœˆë„ìš°ì— ëŒ€í•´ ë§¥ìŠ¤ í’€ë§ -> ì´ë¯¸ì§€ì˜ í¬ê¸°ëŠ” 2ë¶„ì˜ 1ì´ ë˜ë¯€ë¡œ -> ìµœì¢… ì¶œë ¥ ì´ë¯¸ì§€ í¬ê¸°ëŠ” 5 X 5\n",
    "    # í¬ê¸°ê°€ ì œê³±ìˆ˜ë¼ë©´, í•˜ë‚˜ì˜ ìˆ«ìë§Œì„ íŠ¹ì •(specify)\n",
    "    x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2)) # [1, 16, 5, 5]\n",
    "    # torch.flatten(input, start_dim, end_dim) : flattens input by reshaping it into a one-dimensional tensor. \n",
    "    x = torch.flatten(x, 1) # batch ì°¨ì›ì„ ì œì™¸í•œ ëª¨ë“  ì°¨ì›ì„ í•˜ë‚˜ë¡œ í‰íƒ„í™”(flatten) [1, 16 * 5 * 5], Dimension =1 \n",
    "    x = F.relu(self.fc1(x)) # [1, 120]\n",
    "    x = F.relu(self.fc2(x)) # [1, 60]\n",
    "    x = self.fc3(x) # [1, 10]\n",
    "    return x\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32) # [batch size, ì…ë ¥ ì±„ë„ í¬ê¸°, ê°€ë¡œ ê¸¸ì´, ì„¸ë¡œ ê¸¸ì´]\n",
    "out = net(input)\n",
    "print(\"out shape:\", out.shape)\n",
    "print(out)  # out shape: (1, 10)\n",
    "\n",
    "# view í•¨ìˆ˜ : reshape í•¨ìˆ˜. í…ì„œì˜ í˜•íƒœ(Shape)ë¥¼ ë³€ê²½í•¨. ë³€ê²½ ì „ê³¼ í›„ì— ì›ì†Œì˜ ê°¯ìˆ˜ëŠ” ìœ ì§€ë˜ì–´ì•¼ í•œë‹¤.\n",
    "# view(1, -1) : ì²«ë²ˆì§¸ ì°¨ì›ì€ 1ì´ ë˜ë„ë¡ í•˜ë˜,-1ë¡œ í‘œì‹œëœ 2ë²ˆì§¸ ì°¨ì›ì€ íŒŒì´í† ì¹˜ê°€ ì•Œì•„ì„œ ê³„ì‚°í•˜ë¼ëŠ” ì˜ë¯¸\n",
    "output = net(input)               # ì…ë ¥ ë°ì´í„°ë¥¼ ì‹ ê²½ë§ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ì˜ˆì¸¡ê°’ì„ ì–»ìŒ shape: (1, 10)\n",
    "print(\"output shape:\", output.shape)\n",
    "target = torch.randn(10)          # ì„ì˜ì˜ í…ì„œë¥¼ ìƒì„±í•˜ì—¬ ì •ë‹µê°’ìœ¼ë¡œ ê°€ì •\n",
    "print (\"target shape:\", target.shape)\n",
    "target = target.view(1, -1)       # ëª¨ë¸ì˜ ì¶œë ¥ í…ì„œì™€ ë™ì¼í•œ shapeë¡œ ë³€ê²½ : (1, 10)\n",
    "print (\"after reshape(view) -> target shape:\", target.shape)\n",
    "\n",
    "@ ì†ì‹¤í•¨ìˆ˜ (Loss Function) ì„¤ì •\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ì–´ë–¤ í…ì„œê°€ í•™ìŠµì— í•„ìš”í•œ í…ì„œë¼ë©´ backpropagationì„ í†µí•˜ì—¬ gradientë¥¼ êµ¬í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "# í…ì„œì˜ ì˜µì…˜ requireds_gradë¥¼ Trueë¡œ ì„¤ì •í•˜ë©´ í…ì„œì— ì‹¤í–‰ë˜ëŠ” ëª¨ë“  ì—°ì‚°ë“¤ì„ íŠ¸ë™í‚¹í•˜ì—¬ ìë™ìœ¼ë¡œ gradientë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "input = torch.randn(3, 5, requires_grad=True) \n",
    "target = torch.randn(3, 5)\n",
    " # ì†ì‹¤í•¨ìˆ˜ë¡œ MSE(Mean Squared Error)ë¥¼ ì„¤ì •\n",
    " # í‰ê· ì œê³±ì˜¤ì°¨(MSE)ëŠ” ì˜¤ì°¨ë¥¼ ì œê³±í•œ ê°’ì˜ í‰ê· . ì˜¤ì°¨ë€ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’ê³¼ ì‹¤ì œ ì •ë‹µê³¼ì˜ ì°¨ì´\n",
    "criterion = nn.MSELoss()          \n",
    "output = criterion(input, target) # Loss ê³„ì‚°\n",
    "output.backward()\n",
    "print('input: ', input)\n",
    "print('target: ', target)\n",
    "print('output: ', output)\n",
    "\n",
    "@ ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "\n",
    "# torch.optim : ì‹ ê²½ë§ í•™ìŠµì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ êµ¬í˜„ë˜ì–´ ìˆëŠ” í´ë˜ìŠ¤\n",
    "import torch.optim as optim\n",
    "# Adam Optimizer ê°ì²´ ìƒì„±\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "# í•™ìŠµ ê³¼ì •(training loop)\n",
    "# Pytorchì—ì„œëŠ” gradients ê°’ë“¤ì„ backwardë¥¼ í•  ë•Œ ê³„ì† ëˆ„ì í•˜ê¸° ë•Œë¬¸ì— \n",
    "# í•™ìŠµì„ ì‹œì‘í•˜ê¸° ì „ì— gradients ë²„í¼ë¥¼ zeroë¡œ resetí•´ì•¼ í•œë‹¤.\n",
    "optimizer.zero_grad()\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "output = net(input)\n",
    "target = torch.randn(10)          # ì˜ˆì‹œë¥¼ ìœ„í•œ ì„ì˜ì˜ ì •ë‹µ\n",
    "target = target.view(1, -1)       # ì¶œë ¥ê³¼ ê°™ì€ shapeë¡œ ë§Œë“¬\n",
    "loss = criterion(output, target)  # Loss ê³„ì‚°\n",
    "print(loss)\n",
    "\n",
    "loss.backward()         # ì—­ì „íŒŒ í•¨ìˆ˜ ì‹¤í–‰ì„ í†µí•´ gradient ê³„ì‚°\n",
    "optimizer.step()        # gradientë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ë¥¼ ì‹¤í–‰\n",
    "\n",
    "# torch.optim : ì‹ ê²½ë§ í•™ìŠµì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ìµœì í™” ì•Œê³ ë¦¬ì¦˜ë“¤ì´ êµ¬í˜„ë˜ì–´ ìˆëŠ” íŒ©í‚¤ì§€\n",
    "import torch.optim as optim\n",
    "# Optimizer ê°ì²´ë¥¼ ìƒì„±í•˜ê³  ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì „ë‹¬, learning rate ì„¤ì •\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01) \n",
    "\n",
    "# í•™ìŠµ ê³¼ì •(training loop)\n",
    "# Pytorchì—ì„œëŠ” gradients ê°’ë“¤ì„ backwardë¥¼ í•  ë•Œ ê³„ì† ëˆ„ì í•˜ê¸° ë•Œë¬¸ì— \n",
    "# í•™ìŠµì„ ì‹œì‘í•˜ê¸° ì „ì— gradients ë²„í¼ë¥¼ zeroë¡œ resetí•´ì•¼ í•œë‹¤.\n",
    "optimizer.zero_grad()\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "output = net(input)\n",
    "target = torch.randn(10)          # ì˜ˆì‹œë¥¼ ìœ„í•œ ì„ì˜ì˜ ì •ë‹µ\n",
    "target = target.view(1, -1)       # ì¶œë ¥ê³¼ ê°™ì€ shapeë¡œ ë§Œë“¬\n",
    "loss = criterion(output, target)  # Loss ê³„ì‚°\n",
    "print(loss)\n",
    "\n",
    "loss.backward()         # ì—­ì „íŒŒ í•¨ìˆ˜ ì‹¤í–‰ì„ í†µí•´ gradient ê³„ì‚°\n",
    "optimizer.step()        # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ë¥¼ ì‹¤í–‰\n",
    "\n",
    "@ GPU ê°€ì† ì´ìš©í•˜ê¸°\n",
    "# í˜„ì¬ ê°œë°œí™˜ê²½ì—ì„œ GPU ê°€ì†ì´ ê°€ëŠ¥í•œì§€ í™•ì¸\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# GPU ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë©´ cudaë¡œ ì—°ì‚°í•˜ë„ë¡ deviceë¥¼ ì„¤ì • ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ cpuë¡œ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "# ëœë¤í•œ í•™ìŠµ ë°ì´í„° ìƒì„±\n",
    "train_inputs = torch.randn(100, 32, 128) # [ì „ì²´ ë°ì´í„° ê°œìˆ˜, ë°ì´í„°ì˜ ìµœëŒ€ ê¸¸ì´, íˆë“  ë²¡í„° í¬ê¸°]\n",
    "train_labels = torch.randn(100, 3) # [ì „ì²´ ë°ì´í„° ê°œìˆ˜, ì˜ˆì¸¡í•  í´ë˜ìŠ¤ ê°œìˆ˜]\n",
    "\n",
    "# Datasetì— í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„í¬íŠ¸\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 12 # ë§¤ í•™ìŠµ Step ë§ˆë‹¤ ìƒ˜í”Œë§í•  ë°ì´í„° ê°œìˆ˜\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_labels) # í•™ìŠµë°ì´í„°ì™€ ë ˆì´ë¸”ì„ í•˜ë‚˜ì˜ TensorDatasetìœ¼ë¡œ ê²°í•© ê°€ëŠ¥\n",
    "train_sampler = RandomSampler(train_data) # ë°ì´í„°ë¥¼ ìƒ˜í”Œë§ í•  í•¨ìˆ˜(ìˆœì°¨ì ìœ¼ë¡œ ë½‘ì•„ì˜¬ì§€, ëœë¤í•˜ê²Œ ë½‘ì•„ì˜¬ì§€)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) # ë¯¸ë‹ˆë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ìë™ ë¡œë”©\n",
    "\n",
    "# dataloaderë¥¼ í†µí•´ì„œ í•™ìŠµ ë£¨í”„ ìƒì„±\n",
    "for step, batch in enumerate(train_dataloader): # enumerate : ì¸ë±ìŠ¤(index)ì™€ ë°ì´í„°ê°’ì— ë™ì‹œì— ì ‘ê·¼\n",
    "  x_data, x_label = batch\n",
    "  # ë°ì´í„° ë¡œë”© step \n",
    "  print(step, x_data.shape, x_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84734415-ad67-49b5-b724-1be90b122dbe",
   "metadata": {},
   "source": [
    "### Sentiment_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd493a93-2dc8-4ce4-ac4a-0a339c8ba77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext.legacyë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” torchtext ë²„ì „ ì„¤ì¹˜\n",
    "!pip install -U torchtext==0.10.0\n",
    "\n",
    "#colab ì„ ì´ìš©í•œ ì‹¤í–‰ì‹œ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "import torch\n",
    "# torch.nn : ì‹ ê²½ë§ êµ¬í˜„ì„ ìœ„í•œ ë°ì´í„° êµ¬ì¡°, ì‹ ê²½ë§ ë ˆì´ì–´, ê´€ë ¨í•¨ìˆ˜ë“¤ì´ êµ¬í˜„ë˜ì–´ ìˆëŠ” íŒ©í‚¤ì§€\n",
    "# torch.nn.functional: torch.nn íŒ©í‚¤ì§€ì˜ í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì–´ ìˆìŒ (ì†ì‹¤í•¨ìˆ˜, í™œì„±í™”í•¨ìˆ˜, í’€ë§í•¨ìˆ˜ ë“±) \n",
    "# torch. autograd : ë¯¸ë¶„ì„ ìœ„í•œ í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì–´ ìˆìŒ\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#import torchtext.data as data\n",
    "#import torchtext.datasets as datasets\n",
    "#legacy ë²„ì „ìœ¼ë¡œ ë³€ê²½\n",
    "\n",
    "# torchtext : textì˜ preprocessing íŒŒì´í”„ë¼ì¸ ì •ì˜, \n",
    "# í† í¬ë‚˜ì´ì§•, Vocab ìƒì„±, dataset splits, ë°ì´í„° ë¡œë” ë“± ì§€ì›\n",
    "from torchtext.legacy import data\n",
    "import torchtext.datasets as datasets\n",
    "import pickle\n",
    "\n",
    "# CNN ëª¨ë¸ì˜ êµ¬ì¡°ì™€ ì—°ì‚°ì„ ì •ì˜\n",
    "class CNN_Text(nn.Module):\n",
    "    # ìƒì„±ì : ëª¨ë¸ì˜ êµ¬ì¡°ì™€ ë™ì‘ì„ ì •ì˜\n",
    "    # ê°ì²´ê°€ ê°–ëŠ” ì†ì„±ê°’ì„ ì´ˆê¸°í™”í•¨. ê°ì²´ê°€ ìƒì„±ë  ë•Œ ìë™ìœ¼ë¡œ í˜¸ì¶œëœë‹¤.\n",
    "    def __init__(self, embed_num, class_num):\n",
    "        super(CNN_Text, self).__init__() # nn.Module í´ë˜ìŠ¤ë¥¼ ì´ˆê¸°í™”(ë¶€ëª¨í´ë˜ìŠ¤ë¥¼ ì´ˆê¸°í™”)\n",
    "        # V: ì‚¬ì „ì˜ í¬ê¸°(vocavilary)\n",
    "        # D: embed_dim (ë‹¨ì–´ ë²¡í„°ì˜ ì°¨ì›)\n",
    "        # C: ë¶„ë¥˜í•˜ê³ ì í•˜ëŠ” í´ë˜ìŠ¤ì˜ ê°œìˆ˜(ê¸ì • ë¶€ì • 2ê°œ í´ë˜ìŠ¤)\n",
    "        # Co : ê° ì»¤ë„(í•„í„°)ì˜ ê°¯ìˆ˜\n",
    "        V = embed_num\n",
    "        D = 100 \n",
    "        C = class_num\n",
    "        Co = 50         # output channel ìˆ˜ (í•„í„°ì˜ ê°¯ìˆ˜)\n",
    "        Ks = [2,3,4]\n",
    "\n",
    "        # ì‚¬ì „ì— ìˆëŠ” ëª¨ë“  ë‹¨ì–´ ë²¡í„°ì— random ì´ˆê¸°ê°’\n",
    "        self.embed = nn.Embedding(V, D) \n",
    "        # torch.nn.Conv2d (in_channels, out_channels, kernel_size, stride=1)\n",
    "        # convs1ì— ì»¨ë³¼ë£¨ì…˜ ëª¨ë“ˆì˜ ë¦¬ìŠ¤íŠ¸ê°€ ë“¤ì–´ê° (í•„í„°(ì»¤ë„) ê°¯ìˆ˜ë§Œí¼) \n",
    "        # forwardì—ì„œ ìˆœì°¨ì ìœ¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(1, Co, (K, 100)) for K in Ks])\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # nn.Linear í´ë˜ìŠ¤. Fully Connected Layer \n",
    "        # Applies a linear transformation to the incoming data (y = Wx + b)\n",
    "        # torch.nn.Linear(in_features, out_features)\n",
    "        # in_features: size of each input sample, out_features: size of each output sample \n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C) # 150(3*50), 2 \n",
    "\n",
    "        # foward í•¨ìˆ˜ : ëª¨ë¸ì´ í•™ìŠµë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„ì„œ forward ì—°ì‚°ì„ ì§„í–‰\n",
    "        # model ê°ì²´ë¥¼ ë°ì´í„°ì™€ í•¨ê»˜ í˜¸ì¶œí•˜ë©´ ìë™ìœ¼ë¡œ ì‹¤í–‰ëœë‹¤.\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)   # (N, W, D) ë¯¸ë‹ˆë°°ì¹˜, ë¬¸ì¥ ìµœëŒ€ê¸¸ì´, ë‹¨ì–´ë²¡í„° ì°¨ì›\n",
    "        x = x.unsqueeze(1)  # (N x Ci x W x D) Conv2dë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì…ë ¥ì±„ë„ ìˆ˜ ì¶”ê°€í•´ì•¼ í•¨\n",
    "\n",
    "        # Convolution Layer\n",
    "        # Convolution -> ReLU -> í…ì„œì˜ dimension 3ì„ squeeze(max_pool1dëŠ” 3D ì…ë ¥ì„ ë°›ìŒ)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]\n",
    "\n",
    "        # Max Pooling\n",
    "        # F.max_pool1d(input, kernel_size): Applies a 1D max pooling over an input\n",
    "        # Tensor.size(dim=None) : Returns the size of the self tensor. If dim is specified, returns the size of that dimension.\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks) max pooling í›„ì— ë§ˆì§€ë§‰ ì°¨ì›ì€ 1 -> squeeze\n",
    "        x = torch.cat(x, 1) # torch.cat(tensors, dim), dim=1ì´ë©´ ë‘ë²ˆì§¸ ì°¨ì›ì´ ëŠ˜ì–´ë‚˜ê²Œ concat (ì²«ë²ˆì§¸ ì°¨ì›ì€ N)\n",
    "        x = self.dropout(x) # (N, len(Ks)*Co), dropoutì„ ì ìš©\n",
    "        logit = self.fc1(x) # fully-connected layer ì ìš©\n",
    "        return logit\n",
    "    \n",
    "class mydataset(data.Dataset):\n",
    "    @staticmethod  # ìœ í‹¸ë¦¬í‹° ë©”ì†Œë“œ ì •ì˜ ì‹œ ì„ ì–¸\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "    def __init__(self, text_field, label_field, path=None, examples=None, **kwargs):\n",
    "        fields = [('text', text_field), ('label', label_field)] # text_field ë ˆì´ë¸”ì€ text, label_field ë ˆì´ë¸”ì€ label\n",
    "        if examples is None:\n",
    "            path = self.dirname if path is None else path\n",
    "            examples = []\n",
    "            for i,line in enumerate(open(path,'r',encoding='utf-8')):\n",
    "                if i==0:\n",
    "                    continue\n",
    "                line = line.strip().split('\\t')\n",
    "                txt = line[1].split(' ')               \n",
    "                                  \n",
    "                examples += [ data.Example.fromlist( [ txt, line[2]],fields ) ]\n",
    "        super(mydataset, self).__init__(examples, fields, **kwargs)\n",
    "        \n",
    "# Field : í…ì„œë¡œ ë³€í™˜ë  í…ìŠ¤íŠ¸ ë°ì´í„°íƒ€ì…ì„ ì •ì˜ \n",
    "# text_field, label_field : ì „ì²˜ë¦¬ ê´€ë ¨ëœ field ê°ì²´ë¥¼ ê°ê° ìƒì„± \n",
    "# batch_first : ë¯¸ë‹ˆë°°ì¹˜ ì°¨ì›ì„ ë§¨ ì•ì— ë‘” í…ì„œë¥¼ ìƒì„±í•  ê²ƒì¸ì§€\n",
    "# fix_length : í•˜ë‚˜ì˜ ë¬¸ì¥ ë‚´ max í† í°ìˆ˜ \n",
    "# sequential : ì‹œí€€ìŠ¤ ë°ì´í„° ì—¬ë¶€\n",
    "text_field = data.Field(batch_first = True, fix_length = 20)\n",
    "label_field = data.Field(sequential= False, batch_first = True, unk_token = None) # unk_tokenì„ í‘œí˜„í•  ìŠ¤íŠ¸ë§\n",
    "\n",
    "train_data = mydataset(text_field,label_field,path='/content/gdrive/My Drive/aivle/data/nsm/small_ratings_train_tok.txt')\n",
    "\n",
    "test_data = mydataset(text_field,label_field,path='/content/gdrive/My Drive/aivle/data/nsm/small_ratings_test_tok.txt')\n",
    "#print(test_data.fields.items())\n",
    "\n",
    "# vocab ìƒì„±\n",
    "text_field.build_vocab(train_data)\n",
    "label_field.build_vocab(train_data)\n",
    "\n",
    "# Data Loader ìƒì„± (train_data, test_dataë¥¼ ê°ê° 100ê°œ, 1ê°œì”© ë°ì´í„° ë¡œë”©)\n",
    "train_iter, test_iter = data.Iterator.splits(\n",
    "                            (train_data, test_data), \n",
    "                            batch_sizes=(100, 1))#, device= 'cuda')\n",
    "len(text_field.vocab)\n",
    "\n",
    "\n",
    "# CNNëª¨ë¸ ê°ì²´ë¥¼ ìƒì„± (embed_num, class_num)\n",
    "cnn = CNN_Text(len(text_field.vocab),2)\n",
    "\n",
    "# torch.optim : ì‹ ê²½ë§ í•™ìŠµì„ ìœ„í•œ ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ êµ¬í˜„ë˜ì–´ ìˆëŠ” íŒ©í‚¤ì§€\n",
    "# Optimizerë¥¼ ì„¤ì •\n",
    "optimizer = torch.optim.Adam(cnn.parameters())\n",
    "cnn.train()\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    totalloss = 0\n",
    "    for batch in train_iter:\n",
    "        optimizer.zero_grad() # resets the gradient to 0\n",
    "        \n",
    "        txt = batch.text\n",
    "        label = batch.label\n",
    "                \n",
    "        #print(txt.size()) -> torch.Size([100, 20])\n",
    "        pred = cnn(txt)\n",
    "                \n",
    "        #print(pred.size(), label.size()) -> torch.Size([100, 2]) torch.Size([100])\n",
    "        #print(label)\n",
    "        loss = F.cross_entropy(pred, label)\n",
    "        totalloss += loss.data\n",
    "        \n",
    "        loss.backward() # backward ì—°ì‚°\n",
    "        optimizer.step() # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
    "        \n",
    "    print(epoch,'epoch')    \n",
    "    print('loss : {:.3f}'.format(totalloss.numpy()))\n",
    "\n",
    "torch.save(cnn,'/content/gdrive/My Drive/aivle/cnn_model.pt')\n",
    "\n",
    "%%time\n",
    "from sklearn.metrics import classification_report\n",
    "cnn.eval() # ëª¨ë¸ì„ evaluation modeë¡œ ì„¤ì •. ì •ê·œí™” ê¸°ìˆ (dropout ë“±)ì„ ë°°ì œí•˜ì—¬ ì˜¨ì „í•œ ëª¨ë¸ë¡œ í‰ê°€\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "y_test = []\n",
    "prediction = []\n",
    "\n",
    "for batch in test_iter:\n",
    "    txt = batch.text\n",
    "    label = batch.label\n",
    "    y_test.append(label.data[0])\n",
    "\n",
    "    pred = cnn(txt)\n",
    "    _,ans = torch.max(pred,dim=1) # dimensionì„ ê¸°ì¤€ìœ¼ë¡œ (ìµœëŒ€ê°’, ìµœëŒ€ê°’ì´ ìˆëŠ” ì¸ë±ìŠ¤) ë°˜í™˜ \n",
    "    prediction.append(ans.data[0]) # ans.data[0]: ìµœëŒ€ê°’ì´ ë“¤ì–´ìˆëŠ” ì¸ë±ìŠ¤ (0 ë˜ëŠ” 1) \n",
    "  \n",
    "    if ans.data[0] == label.data[0]:        \n",
    "        correct += 1    \n",
    "    else:\n",
    "        incorrect += 1\n",
    "    \n",
    "print ('correct : ', correct)\n",
    "print ('incorrect : ', incorrect)\n",
    "print(classification_report(torch.tensor(y_test),     # ì •ë‹µê°’\n",
    "                            torch.tensor(prediction), # ì˜ˆì¸¡ê°’\n",
    "                            digits=4,                 # ì¶œë ¥í•  ìë¦¬ìˆ˜\n",
    "                            target_names=['negative', 'positive'])) # display names matching the label\n",
    "\n",
    "# Weighted AvgëŠ” í´ë˜ìŠ¤ì˜ ìˆ˜ì¹˜ê°„ì˜ í‰ê·  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f846a99-d807-4fe0-8717-6dee0470331b",
   "metadata": {},
   "source": [
    "### SonnyBot_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05847b54-2284-4293-b14f-4715543d4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "    \n",
    "cd Mecab-ko-for-Google-Colab\n",
    "\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh \n",
    "\n",
    "def mecabsplit(mecab_tagger,inputs, pos):\n",
    "    r=[]\n",
    "    inputs = mecab_tagger.parse(inputs)\n",
    "    t = inputs.split('\\n')[:-2]\n",
    "    for i in t:\n",
    "        field = i.split('\\t')\n",
    "        if field[1].split(',')[-1] is not '*':\n",
    "            r.extend( [ (x.split('/')[0],x.split('/')[1]) for x in field[1].split(',')[-1].split('+') ] )\n",
    "        else:\n",
    "            r.append( (field[0],field[1].split(',')[0]) )\n",
    "    if pos:\n",
    "        return r\n",
    "    else:\n",
    "        return [ x[0] for x in r ]\n",
    "    return r\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable # tensorë¡œ í•´ë„ë¨\n",
    "import pickle\n",
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "\n",
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_num, class_num, ):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        \n",
    "        # ë‹¨ì–´ ì‚¬ì „ í¬ê¸°\n",
    "        V = embed_num\n",
    "        # ì„ë² ë”©ë²¡í„° í¬ê¸°\n",
    "        D = 100 #args.embed_dim\n",
    "        # ë¶„ë¥˜í•˜ê³ ì í•˜ëŠ” í´ë˜ìŠ¤ì˜ ê°œìˆ˜\n",
    "        C = class_num\n",
    "        # ì…ë ¥ ì±„ë„ ìˆ˜\n",
    "        Ci = 1\n",
    "        # ì¶œë ¥ ì±„ë„ ìˆ˜\n",
    "        Co = 20 #args.kernel_num\n",
    "        # ì»¤ë„(í•„í„°) ì‚¬ì´ì¦ˆ 1ë‹¨ì–´, 2ë‹¨ì–´, 3ë‹¨ì–´\n",
    "        Ks = [1,2,3]\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        # padding numbers for (height,width)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D), padding=(2,0)) for K in Ks])\n",
    "        # dropout ì„¤ì •\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # FC ë ˆì´ì–´\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # (B, W, D)\n",
    "        \n",
    "        # ì…ë ¥ xë¥¼ 4Dë¡œ ë³€í™˜\n",
    "        x = x.unsqueeze(1)  # (B(batch), Ci(input channel), W(sent), D(dimension))\n",
    "        # output = F.relu(x) -> B x Co x W x 1\n",
    "        # max_pool1DëŠ” 3D ì…ë ¥ë§Œ ë°›ìŒ -> size 1ì¸ ì°¨ì›ì„ ì œê±°(squeeze)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(B, Co, W), ...]*len(Ks)\n",
    "        \n",
    "        # (B x Co x 1) -> size 1ì¸ ì°¨ì›ì„ ì œê±°(squeeze)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(B, Co), ...]*len(Ks)\n",
    "        \n",
    "        # concatenate\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        x = self.dropout(x)  # (B, len(Ks)*Co)\n",
    "        logit = self.fc1(x)  \n",
    "        return logit\n",
    "    \n",
    "# í•™ìŠµë°ì´í„°ì— ë‚˜íƒ€ë‚œ ë‹¨ì–´ ì‚¬ì „\n",
    "content_vocab = {'unk':0}\n",
    "# ì˜ë„ ë ˆì´ë¸” ì‚¬ì „\n",
    "intent_vocab={}\n",
    "# ì˜ë„ í´ë˜ìŠ¤\n",
    "intent_list=[]\n",
    "\n",
    "data_intent=''\n",
    "intent_idx=0\n",
    "vocab_idx=1\n",
    "\n",
    "for line in open('/content/gdrive/My Drive/aivle/data/sonny/mydata.txt','r',encoding='utf-8'):\n",
    "    line = line.strip().split('\\t')\n",
    "    if len(line)>1:\n",
    "        intent=line[1]\n",
    "        if intent not in intent_vocab:\n",
    "            intent_vocab[intent]=intent_idx\n",
    "            intent_list.append(intent)\n",
    "            intent_idx +=1\n",
    "    else:\n",
    "        line = mecabsplit(tagger,line[0],False)\n",
    "        for it in line:\n",
    "            if it not in content_vocab:\n",
    "                content_vocab[it] = vocab_idx\n",
    "                vocab_idx +=1\n",
    "                \n",
    "cnn = CNN_Text(vocab_idx,intent_idx)\n",
    "print(vocab_idx, intent_idx) # íŒŒì¼ì— ë‚˜íƒ€ë‚œ ë‹¨ì–´ìˆ˜, ì˜ë„ ê°¯ìˆ˜(Class ê°¯ìˆ˜)\n",
    "optimizer = torch.optim.Adam(cnn.parameters())\n",
    "cnn.train()\n",
    "\n",
    "epoch = 10\n",
    "for e in range(epoch):\n",
    "    totalloss = 0\n",
    "    for line in open('/content/gdrive/My Drive/aivle/data/sonny/mydata.txt','r',encoding='utf-8'):\n",
    "        line = line.strip().split('\\t')\n",
    "    \n",
    "        if len(line)> 1:\n",
    "            target = Variable(torch.LongTensor([intent_vocab[line[1]]]))\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cont = []\n",
    "        line = mecabsplit(tagger,line[0],False)\n",
    "        for it in line:\n",
    "            cont.append(content_vocab[it])\n",
    "        # view : ì›ì†Œì˜ ìˆ˜ë¥¼ ìœ ì§€í•˜ë©´ì„œ í…ì„œë¥¼ reshape, í…ì„œì˜ ì²«ë²ˆì§¸ ì°¨ì›ì„ 1ë¡œ reshape\n",
    "        cont = Variable(torch.LongTensor(cont)).view(1,-1)\n",
    "        pred = cnn(cont)\n",
    "\n",
    "        loss = F.cross_entropy(pred,target)\n",
    "        totalloss += loss.data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print (e, 'epoch')\n",
    "    print('loss : {:.3f}'.format(totalloss.numpy()))\n",
    "    \n",
    "response = []\n",
    "for line in open('/content/gdrive/My Drive/aivle/data/sonny/response.txt','r',encoding='utf-8'):\n",
    "    line=line.strip()\n",
    "    response.append(line)\n",
    "    \n",
    "    \n",
    "cnn.eval()\n",
    "for line in open('/content/gdrive/My Drive/aivle/data/sonny/testdata.txt','r',encoding='utf-8'):\n",
    "    line = line.strip()\n",
    "    \n",
    "    line = mecabsplit(tagger,line,False)\n",
    "    cont = []\n",
    "    for it in line:\n",
    "        if it in content_vocab:\n",
    "            cont.append(content_vocab[it]) # contì—ëŠ” ì…ë ¥ë¬¸ì— ë‚˜íƒ€ë‚œ ë‹¨ì–´ë“¤ì˜ index ì €ì¥ë¨\n",
    "        else:\n",
    "            cont.append(content_vocab['unk'])\n",
    "    cont = Variable(torch.LongTensor(cont)).view(1,-1)\n",
    "    pred = cnn(cont)\n",
    "    v,i = torch.max(pred,1) # predëŠ” (p1, p2) ì¦‰, í´ë˜ìŠ¤ë³„ í™•ë¥  v: ë‘˜ì¤‘ í°ê°’ i:í°ê°’ í´ë˜ìŠ¤ì˜ ì¸ë±ìŠ¤\n",
    "    \n",
    "    print('input : ',line)\n",
    "    # 3ê°œ í´ë˜ìŠ¤ì˜ í™•ë¥ ê°’\n",
    "    probs = torch.nn.functional.softmax(pred,dim=-1).data.numpy()[0]\n",
    "    print ([probs[0], probs[1], probs[2]])\n",
    "    print('intent : ',intent_list[int(i)])\n",
    "    print(response[int(i)])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b816a2-7eb4-43c4-af4e-924f251533d5",
   "metadata": {},
   "source": [
    "### Sentiment_RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df4e17-d087-42af-9aed-ccb4ff5022af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext.legacyë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” torchtext ë²„ì „ ì„¤ì¹˜\n",
    "!pip install -U torchtext==0.10.0\n",
    "\n",
    "#colab ì„ ì´ìš©í•œ ì‹¤í–‰ì‹œ\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# torchtext.legacy : textì˜ preprocessing íŒŒì´í”„ë¼ì¸ ì •ì˜\n",
    "# 1) í† í¬ë‚˜ì´ì§•(Tokenization)\n",
    "# 2) ë‹¨ì–´ì¥ ìƒì„±(Build Vocabulary)\n",
    "# 3) í† í°ì˜ ìˆ˜ì¹˜í™”(Numericalize all tokens)\n",
    "# 4) ë°ì´í„° ë¡œë” ìƒì„±(Create Data Loader)\n",
    "from torchtext.legacy import data\n",
    "import torchtext.datasets as datasets\n",
    "\n",
    "import pickle\n",
    "print (torch.__version__)\n",
    "\n",
    "class RNN_Text(nn.Module):   # init, forwardë¥¼ ê¼­ ì¬ì •ì˜ í•´ì¤˜ì•¼í•¨ \n",
    "    def __init__(self, embed_num, class_num):\n",
    "        # super()ë¡œ Base Classì˜ __init__() í˜¸ì¶œ (nn.Module í´ë˜ìŠ¤ ìƒì„±ì í˜¸ì¶œ)\n",
    "        # super(íŒŒìƒí´ë˜ìŠ¤, self).__init__() íŒŒì´ì¬ 2.x ë¬¸ë²•\n",
    "        # super().__init__() íŒŒì´ì¬ 3.x ë¬¸ë²• ë‘˜ë‹¤ ì‚¬ìš© ê°€ëŠ¥\n",
    "        super(RNN_Text, self).__init__()\n",
    "          \n",
    "        V = embed_num   # ë‹¨ì–´ ì‚¬ì „ì˜ í¬ê¸°\n",
    "        C = class_num   # ë¶„ë¥˜í•˜ê³ ì í•˜ëŠ” í´ë˜ìŠ¤ ê°œìˆ˜        \n",
    "        H = 256         # íˆë“  ì‚¬ì´ì¦ˆ\n",
    "        D = 100         # ë‹¨ì–´ë²¡í„° ì°¨ì› 100        \n",
    "        self.embed = nn.Embedding(V, D)        \n",
    "        \n",
    "        # LSTM Layer, bidirectionalì´ë¯€ë¡œ ì¶œë ¥ë˜ëŠ” ë²¡í„°ì˜ í¬ê¸°ëŠ” H * 2\n",
    "        self.rnn = nn.LSTM(D, H, bidirectional = True) # bidirectional = ì–‘ë°©í–¥ì˜ ì—¬ë¶€\n",
    "                 \n",
    "        # Linear Layer : (512, 2) # ê¸ì •ë¶€ì •(2ê°œ)\n",
    "        self.out = nn.Linear(H*2, C) # ë§ˆì§€ë§‰ ìƒíƒœì˜ íˆë“ ìŠ¤í…Œì´íŠ¸ê°€ ë¨(ì–‘ë°©í–¥ì´ ë˜ê¸° ë•Œë¬¸ì— 256*2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)     # (N, W, D) ë¬¸ì¥ xì˜ ë‹¨ì–´ ë²¡í„°ê°’ ê°€ì ¸ì˜´ (ì„ë² ë”©í•´ì¤Œ) ì°¨ì›ì„ ë‚˜íƒ€ëƒ„, í•œë¬¸ì¥ê³¼ batch sizeë¥¼ ê³±í•´ì¤Œ\n",
    "      \n",
    "        # LSTM ëª¨ë“ˆ ì‹¤í–‰\n",
    "        # LSTM ì…ë ¥ë°ì´í„°\n",
    "        # input x : torch.Size([30, 100, 100]) [ì‹œí€€ìŠ¤ ê¸¸ì´, ë°°ì¹˜ ì‚¬ì´ì¦ˆ, Dimension] -> ë¬¸ì¥í•˜ë‚˜ë‹¤\n",
    "        x,(_,__) = self.rnn( x, ( self.h, self.c ) )  # ëª‡ì°¨ì›ì˜ í…ì„œê°€ ë“¤ì–´ê°€ê³  ë‚˜ì˜¤ëŠ”ì§€ê°€ ì¤‘ìš”í•¨\n",
    "\n",
    "        # output x : torch.Size([30, 100, 512]) [ì‹œí€€ìŠ¤ ê¸¸ì´, ë°°ì¹˜ ì‚¬ì´ì¦ˆ, 256 * 2]# ë§ˆì§€ë§‰ ìƒíƒœì˜ íˆë“ ìŠ¤í…Œì´íŠ¸ê°€ ë¨\n",
    "        # print('output * size', x.size)\n",
    "        # ìµœì¢… Hidden Layerë¡œ Linear ëª¨ë“ˆ ì‹¤í–‰   \n",
    "        logit = self.out(x[-1])  # ì²«ë²ˆì§¸ ì°¨ì›, ì¦‰ ì‹œí€€ìŠ¤ì°¨ì›ì— ìˆëŠ” ë§ˆì§€ë§‰ ë‹¨ì–´ì˜ ë°°ì¹˜ê°’ì´ ë‹´ê²¨ìˆëŠ” tensorë¥¼ ê°€ì ¸ì˜´\n",
    "\n",
    "        # ìµœì¢… ì˜ˆì¸¡ ë²¡í„° í¬ê¸°: [ë°°ì¹˜ ì‚¬ì´ì¦ˆ, C], C: í´ë˜ìŠ¤ ê°œìˆ˜\n",
    "        return logit       # logit : torch.Size([100, 2])\n",
    "\n",
    "    def inithidden(self, b): # íˆë“ ìŠ¤í…Œì´íŠ¸ ì´ˆê¸°í™” í•¨ìˆ˜(cell, hidden ë‘˜ë‹¤ ëœë¤ìœ¼ë¡œ ì´ˆê¸°í™”)\n",
    "        #self.h = Variable(torch.randn(2, b, 256))\n",
    "        #self.c = Variable(torch.randn(2, b, 256))    \n",
    "        self.h = torch.randn(2, b, 256)   # [2, batch_size, 256]\n",
    "        self.c = torch.randn(2, b, 256)   # [2, batch_size, 256]\n",
    "        \n",
    "        \n",
    "# train, test datasetì„ ë§Œë“¤ì–´ì¤€ë‹¤\n",
    "class mydataset(data.Dataset):\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "    def __init__(self, text_field, label_field, path=None, examples=None, **kwargs):\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "        if examples is None:\n",
    "            path = self.dirname if path is None else path\n",
    "            examples = []\n",
    "            for i,line in enumerate(open(path,'r',encoding='utf-8')):\n",
    "                if i==0:      # ì²«ë²ˆì§¸ ë¼ì¸ì€ skip\n",
    "                    continue\n",
    "                line = line.strip().split('\\t') # text, label í•„ë“œê°€ /tabìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆë‹¤                  \n",
    "                txt = line[1].split(' ')  # ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ìì—´ì„ ë‚˜ëˆ„ì–´ í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“ ë‹¤. line[0]ì—ëŠ” ID\n",
    "               \n",
    "                # examples: í•™ìŠµ í…ìŠ¤íŠ¸, ë¼ë²¨ í…ìŠ¤íŠ¸\n",
    "                # data.Example : Defines a single training or test example.\n",
    "                examples += [ data.Example.fromlist( [txt, line[2]],fields ) ]\n",
    "        # Create a dataset from a list of Examples and Fields.\n",
    "        # fields : field name, field \n",
    "        super(mydataset, self).__init__(examples, fields, **kwargs) \n",
    "\n",
    "        \n",
    "# Field ê°ì²´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê°’ì„ í†µí•˜ì—¬ ë°ì´í„°ì˜ ê° í•„ë“œë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì§€ì •\n",
    "# fix_length: A fixed length that all examples using this field will be padded to, or None for flexible sequence lengths. \n",
    "# sequential: Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.\n",
    "# batch_first: Whether to produce tensors with the batch dimension first. Default: False.\n",
    "##text_field = data.Field(fix_length=20)\n",
    "text_field = data.Field(fix_length=30)\n",
    "label_field = data.Field(sequential=False, batch_first = True, unk_token = None)\n",
    "\n",
    "# í•™ìŠµë°ì´í„° Dataset\n",
    "train_data = mydataset(text_field,label_field,path='/content/gdrive/My Drive/aivle/data/nsm/small_ratings_train_tok.txt')\n",
    "# í…ŒìŠ¤íŠ¸ë°ì´í„° Dataset\n",
    "test_data = mydataset(text_field,label_field,path='/content/gdrive/My Drive/aivle/data/nsm/small_ratings_test_tok.txt')\n",
    "\n",
    "text_field.build_vocab(train_data)    # Construct the Vocab object \n",
    "label_field.build_vocab(train_data)   # Construct the Vocab object \n",
    "\n",
    "# Create Iterator objects for train data, test data\n",
    "train_iter, test_iter = data.Iterator.splits(\n",
    "                            (train_data, test_data), \n",
    "                            batch_sizes=(100, 1), repeat=False)#, device = -1)\n",
    "len(text_field.vocab)\n",
    "\n",
    "rnn = RNN_Text(len(text_field.vocab),2)     # embed_num, class_num\n",
    "optimizer = torch.optim.Adam(rnn.parameters())\n",
    "rnn.train()\n",
    "\n",
    "%%time\n",
    "bool_debug = True    # í…ì„œì˜ ì°¨ì›ì„ ì¶œë ¥í•  ê²½ìš° Trueë¡œ ì„¤ì •\n",
    "print_idx = 3        # ì¶œë ¥ íšŸìˆ˜\n",
    "for epoch in range(10):\n",
    "    \n",
    "    totalloss = 0\n",
    "    for batch in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        txt = batch.text        # torch.Size([30, 100])\n",
    "        label = batch.label     # torch.Size([100])\n",
    "        \n",
    "        if bool_debug and print_idx > 0:\n",
    "          # print('txt.size():', txt.shape)\n",
    "          print (\"txt.shape:\", txt.shape)\n",
    "          print_idx -= 1\n",
    "\n",
    "        # inithiddend : hidden state, cell state ì´ˆê¸°í™” í•¨ìˆ˜\n",
    "        rnn.inithidden(txt.size(1))   # ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì „ë‹¬\n",
    "        # í•™ìŠµ ì‹¤í–‰\n",
    "        pred = rnn(txt)\n",
    "        \n",
    "        if bool_debug and print_idx > 0:\n",
    "          print(\"pred.shape:\", pred.shape)\n",
    "          print(\"label.shape:\", label.shape)\n",
    "          print_idx -= 1        \n",
    "\n",
    "        loss = F.cross_entropy(pred, label)\n",
    "        totalloss += loss.data\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(epoch,'epoch')  \n",
    "    print('loss : {:.3f}'.format(totalloss.numpy()))\n",
    "       \n",
    "torch.save(rnn,'/content/gdrive/My Drive/aivle/model/rnn_model.pt')\n",
    "\n",
    "%%time\n",
    "bool_debug = True    # í…ì„œì˜ ì°¨ì›ì„ ì¶œë ¥í•  ê²½ìš° Trueë¡œ ì„¤ì •\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "rnn.eval()\n",
    "y_test = []\n",
    "prediction = []\n",
    "\n",
    "# í…ì„œ ì°¨ì› í™•ì¸ìš©\n",
    "print_tensor_shape = 2\n",
    "print_idx = 1\n",
    "\n",
    "for batch in test_iter:\n",
    "    txt = batch.text            # txt.shape: torch.Size([max_sent_len, 1])\n",
    "    label = batch.label         # label.shape: torch.Size([1])\n",
    "    y_test.append(label.data[0])\n",
    "    \n",
    "    rnn.inithidden(txt.size(1))\n",
    "   \n",
    "    pred = rnn(txt)               # pred.shape: torch.Size([1, 2])\n",
    "    \n",
    "    _ , ans = torch.max(pred,dim=1) # ans.shape: torch.Size([1])\n",
    "    prediction.append(ans.data[0])\n",
    "    \n",
    "    \n",
    "    #---------------------------------------\n",
    "    # í…ì„œ í˜•íƒœ, ë°ì´í„°ë¥¼ ì¶œë ¥\n",
    "    if bool_debug and print_tensor_shape > 0:\n",
    "      print(\"-----\", print_idx, \"-----\") \n",
    "      print(\"prediction:\", prediction)\n",
    "      print(\"y_test:\", y_test)\n",
    "      print(\"pred.shape:\", pred.shape)\n",
    "      #print(\"pred.data[0]:\", pred.data[0])\n",
    "      print(\"pred[0]:\", pred[0])\n",
    "      print(\"pred[0][0]:\", pred[0][0])\n",
    "      print(\"pred[0][1]:\", pred[0][1])\n",
    "      print(\"ans.data[0]:\", ans.data[0])\n",
    "      print(\"ans.shape:\", ans.shape)\n",
    "      print(\"txt.shape:\", txt.shape)\n",
    "      print(\"label.shape:\", label.shape)\n",
    "      print(\"label.data[0]:\", label.data[0])\n",
    "      \n",
    "      print()\n",
    "      print_tensor_shape -= 1\n",
    "      print_idx += 1\n",
    "      #---------------------------------------\n",
    "\n",
    "    if ans.data[0] == label.data[0]:  # ans.data[0]: tensor(0) ë˜ëŠ” tensor(1)\n",
    "        correct += 1    \n",
    "    else:\n",
    "        incorrect += 1\n",
    "    \n",
    "print ('correct : ', correct)\n",
    "print ('incorrect : ', incorrect)\n",
    "print(classification_report(torch.tensor(y_test), \n",
    "                            torch.tensor(prediction), \n",
    "                            digits=4, \n",
    "                            target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b004148-0bdf-40c8-bda7-59a467b53854",
   "metadata": {},
   "source": [
    "### Intent_analysis_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bdf9f-032c-4503-b3bc-9024db268821",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "%cd Mecab-ko-for-Google-Colab\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh \n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable \n",
    "import pickle \n",
    "\n",
    "import MeCab\n",
    "tagger = MeCab.Tagger()\n",
    "#localì—ì„œ ì‹¤í–‰í•  ê²½ìš° ì°¸ê³ ì‚¬í•­ : ì‚¬ì „ ë””ë ‰í† ë¦¬ë¥¼ ì¸ìë¡œ ì „ë‹¬í•  ìˆ˜ ìˆìŒ\n",
    "#tagger = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "df=pd.read_csv('/content/gdrive/My Drive/aivle/data/college/college_FAQ.csv',encoding='utf-8', on_bad_lines='skip')\n",
    "df.tail()\n",
    "\n",
    "df.category.value_counts()\n",
    "\n",
    "subset_category=['ë“±ë¡','íœ´ë³µí•™','ìˆ˜ê°•ì‹ ì²­','ì¥í•™ê¸ˆ','ìƒí™œê´€']\n",
    "\n",
    "df=df[df.category.isin(subset_category)]\n",
    "# í•™ìŠµë°ì´í„° ì›ë¬¸ í™•ì¸\n",
    "print (df.head())\n",
    "print()\n",
    "print (df.tail())\n",
    "print()\n",
    "\n",
    "def mecabsplit(mecab_tagger,inputs, pos):\n",
    "    r=[]\n",
    "    inputs = mecab_tagger.parse(inputs)\n",
    "    t = inputs.split('\\n')[:-2]\n",
    "    for i in t:\n",
    "        field = i.split('\\t')\n",
    "        if field[1].split(',')[-1].strip() is not '*':\n",
    "            r.extend( [ (x.split('/')[0],x.split('/')[1]) for x in field[1].split(',')[-1].split('+') ] )\n",
    "        else:\n",
    "            r.append( (field[0],field[1].split(',')[0]) )\n",
    "    if pos:\n",
    "        return r\n",
    "    else:\n",
    "        return [ x[0] for x in r ]\n",
    "    return r\n",
    "\n",
    "content_vocab = {'unk':0}\n",
    "intent_vocab={}\n",
    "intent_list=[]\n",
    "\n",
    "data_intent=''\n",
    "intent_idx=0\n",
    "vocab_idx=1\n",
    "\n",
    "for line in df.category.unique():\n",
    "    intent=line\n",
    "    if intent not in intent_vocab:\n",
    "        intent_vocab[intent]=intent_idx\n",
    "        intent_list.append(intent)\n",
    "        intent_idx +=1\n",
    "\n",
    "for line in list(df.question):\n",
    "    line = mecabsplit(tagger,line,False)\n",
    "    for it in line:\n",
    "        if it not in content_vocab:\n",
    "            content_vocab[it] = vocab_idx # it : ë‹¨ì–´\n",
    "            vocab_idx +=1\n",
    "            \n",
    "print(\"vocab size:\", len(content_vocab))\n",
    "print(content_vocab)  # ë‹¨ì–´ ì‚¬ì „\n",
    "print(intent_vocab)   # intent ì‚¬ì „\n",
    "print(\"intent size:\", len(intent_list))\n",
    "print(intent_list)    # intent_list : FAQ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "@ ëª¨ë¸ë§\n",
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_num, class_num, ):\n",
    "        super(CNN_Text, self).__init__()\n",
    "\n",
    "        #--- <ì—°ìŠµ2> ì•„ë˜ ë¹ˆ ì½”ë“œë¥¼ ì±„ìš°ì„¸ìš”\n",
    "        V = embed_num\n",
    "        D = 100            # ë‹¨ì–´ì„ë² ë”© ì°¨ì› \n",
    "        C = class_num   # ë¶„ë¥˜í•  ì¹´í…Œê³ ë¦¬ ê°¯ìˆ˜\n",
    "        Ci = 1          # ì…ë ¥ ì±„ë„ì˜ ìˆ˜\n",
    "        Co = 20           # ì¶œë ¥ ì±„ë„ì˜ ìˆ˜\n",
    "        Ks = [1,2,3]           # ì»¤ë„(í•„í„°) ì‚¬ì´ì¦ˆ ì •ì˜\n",
    "        #--- end\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)  # ë‹¨ì–´ ë²¡í„°ë¥¼ ì„ì˜ì˜ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)  # dropout ì •ì˜\n",
    "        \n",
    "        #--- <ì—°ìŠµ3> Fully-connected layerë¥¼ ì •ì˜í•˜ì„¸ìš”\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)                     \n",
    "        #--- end\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : í•™ìŠµë°ì´í„° íŒŒì¼ì˜ í•œ ë¬¸ì¥\n",
    "        # ì…ë ¥ë¬¸ì¥ì´ [ëŒ€í•™, ì›, ìˆ˜ë£Œ, ìƒ, ë…¼ë¬¸, ì œì¶œ] => \n",
    "        # x: [1, 2, 3, 4, 8, 9]ì˜ ì…ë ¥ì´ ìƒì„±ë¨ (content_vocab ì¶œë ¥ê²°ê³¼ ì°¸ê³ )\n",
    "        \n",
    "        # (N, W) Integer => (W) => view(1, -1) => (1, W)\n",
    "        x = self.embed(x)         # (N, W, D) (1ë¬¸ì¥, ìµœëŒ€ ì…ë ¥ ë‹¨ì–´ ê°œìˆ˜, Dì°¨ì›)\n",
    "        # ë¬¸ì¥ xì˜ ë‹¨ì–´ ë²¡í„°ê°’ ê°€ì ¸ì˜´\n",
    "        #print (x.size())\n",
    "\n",
    "        #--- <ì—°ìŠµ4> unsqueeze í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”\n",
    "        x = x.unsqueeze(1)      # (N, Ci, W, D) -> Cië¥¼ ì¶”ê°€í•˜ì—¬ unsqueeze (TextëŠ” ì±„ë„ 1ê°œ, ì´ë¯¸ì§€ëŠ” ì±„ë„ 3ê°œ)\n",
    "        #--- end\n",
    "        \n",
    "        #print (x.size())\n",
    "        \n",
    "        # ì»¨ë³¼ë£¨ì…˜ ë¶€ë¶„ : ì…ë ¥ë¬¸ ë³„ë¡œ í˜¸ì¶œë˜ì–´ ì»¤ë„ì„ ì ìš©í•˜ë©´ì„œ ì»¨ë³¼ë£¨ì…˜ ìˆ˜í–‰\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "        \n",
    "        #print(len(x))\n",
    "        #print(x[0].size())\n",
    "        #print(x[1].size())\n",
    "        #print(x[2].size())\n",
    "        \n",
    "        # max pooling ë¶€ë¶„. max poolingì„ ìˆ˜í–‰í•œ ê²°ê³¼ ë²¡í„°ì˜ ì°¨ì›ì€? (=ì¶œë ¥ì±„ë„ì˜ ê°¯ìˆ˜)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "                \n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "        \n",
    "        # ê²°ê³¼ê°’(logit)ì€ ì‹¤ìˆ˜ê°’ 2ê°œë¥¼ ì¶œë ¥í•˜ë©° ì´ ì¤‘ í°ê°’ì„ ê°€ì§„ ìª½ì´ ì˜ˆì¸¡ê°’\n",
    "        logit = self.fc1(x)  # (N, C) \n",
    "\n",
    "        #print(logit)\n",
    "        return logit\n",
    "\n",
    "    \n",
    "cnn = CNN_Text(vocab_idx,intent_idx)\n",
    "print(vocab_idx, intent_idx) # íŒŒì¼ì— ë‚˜íƒ€ë‚œ ë‹¨ì–´ìˆ˜, ì˜ë„ ê°¯ìˆ˜(í´ë˜ìŠ¤)\n",
    "optimizer = torch.optim.Adam(cnn.parameters())\n",
    "cnn.train()    # train modeë¡œ ì¤€ë¹„\n",
    "\n",
    "@ Training\n",
    "epoch = 10\n",
    "for e in range(epoch):\n",
    "    totalloss = 0\n",
    "    \n",
    "    for line in df.values:\n",
    "        target=Variable(torch.LongTensor([intent_vocab[line[1]]])) # ë”•ì…”ë„ˆë¦¬ íƒ€ì…\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cont = []\n",
    "        # ì…ë ¥ ë¬¸ì¥ì„ í˜•íƒœì†Œ ë‹¨ìœ„ì˜ í† í°ìœ¼ë¡œ ë¶„ë¦¬\n",
    "        line = mecabsplit(tagger,line[2],False)\n",
    "        \n",
    "        for it in line:\n",
    "            # í˜•íƒœì†Œ í† í°ì„ content_vocabì„ ê¸°ì¤€ìœ¼ë¡œ id ê°’ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì…ë ¥ ìƒì„±\n",
    "            cont.append(content_vocab[it])\n",
    "        \n",
    "        # list ë³€ìˆ˜ë¥¼ torchì—ì„œ ì‚¬ìš©ê°€ëŠ¥í•œ data_typeìœ¼ë¡œ ë³€í™˜\n",
    "        # view í•¨ìˆ˜ëŠ” ì…ë ¥ í…ì„œë¥¼ reshapeí•˜ëŠ” í•¨ìˆ˜\n",
    "        # [ì…ë ¥ í† í° ê°œìˆ˜] => [1, ì…ë ¥ í† í° ê°œìˆ˜]\n",
    "        # \n",
    "        cont = Variable(torch.LongTensor(cont)).view(1,-1)\n",
    "        pred = cnn(cont)\n",
    "        \n",
    "        #--- <ì—°ìŠµ5> ì •ë‹µê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•˜ëŠ” ì†ì‹¤í•¨ìˆ˜ cross_entropyë¥¼ í˜¸ì¶œí•˜ì„¸ìš”\n",
    "        loss = F.cross_entropy(pred,target)\n",
    "        #--- end\n",
    "\n",
    "        totalloss += float(loss.data)\n",
    "        loss.backward()  # í•„í„°, ì„ë² ë”©ë²¡í„° ê°’, FC ë ˆì´ì–´ì˜ matrix ë“± ëª¨ë“  ê°’ì„ lossë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ê³„ì‚°\n",
    "\n",
    "        \n",
    "        #--- <ì—°ìŠµ6> íŒŒë¼ë¯¸í„°ê°’ì„ ë³€ê²½í•˜ëŠ” í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”\n",
    "        # write code here\n",
    "        optimizer.step()\n",
    "        #--- end\n",
    "    \n",
    "    print(e,'epoch')    \n",
    "    print('loss : {:.3f}'.format(totalloss))\n",
    "    #print (totalloss)\n",
    "\n",
    "    \n",
    "@ Response\n",
    "response = df.answer\n",
    "with open('/content/gdrive/My Drive/aivle/data/college/college_FAQ_answer.txt', 'r', encoding='utf-8') as response_file:\n",
    "    response = [row.split('\\t')[1] for row in response_file.readlines()[1:]]\n",
    "    \n",
    "@ Test\n",
    "test=[]\n",
    "# intent_listëŠ” FAQ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸:\n",
    "# ['ë“±ë¡', 'íœ´ë³µí•™', 'ìˆ˜ê°•ì‹ ì²­', 'ì¥í•™ê¸ˆ', 'ìƒí™œê´€']\n",
    "for i in intent_list:\n",
    "    subset_df=df.query('category==\"{}\"'.format(i))\n",
    "    test.append(subset_df.question.values[0])\n",
    "    print(subset_df.question.values[0])\n",
    "    \n",
    "test\n",
    "\n",
    "# your test : input questions\n",
    "#test=['2í•™ê¸° ìˆ˜ê°•ì‹ ì²­ ê¸°ê°„ ì–¸ì œì¸ê°€ìš”?']\n",
    "#test=['ì¥í•™ê¸ˆ ë°›ìœ¼ë ¤ë©´ ëª‡ í•™ì  ì´ìƒì´ì–´ì•¼ ë˜ë‚˜ìš”?']\n",
    "test = ['ì¬ìˆ˜ê°• ë“¤ì–´ê°€ë ¤ë©´ ì–´ë–»ê²Œ?']\n",
    "\n",
    "for idx in list(test):\n",
    "    line = mecabsplit(tagger,idx, False)\n",
    "   \n",
    "    cont = []\n",
    "    for it in line:\n",
    "        if it in content_vocab:\n",
    "            cont.append(content_vocab[it])  # contì—ëŠ” ì…ë ¥ë¬¸ì— ë‚˜íƒ€ë‚œ ë‹¨ì–´ë“¤ì˜ index ì €ì¥ë¨\n",
    "        else:\n",
    "            cont.append(content_vocab['unk'])\n",
    "    \n",
    "    cont = Variable(torch.LongTensor(cont)).view(1,-1)\n",
    "    pred = cnn(cont)\n",
    "    #pred : (1, C)\n",
    "    v,i = torch.max(pred,1) # i: ê°€ì¥ í° ê°’ì´ ë“¤ì–´ìˆëŠ” ìœ„ì¹˜ ì¸ë±ìŠ¤\n",
    "    \n",
    "    print('input : ', line)\n",
    "    print('intent : ', intent_list[int(i)])\n",
    "    #print(int(i))\n",
    "    print('answer : ', response[int(i)],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ccbc08-5522-403b-a0a8-bad097d4c070",
   "metadata": {},
   "source": [
    "### bert_for_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29209bd-c90b-4e8c-9f71-c2861a3e24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "@ Import modules\n",
    "import pickle as pc\n",
    "import os\n",
    "import numpy as np\n",
    "# csv ëª¨ë“ˆì€ CSV í˜•ì‹(ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‘œ í˜•ì‹) ë°ì´í„°ë¥¼ ì½ê³  ì“°ëŠ” í´ë˜ìŠ¤ë¥¼ ì œê³µ\n",
    "import csv\n",
    "import torch\n",
    "# torch ë²„ì „ í™•ì¸\n",
    "print(\"Pytorch Version: \", torch.__version__)\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥í•œì§€ ì—¬ë¶€ í™•ì¸\n",
    "if torch.cuda.is_available():\n",
    "    # PyTorch ì—ê²Œ GPU ì‚¬ìš©í• ê±°ë¼ê³  ì•Œë ¤ì£¼ê¸°\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "@ Installing the Hugging Face Library\n",
    "# transformers íŒ©í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install transformers\n",
    "\n",
    "@ Configure the experiments\n",
    "train_filename = '/content/gdrive/My Drive/aivle/data/amazon/bert_train_data_all.csv'\n",
    "test_data = '/content/gdrive/My Drive/aivle/data/amazon/bert_balanced_data'\n",
    "test_label = '/content/gdrive/My Drive/aivle/data/amazon/bert_balanced_label'\n",
    "\n",
    "@ Load Amazon Review Dataset\n",
    "def load_data(filename):\n",
    "    data = list()\n",
    "    label = list()\n",
    "    \n",
    "    f = open(filename, 'r', encoding='utf-8')\n",
    "    reader = csv.reader(f)\n",
    "    for idx, line in enumerate(reader):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        # line[2]ì— label 1(ê¸ì •), 0(ë¶€ì •) line[5]ì— review textê°€ ìˆìŒ \n",
    "        data.append(line[5])\n",
    "        label.append(int(line[2]))\n",
    "\n",
    "    f.close() \n",
    "   \n",
    "    # dataì™€ label ì‚¬ì´ì¦ˆ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸\n",
    "    assert len(data) == len(label)  \n",
    "    return data, label\n",
    "\n",
    "# train_data : ë¦¬ë·°ë¬¸ì¥ (text), train_lable : 1(ê¸ì •) ë˜ëŠ” 0(ë¶€ì •)\n",
    "train_data, train_label = load_data(train_filename)\n",
    "\n",
    "print(\"Size of train data: {}\".format(len(train_data)))\n",
    "print(\"Size of train label: {}\".format(len(train_label)))\n",
    "\n",
    "@ Tokenization & Input Formatting\n",
    "## BERT Tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# BERT tokenizer ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# do_lower_case : Trueì´ë©´ ëª¨ë‘ ì†Œë¬¸ìë¡œ ë³€í™˜, Falseì´ë©´ ëŒ€ì†Œë¬¸ì êµ¬ë¶„\n",
    "print(\"Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# í•˜ë‚˜ì˜ sentenceì— ëŒ€í•´ BertTokenizer ì ìš©\n",
    "# Print the original sentence.\n",
    "print(\"Original: \", train_data[0])\n",
    "print(\"Original: \", train_data[1])\n",
    "print()\n",
    "# Print the sentence split into tokens.\n",
    "print(\"Tokenized: \", tokenizer.tokenize(train_data[0]))\n",
    "print(\"Tokenized: \", tokenizer.tokenize(train_data[1]))\n",
    "print()\n",
    "# Print the sentence mapped to token ids.\n",
    "print(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_data[0])))\n",
    "print(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_data[1])))\n",
    "print()\n",
    "\n",
    "## Required Formatting\n",
    "ê° ë¬¸ì¥ì˜ ì²˜ìŒê³¼ ëì— special token ë”í•˜ê¸°\n",
    "ê° ë¬¸ì¥ì„ maximum length ë§Œí¼ ìë¥´ê³  padding token ì±„ì›Œì£¼ê¸°\n",
    "ê° ë¬¸ì¥ì—ì„œ padding token ê³¼ ì‹¤ì œ token ë“¤ êµ¬ë¶„í•˜ê¸° ìœ„í•œ attention masking ì ìš©\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence\n",
    "for sent in train_data:\n",
    "    # 'encode' will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the '[CLS]' token to the start.\n",
    "    #   (3) Append the '[SEP]' token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   max_length : ë¬¸ì¥ì˜ ìµœëŒ€ê¸¸ì´\n",
    "    #   encoded_sent : token IDs\n",
    "    #---------------------------------------------------------------\n",
    "    #      ì—°ìŠµ (1)     tokenizerì˜ encode í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ ì£¼ì„¸ìš”.  \n",
    "    #---------------------------------------------------------------                \n",
    "  \n",
    "    encoded_sent = tokenizer.encode(sent, \n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length = 64)\n",
    "    \n",
    "    # Add the encoded sentence to the list\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Print train data[0]\n",
    "print(\"Original: \", train_data[0])\n",
    "print()\n",
    "print(\"Token IDs: \", input_ids[0])\n",
    "\n",
    "# Print special tokens and tokenized sentence\n",
    "print(\"\\n[CLS] token: {:}, ID: {:}\".format(tokenizer.cls_token, tokenizer.cls_token_id))\n",
    "print(\"\\n[PAD] token: {:}, ID: {:}\".format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "print(\"\\n[SEP] token: {:}, ID: {:}\".format(tokenizer.sep_token, tokenizer.sep_token_id))\n",
    "print(\"\\nTokenized: \", tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
    "\n",
    "# Padding & Truncating\n",
    "print(\"Max length: \", max([len(each) for each in input_ids]))\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "MAXLEN = 64\n",
    "# post-sequence truncation, post-sequence padding\n",
    "# padding value 0, type of output sequences long\n",
    "input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, \n",
    "                                                          maxlen=MAXLEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print(\"\\nPadding is done.\")\n",
    "\n",
    "# Attention Masks\n",
    "# Create attention masks\n",
    "attention_masks = [] # ê° ë¬¸ì¥ì— ëŒ€í•œ attention mask ë¦¬ìŠ¤íŠ¸ë¥¼ ì €ì¥\n",
    "\n",
    "# í† í° ì‹œí€€ìŠ¤ì—ì„œ íŒ¨ë”©ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì€ 0, íŒ¨ë”©ì´ ì•„ë‹Œ ë¶€ë¶„ì€ 1ì„ ë„£ì€ maskë¥¼ ìƒì„±\n",
    "# íŒ¨ë”© ë¶€ë¶„ì€ ëª¨ë¸ ë‚´ì—ì„œ Attentionì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•„ í•™ìŠµì†ë„ë¥¼ í–¥ìƒ\n",
    "# For every sentence\n",
    "for sent in input_ids:\n",
    "    # Create the attention mask.\n",
    "    #  - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #  - If a token ID is not 0 ( > 0), then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)\n",
    "print(\"\\nAttention masking is done.\")\n",
    "\n",
    "# Training & Validation Split\n",
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_test_split(arrays, test_size, train_size, random_state, shuffle, stratify)\n",
    "#     arrays : ë¶„í• ì‹œí‚¬ ë°ì´í„°\n",
    "#     test_size : í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì˜ ë¹„ìœ¨ (default = 0.25)\n",
    "#     random_state : ë°ì´í„° ì…”í”Œ ì‹œ seed value. í˜¸ì¶œí•  ë•Œë§ˆë‹¤ ë™ì¼í•œ í•™ìŠµ ë°ì´í„°, í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì„¤ì •\n",
    "#     shuffle : ì…”í”Œ ì—¬ë¶€ (default = True)\n",
    "#     stratify : ì§€ì •í•œ Dataì˜ ë¹„ìœ¨ì„ ìœ ì§€í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Label Setì¸ Yê°€ 25%ì˜ 0ê³¼ 75%ì˜ 1ë¡œ ì´ë£¨ì–´ì§„ Binary Setì¼ ë•Œ\n",
    "#     stratify=Yë¡œ ì„¤ì •í•˜ë©´ ë‚˜ëˆ„ì–´ì§„ ë°ì´í„°ì…‹ë“¤ë„ 0ê³¼ 1ì„ ê°ê° 25%, 75%ë¡œ ìœ ì§€\n",
    "# ë°˜í™˜ê°’ : (í•™ìŠµ ë°ì´í„°, í…ŒìŠ¤íŠ¸ ë°ì´í„°, í•™ìŠµë°ì´í„° label, í…ŒìŠ¤íŠ¸ë°ì´í„° label)\n",
    "# test_size=0.1ë¡œ ì„¤ì •, Use 90% for training and 10% for validation\n",
    "train_inputs, valid_inputs, train_labels, valid_labels = train_test_split(input_ids, train_label, random_state=2018, test_size=0.1)\n",
    "# Do the same for the masks.\n",
    "train_masks, valid_masks, _, _ = train_test_split(attention_masks, train_label, random_state=2018, test_size=0.1)\n",
    "# print train_inputs, valid_inputs\n",
    "# ì²«ë²ˆì§¸ í•™ìŠµë°ì´í„°, ì²«ë²ˆì§¸ attention maskë¥¼ ì¶œë ¥í•´ì„œ í™•ì¸\n",
    "print(train_inputs[:1])\n",
    "print(train_masks[:1])\n",
    "\n",
    "# Converting to PyTorch Data Types\n",
    "# Convert all inputs and labels into torch tensors, the required data type for our model.\n",
    "# torch.tensor(data) -> tensorë¥¼ ìƒì„±\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "valid_inputs = torch.tensor(valid_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "valid_labels = torch.tensor(valid_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "valid_masks = torch.tensor(valid_masks)\n",
    "\n",
    "# PyTorch ì˜ DataLoader class ë¥¼ ì´ìš©í•˜ì—¬ amazon review dataset ì— ëŒ€í•œ iterator ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# torch.utils.data : íŒŒì´í† ì¹˜ì˜ ë°ì´í„° ë¡œë”© ìœ í‹¸ë¦¬í‹°. ë°ì´í„°ì…‹, ë°ì´í„°ë¡œë”, ìƒ˜í”ŒëŸ¬ ë“±ì„ ì œê³µí•¨\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# The DataLoader needs to know our batch size for training, so we specify it here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "#---------------------------------------------------------------\n",
    "#      ì—°ìŠµ (2) train_data, valid_dataì˜ ë°ì´í„° ë¡œë”ë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.\n",
    "#---------------------------------------------------------------   \n",
    "# Create the DataLoader for our training set.\n",
    "# torch.utils.data.TensorDataset(*tensors)\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "# Create the DataLoader for our validation set.\n",
    "valid_data = TensorDataset(valid_inputs, valid_masks, valid_labels)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)\n",
    "\n",
    "@ Train our classification model\n",
    "## BertForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassfication, the pretrained BERT model \n",
    "# with a single linear classification layer on top.\n",
    "# BERT ëª¨ë¸ì˜ ë„¤íŠ¸ì›Œí¬ í˜•íƒœë¥¼ ì¶œë ¥\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab\n",
    "                                                      num_labels = 2, # The number of output labels 2 for binary classification\n",
    "                                                                      # You can increase this for multi-class tasks\n",
    "                                                      output_attentions = False, # whether the model returns attentions weight (correponding to multi-head self attentions)\n",
    "                                                      output_hidden_states = False) # whether the model returns all hidden states\n",
    "\n",
    "# Tell PyTorch to run this model on the GPU\n",
    "# (modelì˜ ëª¨ë“  parameterë¥¼ GPUì— loading)\n",
    "model.cuda()\n",
    "\n",
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "print(\"The BERT model has {:} different named parameters.\\n\".format(len(params)))\n",
    "print(\"=== Embedding Layer ===\\n\")\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print(\"\\n==== First Transformer ====\\n\")\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print(\"\\n==== Output Layer====\\n\")\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "    \n",
    "## Optimizer & Learning Rate Scheduler\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
    "# I believe the 'W' stands for \"Weight Decay fix\"\n",
    "# Weight Decay: weightë“¤ì˜ ê°’ì´ ì¦ê°€í•˜ëŠ” ê²ƒì„ ì œí•œí•¨ìœ¼ë¡œì¨ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ê°ì†Œì‹œì¼œ ì˜¤ë²„í”¼íŒ…ì„ ë°©ì§€í•˜ëŠ” ê¸°ë²• \n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "# Number of training epochs (we recommend between 2 and 4)\n",
    "epochs = 4\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "print(\"number of batches:\", len(train_dataloader))\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "print(\"total_steps:\", total_steps)\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "## Training Loop\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "# numpy argmax : í•´ë‹¹ ì°¨ì›(axis)ì˜ ê°’ ì¤‘ì—ì„œ ê°€ì¥ í° ê°’ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜\n",
    "# flatten() : ë‹¤ì°¨ì› ë°°ì—´ì„ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜\n",
    "# sum() : ë°°ì—´ ë‚´ ì „ì²´ ê°’ë“¤ì˜ í•©\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "import time\n",
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Take a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "import random\n",
    "# Set the seed value all over the place to make this reproducible\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "seed_val = 42\n",
    "set_seed(seed_val)\n",
    "# Store the average loss after each epoch so we can plot them\n",
    "loss_values = []\n",
    "# For each epoch\n",
    "for epoch in range(0, epochs):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0.\n",
    "    # Put the model into training mode.\n",
    "    # Don't be mislead -- the call to 'train' just changes the \"mode\", it doesn't \"perform\" the training.\n",
    "    # 'dropout' and 'bachnorm' layers behave differently during training vs test\n",
    "    model.train()\n",
    "    # For each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress\n",
    "            print(\"Batch {:>5,} of {:>5,}. Elapsed: {:}.\".format(step, len(train_dataloader), elapsed))\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        #---------------------------------------------------------------\n",
    "        #      ì—°ìŠµ (3) ì•„ë˜ ë¼ì¸ì— ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ ì£¼ì„¸ìš”.\n",
    "        #--------------------------------------------------------------- \n",
    "        model.zero_grad()\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value from the tensor.\n",
    "        total_loss += loss.item()\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.        \n",
    "        #---------------------------------------------------------------\n",
    "        #      ì—°ìŠµ (4) ì•„ë˜ ë¼ì¸ì— íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ ì£¼ì„¸ìš”.\n",
    "        #--------------------------------------------------------------- \n",
    "        optimizer.step() # ìµœì í™”, ì—…ë°ì´íŠ¸ \n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on our validation set.\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode -- the dropout layers behave differently during evaluation\n",
    "    model.eval()\n",
    "    # Tracking variables\n",
    "    eval_loss, eval_acc = 0., 0.\n",
    "    # Evaluate data for one epoch\n",
    "    for valid_step, batch in enumerate(valid_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            #---------------------------------------------------------------\n",
    "            #      ì—°ìŠµ (5) ì•„ë˜ í•¨ìˆ˜ì—ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ì±„ì›Œì£¼ì„¸ìš”\n",
    "            #--------------------------------------------------------------- \n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        # Get the \"logits\" output by the model.\n",
    "        # The \"logits\" are the output values prior to applying an activation function like the softmax\n",
    "        # output íƒ€ì…: class transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # (https://huggingface.co/docs/transformers/v4.22.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)\n",
    "        # our model will return (outputs.loss(optional)=None, outputs.logits)\n",
    "        logits = outputs[0]\n",
    "        # Move logits and labels to CPU (Moduleì„ í†µí•´ ë‚˜ì˜¨ tensorì„ í›„ì²˜ë¦¬ì— ì‚¬ìš©í•˜ê±°ë‚˜, ê³„ì‚°ëœ lossë¥¼ ë¡œê¹… ë“±)\n",
    "        # detach() : íŒŒì´í† ì¹˜ëŠ” tensorì—ì„œ ì´ë£¨ì–´ì§„ ëª¨ë“  ì—°ì‚°ì„ ì¶”ì í•´ì„œ graphì— ê¸°ë¡í•´ë‘ëŠ”ë° ì´ ì—°ì‚° ê¸°ë¡ìœ¼ë¡œë¶€í„° \n",
    "        # ê·¸ë˜ë””ì–¸íŠ¸ê°€ ê³„ì‚°ë˜ê³  ì—­ì „íŒŒê°€ ì´ë£¨ì–´ì§€ê²Œ ëœë‹¤. detach()ëŠ” ì´ ì—°ì‚° ê¸°ë¡ìœ¼ë¡œë¶€í„° ë¶„ë¦¬í•œ tensorì„ ë°˜í™˜í•˜ëŠ” method     \n",
    "        # cpu() : GPU ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ì ¸ ìˆëŠ” tensorë¥¼ cpu ë©”ëª¨ë¦¬ë¡œ ë³µì‚¬í•˜ëŠ” method   \n",
    "        # numpy() : tensorë¥¼ numpyë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_acc = flat_accuracy(logits, label_ids)\n",
    "        # Accumulate the total accuracy.\n",
    "        eval_acc += tmp_eval_acc        \n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"Accuracy: {0:.2f}\".format(eval_acc / (valid_step + 1)))\n",
    "    print(\"Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "print(\"\")\n",
    "print(\"Training complete!\")        \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o')\n",
    "# Label the plot.\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "@ Performance on Test set\n",
    "## Load Amazon Review Test Dataset\n",
    "# pickle ëª¨ë“ˆë¡œ í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ load : bert_balanced_data, bert_balanced_label\n",
    "with open(test_data, 'rb') as f:\n",
    "    test_data = pc.load(f)\n",
    "with open(test_label, 'rb') as f:\n",
    "    test_label = pc.load(f)\n",
    "print(\"Size of test data: {}\".format(len(test_data)))\n",
    "print(\"Size of test label: {}\".format(len(test_label)))\n",
    "\n",
    "## Tokenization & Input Formatting\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "# For every sentence...\n",
    "for sent in test_data:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    input_ids.append(encoded_sent)\n",
    "# Pad our input tokens\n",
    "input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=MAXLEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks array\n",
    "attention_masks = []\n",
    "# For every sentence\n",
    "for sent in input_ids:\n",
    "    # Create the attention mask.\n",
    "    #  - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #  - If a token ID is not 0 ( > 0), then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)\n",
    "# Convert to tensors\n",
    "# input_ids : í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì— í¬í•¨ëœ ë‹¨ì–´ ids\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(test_label)\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "prediction_sampler = SequentialSampler(prediction_data) \n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "## Evaluate on Test Set\n",
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "# Tracking variables \n",
    "predictions, true_labels = [], []\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader (batchì—ì„œ ë°ì´í„° ì¶”ì¶œ)\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "  logits = outputs[0]\n",
    "  # Move logits and labels to CPU\n",
    "  # logist, labels í…ì„œë¥¼ CPUë¡œ ì´ë™í•˜ì—¬ ì •í™•ë„ ê³„ì‚°\n",
    "  # detach() : í…ì„œì—ì„œ ì´ë£¨ì–´ì§„ ì—°ì‚° ê¸°ë¡ìœ¼ë¡œë¶€í„° ë¶„ë¦¬í•œ í…ì„œë¥¼ ë°˜í™˜\n",
    "  # cpu() : GPU ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ì ¸ ìˆëŠ” tensorë¥¼ cpu ë©”ëª¨ë¦¬ë¡œ ë³µì‚¬\n",
    "  # numpy() : tensorë¥¼ numpyë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  # Store predictions and true labels\n",
    "  # predictions : ì˜ˆì¸¡ê°’ array\n",
    "  # true_labels : ì •ë‹µê°’ array\n",
    "  # logits arrayì˜ ê°’ë“¤ ì¤‘ ê°€ì¥ í° ê°’ì˜ indexë¥¼ ë°˜í™˜\n",
    "  # when axis = 1, argmax identifies the maximum value for every row. \n",
    "  predictions.extend(np.argmax(logits, axis=1).flatten()) # [0, 1, 1, 0, ...]\n",
    "  true_labels.extend(label_ids.flatten()) # [0, 1, 0, 0, ...]\n",
    "print('DONE.')\n",
    "\n",
    "# accuracy, precision, recall, f1 score ì„±ëŠ¥ í™•ì¸\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['negative', 'positive']\n",
    "# sklearn.metrics.classification_report(y_true, y_pred, digits, target_names)\n",
    "# support is the number of actual occurrences of the class in the specified dataset.\n",
    "print(classification_report(true_labels, predictions, digits=4, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51e2c4-20c1-4aed-a3ef-171619336eff",
   "metadata": {},
   "source": [
    "## ìŠ¤íŒ¸ë¶„ë¥˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192d122-43ec-41a1-8c27-b73b08fe1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4ae33-39ee-4707-abd6-958d2a8742f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import rich  # ì¶œë ¥ì„ ì˜ˆì˜ê²Œ ê¾¸ë©°ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "from rich.table import Table\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "train_df = pd.read_csv('spam.csv')\n",
    "test_df = pd.read_csv('spam_test_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4b157-217c-4551-ae30-98a6ff024c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "train_df.isna().sum()\n",
    "train_df.reset_index(inplace=True)\n",
    "train_df.drop(columns={'index'},inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce781e3c-8ff8-4a2e-8966-53055c4f62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "train_df['label'] = train_df['label'].replace(['ham','spam'],[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5fa70-fd5d-44b1-868b-0e623f1c5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipe(model, model_name: str) -> Pipeline:\n",
    "    \"TfidfVectorizerì™€ ëª¨ë¸ì„ ì—°ê²°í•œ íŒŒì´í”„ë¼ì¸ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\"\n",
    "    tfidf = TfidfVectorizer(analyzer=\"char\", ngram_range=(1, 3))\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", tfidf),\n",
    "        (model_name, model)\n",
    "    ])\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63095e3-c415-48ac-bdf8-2eb68c824e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_kfold_accuarcy(model, k: int = 5) -> float:\n",
    "    \"ëª¨ë¸ì„ ì…ë ¥ë°›ì•„ KFold ì˜ˆì¸¡ í›„ accuracy scoreë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\"\n",
    "    kfold = StratifiedKFold(k, shuffle=True, random_state=42)\n",
    "    result = []\n",
    "    for train_idx, test_idx in kfold.split(train_df[\"text\"], train_df[\"label\"]):\n",
    "        train, val = train_df.iloc[train_idx], train_df.iloc[test_idx]\n",
    "        model.fit(train[\"text\"], train[\"label\"])\n",
    "        pred = model.predict(val[\"text\"])\n",
    "        acc = accuracy_score(val[\"label\"], pred)\n",
    "        result.append(acc)\n",
    "\n",
    "    return np.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371355b3-4854-4681-b994-205b15d5e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"naive_bayes\", BernoulliNB()),\n",
    "    (\"SGD\", SGDClassifier(random_state=42, n_jobs=-1)),\n",
    "]\n",
    "\n",
    "model_pipes = [(name, get_pipe(model, name)) for name, model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783a499-fa01-49c5-8a24-3d5b414fe601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [\n",
    "#     (\"naive_bayes\", BernoulliNB()),\n",
    "#     (\"SGD\", SGDClassifier(random_state=42, n_jobs=-1)),\n",
    "#     (\"rfc\", RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
    "#     (\"SVC\", SVC(random_state=42)),\n",
    "#     (\"ada\", AdaBoostClassifier(random_state=42)),\n",
    "#     (\"lgbm\", LGBMClassifier(random_state=42)),\n",
    "#     (\"lgbm2\", LGBMClassifier(n_estimators=80, random_state=42)),\n",
    "#     (\"xgb\", XGBClassifier(random_state=42)),\n",
    "#     (\"knc1\", KNeighborsClassifier()),\n",
    "#     (\"knc2\", KNeighborsClassifier(n_neighbors=4))\n",
    "# ]\n",
    "\n",
    "# model_pipes = [(name, get_pipe(model, name)) for name, model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddcf9e5-8bbf-4966-9d83-06759b9a88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Table(title=\"Model Comparison Table\")\n",
    "table.add_column(\"Model Name\", justify=\"left\", style=\"green\")\n",
    "table.add_column(\"Accuracy\", justify=\"right\")\n",
    "\n",
    "for model_name, model in tqdm(model_pipes, leave=False):\n",
    "    acc = return_kfold_accuarcy(model)\n",
    "    table.add_row(model_name, f\"{acc:0.3f}\")\n",
    "\n",
    "rich.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4c0fd-b292-4607-b5e1-c7f8a8677c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stack_models = [(name, get_pipe(model, name)) for name, model in models]\n",
    "\n",
    "stacking = StackingClassifier(stack_models)\n",
    "acc = return_kfold_accuarcy(stacking)\n",
    "rich.print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d306dac-36d7-41dc-a10e-e502320ba1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking.fit(train_df['text'], train_df['label'])\n",
    "submission_pred = stacking.predict(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ff0a4-8013-4652-bb09-b141bf56837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('spam_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921e0ae-ce2c-4353-8a11-fdb8788a2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label'] = submission_pred\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093783f-5cc7-4f11-937d-6f257f8b1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label']= submission['label'].replace([0,1],['ham','spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3983453-4e65-40f8-a8be-dbad1c3e8853",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"/aihub/data/M1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae25809f-a793-4a30-82cd-62e193c676eb",
   "metadata": {},
   "source": [
    "# ìˆ˜ì–´ë²ˆì—­[CNN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09888fb4-dfc4-4eab-ac70-cf8c8fc44305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random as rd\n",
    "import cv2, os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.backend import clear_session\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd97a0-cbed-4875-bb6f-7a8638ee96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµê³¡ì„  í•¨ìˆ˜\n",
    "def dl_history_plot(history):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history['loss'], label='train_err')\n",
    "    plt.plot(history['val_loss'], label='val_err')\n",
    "\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d0eb0-616b-4bcd-8882-42b266c2820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460843a8-69eb-4210-8c5d-82171e6f31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'ë‚´ê²½ë¡œ'\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "data.head()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb7a7f-d974-421b-9ca9-ffbf4618ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class name ì°ì–´ë³´ê¸°\n",
    "import string\n",
    "class_names = list(string.ascii_lowercase)\n",
    "len(class_names), class_names\n",
    "\n",
    "\n",
    "# ë°ì´í„° ì‚´í´ë³´ê¸°\n",
    "# ì•„ë˜ ìˆ«ìë¥¼ ë°”ê¿”ê°€ë©° í™”ë©´ì— ê·¸ë ¤ ë´…ì‹œë‹¤.\n",
    "n = 10\n",
    "sign_fig = data.iloc[n, 1:].values\n",
    "sign_fig = sign_fig.reshape(28, 28)\n",
    "\n",
    "sign = class_names[data.iloc[n,0]]\n",
    "\n",
    "plt.title(sign)\n",
    "plt.imshow(255-sign_fig, cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b740ce-73d8-42f1-bb45-caaa2ff0109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬\n",
    "target = 'label'\n",
    "x = data.drop(target, axis = 1)\n",
    "y = data.loc[:, target]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, train_size = 20000, random_state = 2022)\n",
    "\n",
    "x_train.shape, x_val.shape\n",
    "\n",
    "# ëª¨ë‘ ë„˜íŒŒì´ë¡œ ë³€í™˜\n",
    "x_train2, x_val2, y_train2, y_val2 = x_train.values, x_val.values, y_train.values, y_val.values\n",
    "\n",
    "\n",
    "## CNNì„ ìœ„í•´ shape ë§ì¶”ê¸° n, 28,28,1\n",
    "x_train2 = x_train2.reshape(20000,28,28,1)\n",
    "x_val2 = x_val2.reshape(7455,28,28,1)\n",
    "\n",
    "x_train2.shape, x_val2.shape\n",
    "\n",
    "# min_max scaling\n",
    "x_train2 = x_train2 / 255.\n",
    "x_val2 = x_val2 / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87cd37a-6430-41c0-9c43-37aa6be47009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ë§(ìš”ì¦˜ ëœ¨ëŠ”ê±°-> CatBoost)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "m1 = RandomForestClassifier()\n",
    "m1.fit(x_train, y_train)\n",
    "p1 = m1.predict(x_val)\n",
    "\n",
    "cn = np.array(class_names)\n",
    "\n",
    "print(accuracy_score(y_val,p1))\n",
    "print('-'*60)\n",
    "print(confusion_matrix(y_val, p1))\n",
    "print('-'*60)\n",
    "print(classification_report(cn[y_val], cn[p1]))\n",
    "\n",
    "## CNN\n",
    "clear_session()\n",
    "\n",
    "m2 = Sequential([Conv2D(32, kernel_size=3, input_shape=(28, 28, 1), padding='same', strides =1, activation='relu'),\n",
    "                    MaxPooling2D(pool_size=2, strides=2),\n",
    "                    Flatten(),\n",
    "                    Dense(128, activation = 'relu'),\n",
    "                    Dense(25, activation='softmax')\n",
    "])\n",
    "\n",
    "m2.summary()\n",
    "\n",
    "m2.compile(optimizer = 'adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "history = m2.fit(x_train2, y_train2, epochs = 10, validation_split=0.2).history\n",
    "\n",
    "dl_history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a961e-9e8d-417c-bcc1-305a9bcbcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì¸¡ & ê²€ì¦\n",
    "p2 = m2.predict(x_val2)\n",
    "\n",
    "p2_1 = p2.argmax(axis=1)\n",
    "\n",
    "cn = np.array(class_names)\n",
    "\n",
    "print(accuracy_score(y_val,p2_1))\n",
    "print('-'*60)\n",
    "print(confusion_matrix(y_val, p2_1))\n",
    "print('-'*60)\n",
    "print(classification_report(cn[y_val], cn[p2_1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b7a36-6b13-4082-8aeb-de0e77c583d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥í•˜ê¸°(3ê°€ì§€ ë°©ì‹)keras, joblib, pickle\n",
    "import joblib\n",
    "joblib.dump(m1, 'sign_model_rf.pkl')\n",
    "m2.save('sign_model.h5')\n",
    "\n",
    "# ëª¨ë¸ ë¡œë”©\n",
    "m1_2 = joblib.load('sign_model_rf.pkl')\n",
    "from keras.models import load_model\n",
    "m2_1 = load_model('sign_model.h5')\n",
    "\n",
    "# ëª¨ë¸ ì‚¬ìš©\n",
    "p1_2 = m1_2.predict(x_val) # ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸\n",
    "print(accuracy_score(y_val,p1_2))\n",
    "print('-'*60)\n",
    "print(confusion_matrix(y_val, p1_2))\n",
    "print('-'*60)\n",
    "print(classification_report(cn[y_val], cn[p1_2]))\n",
    "\n",
    "p2_1 = m2_1.predict(x_val2) # CNNëª¨ë¸ \n",
    "p2_2 = p2_1.argmax(axis=1)\n",
    "print(accuracy_score(y_val,p2_2))\n",
    "print('-'*60)\n",
    "print(confusion_matrix(y_val, p2_2))\n",
    "print('-'*60)\n",
    "print(classification_report(cn[y_val], cn[p2_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce92c29-b8a8-4640-9c15-45bbbf15969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì´í”„ ë¼ì¸ (Data Pipeline êµ¬ì„±) í•¨ìˆ˜ë¡œ ë§Œë“¤ê¸°\n",
    "\n",
    "# íŒŒì´í”„ë¼ì¸ì—ì„œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬/í•¨ìˆ˜\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "\n",
    "def sign_pipeline(file) :\n",
    "\n",
    "    # class names ì¤€ë¹„\n",
    "    class_names = list(string.ascii_lowercase)\n",
    "    class_names = np.array(class_names)\n",
    "\n",
    "    # í‘ë°±ìœ¼ë¡œ ì½ê¸°\n",
    "    img = cv2.imread(file , cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # í¬ê¸° ì¡°ì •\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "\n",
    "    # input shape ë§ì¶”ê¸°\n",
    "    test_sign = img.reshape(1,28,28,1)\n",
    "\n",
    "    # ìŠ¤ì¼€ì¼ë§\n",
    "    test_sign = test_sign / 255.\n",
    "\n",
    "    # ëª¨ë¸ ë¡œë”©\n",
    "    model = load_model('sign_model.h5')\n",
    "\n",
    "    # ì˜ˆì¸¡\n",
    "    pred = model.predict(test_sign)\n",
    "    pred_1 = pred.argmax(axis=1)\n",
    "\n",
    "    return class_names[pred_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f800b-7a92-4486-873c-b20566a0f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/content/drive/MyDrive/dataset/test image/v.png'\n",
    "sign_pipeline(file) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
