{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"provenance":[{"file_id":"1ZX72JAO-IaAt5fPRKsUVof8JCHxgPaub","timestamp":1579183200965}],"collapsed_sections":[]},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["## 데이터 출처\n","\n","[Naver sentiment movie corpus]: https://github.com/e9t/nsmc/\n","\n","- RNN 모델의 학습을 위해 [Naver sentiment movie corpus] 데이터셋 중 10,000건을 추출하여 사용하였습니다."],"metadata":{"id":"Fo92nKutwxk0"}},{"cell_type":"code","source":["# torchtext.legacy를 사용할 수 있는 torchtext 버전 설치\n","!pip install -U torchtext==0.10.0"],"metadata":{"id":"8WSzxoQ4Nd79","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665007546908,"user_tz":-540,"elapsed":4684,"user":{"displayName":"김나리","userId":"01119342099285995005"}},"outputId":"6431b0a6-e725-44cc-ee3a-bc3278215fff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n","Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n"]}]},{"cell_type":"code","metadata":{"id":"CSQVQtQVkMpg","outputId":"bbe73e90-1a58-450f-ebb9-fbd2931b993b","executionInfo":{"status":"ok","timestamp":1665007553094,"user_tz":-540,"elapsed":2209,"user":{"displayName":"김나리","userId":"01119342099285995005"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["#colab 을 이용한 실행시\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"DDhMi_j7kI8u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665007556027,"user_tz":-540,"elapsed":532,"user":{"displayName":"김나리","userId":"01119342099285995005"}},"outputId":"50ed4920-acc8-47ec-a118-1efab66aaaff"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# torchtext.legacy : text의 preprocessing 파이프라인 정의\n","# 1) 토크나이징(Tokenization)\n","# 2) 단어장 생성(Build Vocabulary)\n","# 3) 토큰의 수치화(Numericalize all tokens)\n","# 4) 데이터 로더 생성(Create Data Loader)\n","from torchtext.legacy import data\n","import torchtext.datasets as datasets\n","\n","import pickle\n","print (torch.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.9.0+cu102\n"]}]},{"cell_type":"markdown","source":["#LSTM\n","Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n","\n","nn.LSTM(input_size, hidden_size, num_layers, bidirectional, batch_first):\n","* input_size – The number of expected features in the input x\n","* hidden_size – The number of features in the hidden state h\n","* num_layers – Number of recurrent layers. \n","* bidirectional – If True, becomes a bidirectional LSTM. Default: False\n","* batch_first – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Default: False. Note that this does not apply to hidden or cell states."],"metadata":{"id":"_XfGe-AqK1NF"}},{"cell_type":"code","metadata":{"id":"MHVoD2RfkI81","executionInfo":{"status":"ok","timestamp":1665010443029,"user_tz":-540,"elapsed":503,"user":{"displayName":"김나리","userId":"01119342099285995005"}}},"source":["class RNN_Text(nn.Module):    \n","    def __init__(self, embed_num, class_num):\n","        # super()로 Base Class의 __init__() 호출 (nn.Module 클래스 생성자 호출)\n","        # super(파생클래스, self).__init__() 파이썬 2.x 문법\n","        # super().__init__() 파이썬 3.x 문법 둘다 사용 가능\n","        super(RNN_Text, self).__init__()\n","          \n","        V = embed_num   # 단어 사전의 크기\n","        C = class_num   # 분류하고자 하는 클래스 개수        \n","        H = 256         # 히든 사이즈\n","        D = 100         # 단어벡터 차원 100        \n","        self.embed = nn.Embedding(V, D)        \n","        \n","        # LSTM Layer, bidirectional이므로 출력되는 벡터의 크기는 H * 2\n","        self.rnn = nn.LSTM(D, H, bidirectional = True)\n","                 \n","        # Linear Layer : (512, 2)\n","        self.out = nn.Linear(H*2, C)\n","        \n","    def forward(self, x):\n","        x = self.embed(x)     # (N, W, D) 문장 x의 단어 벡터값 가져옴\n","      \n","        # LSTM 모듈 실행\n","        # LSTM 입력데이터\n","        # input x : torch.Size([30, 100, 100]) [시퀀스 길이, 배치 사이즈, Dimension]\n","        x,(_,__) = self.rnn( x, ( self.h, self.c ) )  \n","\n","        # output x : torch.Size([30, 100, 512]) [시퀀스 길이, 배치 사이즈, 256 * 2]\n","\n","        # 최종 Hidden Layer로 Linear 모듈 실행   \n","        logit = self.out(x[-1])  \n","\n","        # 최종 예측 벡터 크기: [배치 사이즈, C], C: 클래스 개수\n","        return logit       # logit : torch.Size([100, 2])\n","\n","    def inithidden(self, b):\n","        #self.h = Variable(torch.randn(2, b, 256))\n","        #self.c = Variable(torch.randn(2, b, 256))    \n","        self.h = torch.randn(2, b, 256)   # [2, batch_size, 256]\n","        self.c = torch.randn(2, b, 256)   # [2, batch_size, 256]\n"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"fuVqEcvNkI83","executionInfo":{"status":"ok","timestamp":1665010446783,"user_tz":-540,"elapsed":490,"user":{"displayName":"김나리","userId":"01119342099285995005"}}},"source":["# train, test dataset을 만들어준다\n","class mydataset(data.Dataset):\n","    @staticmethod\n","    def sort_key(ex):\n","        return len(ex.text)\n","    def __init__(self, text_field, label_field, path=None, examples=None, **kwargs):\n","        fields = [('text', text_field), ('label', label_field)]\n","        if examples is None:\n","            path = self.dirname if path is None else path\n","            examples = []\n","            for i,line in enumerate(open(path,'r',encoding='utf-8')):\n","                if i==0:      # 첫번째 라인은 skip\n","                    continue\n","                line = line.strip().split('\\t') # text, label 필드가 /tab으로 구분되어 있다                  \n","                txt = line[1].split(' ')  # 공백을 기준으로 문자열을 나누어 토큰 리스트를 만든다. line[0]에는 ID\n","               \n","                # examples: 학습 텍스트, 라벨 텍스트\n","                # data.Example : Defines a single training or test example.\n","                examples += [ data.Example.fromlist( [txt, line[2]],fields ) ]\n","        # Create a dataset from a list of Examples and Fields.\n","        # fields : field name, field \n","        super(mydataset, self).__init__(examples, fields, **kwargs) \n"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-YgsUg5kI86","outputId":"7e77b88e-a27c-4f4d-817b-067a9f458c80","executionInfo":{"status":"ok","timestamp":1665010449647,"user_tz":-540,"elapsed":13,"user":{"displayName":"김나리","userId":"01119342099285995005"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["# Field 객체는 다음과 같은 값을 통하여 데이터의 각 필드를 처리하는 방법을 지정\n","# fix_length: A fixed length that all examples using this field will be padded to, or None for flexible sequence lengths. \n","# sequential: Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.\n","# batch_first: Whether to produce tensors with the batch dimension first. Default: False.\n","##text_field = data.Field(fix_length=20)\n","text_field = data.Field(fix_length=30)\n","label_field = data.Field(sequential=False, batch_first = True, unk_token = None)\n","\n","# 학습데이터 Dataset\n","train_data = mydataset(text_field,label_field,path='/content/gdrive/My Drive/Colab Notebooks/aivle/data/nsm/small_ratings_train_tok.txt')\n","# 테스트데이터 Dataset\n","test_data = mydataset(text_field,label_field,path='/content/gdrive/My Drive/Colab Notebooks/aivle/data/nsm/small_ratings_test_tok.txt')\n","\n","text_field.build_vocab(train_data)    # Construct the Vocab object \n","label_field.build_vocab(train_data)   # Construct the Vocab object \n","\n","# Create Iterator objects for train data, test data\n","train_iter, test_iter = data.Iterator.splits(\n","                            (train_data, test_data), \n","                            batch_sizes=(100, 1), repeat=False)#, device = -1)\n","len(text_field.vocab)"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["21893"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"WPOJtjBWkI88","outputId":"919ac7bf-5df8-400a-f48e-83c1d596919f","executionInfo":{"status":"ok","timestamp":1665010454578,"user_tz":-540,"elapsed":510,"user":{"displayName":"김나리","userId":"01119342099285995005"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["rnn = RNN_Text(len(text_field.vocab),2)     # embed_num, class_num\n","optimizer = torch.optim.Adam(rnn.parameters())\n","rnn.train()"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RNN_Text(\n","  (embed): Embedding(21893, 100)\n","  (rnn): LSTM(100, 256, bidirectional=True)\n","  (out): Linear(in_features=512, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"HEGfWoiykI8_","outputId":"2feebe99-122b-4fc7-b9b7-6e94a8767cd2","executionInfo":{"status":"ok","timestamp":1665010764569,"user_tz":-540,"elapsed":306404,"user":{"displayName":"김나리","userId":"01119342099285995005"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["%%time\n","bool_debug = True    # 텐서의 차원을 출력할 경우 True로 설정\n","print_idx = 3        # 출력 횟수\n","for epoch in range(10):\n","    \n","    totalloss = 0\n","    for batch in train_iter:\n","        optimizer.zero_grad()\n","        \n","        txt = batch.text        # torch.Size([20, 100])\n","        label = batch.label     # torch.Size([100])\n","        \n","        if bool_debug and print_idx > 0:\n","          print (\"txt.shape:\", txt.shape)\n","          print_idx -= 1\n","\n","        # inithiddend : hidden state, cell state 초기화 함수\n","        rnn.inithidden(txt.size(1))   # 배치 사이즈를 전달\n","        # 학습 실행\n","        pred = rnn(txt)\n","        \n","        if bool_debug and print_idx > 0:\n","          print(\"pred.shape:\", pred.shape)\n","          print(\"label.shape:\", label.shape)\n","          print_idx -= 1        \n","\n","        loss = F.cross_entropy(pred, label)\n","        totalloss += loss.data\n","        \n","        loss.backward()\n","        optimizer.step()\n","        \n","    print(epoch,'epoch')  \n","    print('loss : {:.3f}'.format(totalloss.numpy()))\n","       \n","torch.save(rnn,'/content/gdrive/My Drive/Colab Notebooks/aivle/model/rnn_model.pt')"],"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["txt.shape: torch.Size([30, 100])\n","pred.shape: torch.Size([100, 2])\n","label.shape: torch.Size([100])\n","txt.shape: torch.Size([30, 100])\n","0 epoch\n","loss : 69.746\n","1 epoch\n","loss : 69.193\n","2 epoch\n","loss : 68.289\n","3 epoch\n","loss : 64.821\n","4 epoch\n","loss : 51.177\n","5 epoch\n","loss : 38.424\n","6 epoch\n","loss : 29.222\n","7 epoch\n","loss : 21.683\n","8 epoch\n","loss : 15.690\n","9 epoch\n","loss : 11.219\n","CPU times: user 5min 1s, sys: 6.7 s, total: 5min 8s\n","Wall time: 5min 5s\n"]}]},{"cell_type":"code","metadata":{"id":"ldrqhEt6kI9B","outputId":"3127956a-2dad-45d6-8d4e-165b66a61fed","executionInfo":{"status":"ok","timestamp":1665010845175,"user_tz":-540,"elapsed":2169,"user":{"displayName":"김나리","userId":"01119342099285995005"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["%%time\n","bool_debug = True    # 텐서의 차원을 출력할 경우 True로 설정\n","\n","from sklearn.metrics import classification_report\n","correct = 0\n","incorrect = 0\n","rnn.eval()\n","y_test = []\n","prediction = []\n","\n","# 텐서 차원 확인용\n","print_tensor_shape = 2\n","print_idx = 1\n","\n","for batch in test_iter:\n","    txt = batch.text            # txt.shape: torch.Size([max_sent_len, 1])\n","    label = batch.label         # label.shape: torch.Size([1])\n","    y_test.append(label.data[0])\n","    \n","    rnn.inithidden(txt.size(1))\n","   \n","    pred = rnn(txt)               # pred.shape: torch.Size([1, 2])\n","    \n","    _ , ans = torch.max(pred,dim=1) # ans.shape: torch.Size([1])\n","    prediction.append(ans.data[0])\n","    \n","    \n","    #---------------------------------------\n","    # 텐서 형태, 데이터를 출력\n","    if bool_debug and print_tensor_shape > 0:\n","      print(\"-----\", print_idx, \"-----\") \n","      print(\"prediction:\", prediction)\n","      print(\"y_test:\", y_test)\n","      print(\"pred.shape:\", pred.shape)\n","      #print(\"pred.data[0]:\", pred.data[0])\n","      print(\"pred[0]:\", pred[0])\n","      print(\"pred[0][0]:\", pred[0][0])\n","      print(\"pred[0][1]:\", pred[0][1])\n","      print(\"ans.data[0]:\", ans.data[0])\n","      print(\"ans.shape:\", ans.shape)\n","      print(\"txt.shape:\", txt.shape)\n","      print(\"label.shape:\", label.shape)\n","      print(\"label.data[0]:\", label.data[0])\n","      \n","      print()\n","      print_tensor_shape -= 1\n","      print_idx += 1\n","      #---------------------------------------\n","\n","    if ans.data[0] == label.data[0]:  # ans.data[0]: tensor(0) 또는 tensor(1)\n","        correct += 1    \n","    else:\n","        incorrect += 1\n","    \n","print ('correct : ', correct)\n","print ('incorrect : ', incorrect)\n","print(classification_report(torch.tensor(y_test), \n","                            torch.tensor(prediction), \n","                            digits=4, \n","                            target_names=['negative', 'positive']))\n"],"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["----- 1 -----\n","prediction: [tensor(1)]\n","y_test: [tensor(1)]\n","pred.shape: torch.Size([1, 2])\n","pred[0]: tensor([-0.2583,  0.4676], grad_fn=<SelectBackward>)\n","pred[0][0]: tensor(-0.2583, grad_fn=<SelectBackward>)\n","pred[0][1]: tensor(0.4676, grad_fn=<SelectBackward>)\n","ans.data[0]: tensor(1)\n","ans.shape: torch.Size([1])\n","txt.shape: torch.Size([30, 1])\n","label.shape: torch.Size([1])\n","label.data[0]: tensor(1)\n","\n","----- 2 -----\n","prediction: [tensor(1), tensor(1)]\n","y_test: [tensor(1), tensor(1)]\n","pred.shape: torch.Size([1, 2])\n","pred[0]: tensor([-2.9234,  2.7380], grad_fn=<SelectBackward>)\n","pred[0][0]: tensor(-2.9234, grad_fn=<SelectBackward>)\n","pred[0][1]: tensor(2.7380, grad_fn=<SelectBackward>)\n","ans.data[0]: tensor(1)\n","ans.shape: torch.Size([1])\n","txt.shape: torch.Size([30, 1])\n","label.shape: torch.Size([1])\n","label.data[0]: tensor(1)\n","\n","correct :  77\n","incorrect :  23\n","              precision    recall  f1-score   support\n","\n","    negative     0.9091    0.6000    0.7229        50\n","    positive     0.7015    0.9400    0.8034        50\n","\n","    accuracy                         0.7700       100\n","   macro avg     0.8053    0.7700    0.7632       100\n","weighted avg     0.8053    0.7700    0.7632       100\n","\n","CPU times: user 831 ms, sys: 49.1 ms, total: 880 ms\n","Wall time: 1.16 s\n"]}]},{"cell_type":"code","metadata":{"id":"yEWu-5QokI9E"},"source":[],"execution_count":null,"outputs":[]}]}