{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ec11f1-d725-4f69-9547-6169fc46267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현장에서 많이씀(성능이 좋기때문) => RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6586a9b-b735-473d-b739-6811b33bedc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params \n",
    "KNN { 'n_neighbors' : range(3,51,2), 'metric' : ['euclidean', 'manhattan']  }\n",
    "DecisionTree { 'max_depth':range(2,11), 'min_samples_leaf':range(10,101,10) }\n",
    "RandomForest { 'n_estimators':[20,50,100], 'max_features':range(1,21) }\n",
    "XGBoost {'learning_rate' : np.linspace(0.01,0.2, 20), 'n_estimators':range(60,200,20), 'max_depth':[3,4,5,6]}\n",
    "SVM { 'C' : np.linspace(0.01, 100, 50), 'gamma':[0.001,0.01,.1,1] }\n",
    "\n",
    "GridSearchCV(m, params, cv=5, scoring = 'neg_mean_absolute_error')\n",
    "XGBRegressor(objective = 'reg:squarederror')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21635a-7ffc-458a-8c3f-1153adb17a03",
   "metadata": {},
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7900e-4296-4361-976c-e65a7c03245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 한글폰트 설정\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font=\"NanumGothicCoding\", \n",
    "        rc={\"axes.unicode_minus\":False}, # 마이너스 부호 깨짐 현상 해결\n",
    "        style='darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b2fbff-8574-435a-8be6-f587fc7a189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#라이브러리들을 불러오자.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 전처리\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 모델링\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import * \n",
    "\n",
    "# 비지도 학습\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8c3a6c-841d-4042-bb6e-c7c3520e15f7",
   "metadata": {},
   "source": [
    "## target 변수 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31aedfe-394b-4ab8-8096-622dad5d30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['Creditability'].value_counts())\n",
    "print(data['Creditability'].value_counts()/ data.shape[0])\n",
    "\n",
    "data['Creditability'].value_counts().plot(kind = 'barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcedc36-a3d4-431e-b28b-d4bf36b11b95",
   "metadata": {},
   "source": [
    "## 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f296f100-1f25-458a-9625-b687d7da503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label'] = submission_pred\n",
    "submission\n",
    "\n",
    "submission['label']= submission['label'].replace([0,1],['ham','spam'])\n",
    "\n",
    "submission.to_csv(\"/aihub/data/M2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb77912-4f8d-40f8-850f-56bd72498178",
   "metadata": {},
   "source": [
    "## to_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196b1549-3337-4561-af05-ef18ca0390d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['기준일ID'] = df_total['기준일ID'].astype('str')\n",
    "\n",
    "air_201['time'] = pd.to_datetime(air_201['time'],format='%Y%m%d%H')\n",
    "air_202['time'] = pd.to_datetime(air_202['time'],format='%Y%m%d%H')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d154d6a-7f80-40dc-aa6e-c437c8391566",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3cdeb5-e440-42eb-9823-186597581100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터분할1\n",
    "target = 'Sales'\n",
    "x = data.drop(target, axis=1)\n",
    "y = data.loc[:, target]\n",
    "\n",
    "# 가변수화\n",
    "dumm_cols = ['ShelveLoc','Education','Urban', 'US']\n",
    "x = pd.get_dummies(x, columns = dumm_cols, drop_first = True)\n",
    "\n",
    "# 데이터 분할2\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=.2\n",
    "                                                  , random_state = 2022)\n",
    "\n",
    "# 스케일링\n",
    "scaler = MinMaxScaler()\n",
    "x_train_s = scaler.fit_transform(x_train)\n",
    "x_val_s = scaler.transform(x_val)\n",
    "\n",
    "x_train_s = pd.DataFrame(x_train_s, columns=list(x))\n",
    "x_val_s = pd.DataFrame(x_val_s, columns=list(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4c264-ab90-4350-90f1-4e1db257837e",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4aff41-5048-451a-9a9a-73a9fa73a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost\n",
    "!pip install category_encoders\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffca176-b38f-4b10-bb0a-7cdfc4200586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본모델\n",
    "cat_model= CatBoostRegressor()\n",
    "cat_model.fit(train_x, train_y)\n",
    "cat_predict = cat_model.predict(test_x)\n",
    "\n",
    "print(mean_squared_error(test_y, cat_predict, squared=False))\n",
    "print(r2_score(test_y, cat_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c54b6b1-7b82-4168-a091-cba953a17f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화\n",
    "CB = CatBoostRegressor(depth=4,bagging_temperature=2.099,learning_rate=0.02091,subsample=0.2325)\n",
    "CB.fit(train_x, train_y)\n",
    "CB_pred = CB.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1c352-db6f-4b27-9158-fd4a72209927",
   "metadata": {},
   "source": [
    "* 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e94a4e-4d3a-4be7-b029-aa082c46ecc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bayesian-optimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "def fit_model(train_x,train_y,test_x,depth,bagging_temperature,learning_rate,subsample):\n",
    "    cat_model = CatBoostRegressor(depth = int(depth),\n",
    "                                  bagging_temperature=bagging_temperature,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  random_state=1339,#\n",
    "                                  verbose=0,#\n",
    "                                  subsample=subsample\n",
    "                                  ).fit(train_x,train_y)\n",
    "    cat_predict = cat_model.predict(test_x)\n",
    "    return cat_predict\n",
    "\n",
    "def CAT_cv(depth,bagging_temperature,learning_rate,subsample):\n",
    "\n",
    "    cat_predict = fit_model(np.array(train_x),np.array(train_y),np.array(test_x),depth,bagging_temperature,learning_rate,subsample)\n",
    "    score = r2_score(test_y,cat_predict)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b0d12-697d-4f0c-a639-4225ff466eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = { 'depth': (3, 15),\n",
    "            'bagging_temperature': (1, 10),\n",
    "            'learning_rate': (0.01, 1),\n",
    "            'subsample' : (0.01,1),\n",
    "            }\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "bo = BayesianOptimization(f = CAT_cv, pbounds = pbounds, random_state = 42,verbose = 2)\n",
    "bo.maximize(init_points = 5, n_iter = 10,acq = 'ei',xi = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736578d2-2961-4060-806f-b5d9e2571627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble model\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "cat1 = CatBoostRegressor(depth=5,bagging_temperature=2.909,learning_rate=0.01,subsample=0.7559)\n",
    "cat2 = CatBoostRegressor(depth=4,bagging_temperature=2.308,learning_rate=0.01,subsample=1)\n",
    "cat3 = CatBoostRegressor(depth=4,bagging_temperature=2.099,learning_rate=0.02091,subsample=0.2325)\n",
    "\n",
    "\n",
    "eclf = VotingRegressor(estimators=[\n",
    "         ('cat1', cat1), ('cat2', cat2),('cat3', cat3)])\n",
    "\n",
    "eclf.fit(train_x, train_y)\n",
    "eclf_predict = eclf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060557b2-7651-4219-8c9d-7f9003118188",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(mean_squared_error(test_y,eclf_predict)))\n",
    "print(r2_score(test_y,eclf_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64255320-fd98-4f20-b828-d87efa0f4db4",
   "metadata": {},
   "source": [
    "## Fitting Graph ->Elbow Method [모델과적합]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ec031-30e5-4a94-8c13-8a7602d610b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "* 가장 단순한 모델(평균모델)\n",
    "** knn : k를 최대로 크게하면 평균 모델이 됨.\n",
    "** k의 최대값은 학습 데이터의 행 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fe7a64-aa84-4141-8d05-a60747447be9",
   "metadata": {},
   "source": [
    "* Decision Tree\n",
    "** Decision Tree는 나무의 크기가 클 수록 복잡한 모델\n",
    "** 크기를 결정하는 파라미터는 max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff7702-02a6-428e-aca3-0b02b207cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델링\n",
    "n = x_train_s.shape[0]\n",
    "model = KNeighborsRegressor(n_neighbors = n) # train set의 행 수\n",
    "model.fit(x_train_s, y_train)\n",
    "pred_train = model.predict(x_train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc2d5d-ca76-4a77-aaf1-45bf0b8c5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_train = [] # train set을 가지고 예측한 결과\n",
    "result_val = [] # val set을 가지고 예측한 결과\n",
    "k_values = list(range(1,101))\n",
    "\n",
    "# KNN\n",
    "for d in k_values :\n",
    "    model = KNeighborsClassifier(n_neighbors= d)\n",
    "    model.fit(x_train_s, y_train)\n",
    "    pred_tr, pred_val = model.predict(x_train_s), model.predict(x_val_s)\n",
    "    result_train.append(accuracy_score(y_train, pred_tr))\n",
    "    result_val.append(accuracy_score(y_val, pred_val))\n",
    "    \n",
    "result_train = [] # train set을 가지고 예측한 결과\n",
    "result_val = [] # val set을 가지고 예측한 결과\n",
    "depth = list(range(1,21))\n",
    "\n",
    "# Decision Tree\n",
    "for d in depth :\n",
    "    model = DecisionTreeClassifier(max_depth = d)\n",
    "    model.fit(x_train, y_train)\n",
    "    pred_tr, pred_val = model.predict(x_train), model.predict(x_val)\n",
    "    result_train.append(accuracy_score(y_train, pred_tr))\n",
    "    result_val.append(accuracy_score(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b42a9-2736-410b-9789-40287fd91b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "plt.plot(k_values, result_train, label = 'train_acc', marker = 'o')\n",
    "plt.plot(k_values, result_val, label = 'val_acc', marker = 'o')\n",
    "\n",
    "plt.xlabel('Complexity')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beeb07e-8f9e-450f-a0c8-592fffca7cff",
   "metadata": {},
   "source": [
    "## Decision Tree[ 변수중요도 그래프 ] 실전에서 많이 씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4979c9b5-e881-45d8-91b7-1159b6e0615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 이름을 이용하여 시각화\n",
    "plt.figure(figsize = (20,8)) # 그림 사이즈 조절\n",
    "plot_tree(m3.best_estimator_, feature_names = x_train.columns, \n",
    "               class_names= ['Bad', 'Good'], filled = True, fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484790ba-8b17-43e3-bd6a-852f81a151fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(importance, names):\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "    fi_df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.barplot(x='feature_importance', y='feature_names', data = fi_df)\n",
    "\n",
    "    plt.xlabel('FEATURE IMPORTANCE')\n",
    "    plt.ylabel('FEATURE NAMES')\n",
    "    plt.grid()\n",
    "\n",
    "    return fi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59b9a5-05b1-4e0a-9404-6a1de506990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = plot_feature_importance(model.feature_importances_, list(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19d05d-637d-4055-8820-180621d28edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = plot_feature_importance(model.best_estimator_.feature_importances_, list(x_train)) # 튜닝했기 때문에"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26fdad3-eb3d-4e64-bca5-4d3429b744a8",
   "metadata": {},
   "source": [
    "## Regessor 차트 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672f86a-faa5-4385-85d5-70ad9462e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜닝 과정 로그를 df로 저장 합시다.\n",
    "result = pd.DataFrame(m1_gs.cv_results_)\n",
    "\n",
    "# 튜닝 결과를 그래프로 그려봅시다.\n",
    "plt.figure(figsize = (10,6))\n",
    "sns.lineplot(x='param_max_depth', y='mean_test_score', data = result)  # plt.plot\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620285a-44ce-4fcc-8f12-eec866a79908",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVR(Regressor)\n",
    "# 이를 차트로 그려봅시다.\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.lineplot(x = 'param_C', y = 'mean_test_score', data = result, hue = 'param_gamma')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4d163-b6ec-4ec9-b50e-b91a5054114b",
   "metadata": {},
   "source": [
    "## XGB 변수중요도 / 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79b9a7-8b55-47a7-86c0-ad406137af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#위에꺼 plot_feature_importance 함수 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cfedd-07e6-484e-b631-9e5fc4a9fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb의 변수 중요도\n",
    "weight : 모델 전체에서 해당 feature가 split될 때 사용된 횟수의 합(plot_tree 에서의 기본값)\n",
    "gain : feature별 평균 imformation gain.(model.feature_importances_ 의 기본값)\n",
    "cover : feature가 split 할때의 샘플 수의 평균."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf69414-2d92-4b9e-b32d-1d520116b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, plot_tree, plot_importance\n",
    "plt.rcParams['figure.figsize'] = 8, 5\n",
    "plot_importance(model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c8e9e-0bb5-4b42-a041-af290eb0af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = plot_feature_importance(model.feature_importances_, list(x),6) #x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f47772-a1b8-44b5-b798-32b0f78cb161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화(Tree하나씩 열어볼 수 있음)\n",
    "plt.rcParams['figure.figsize'] = 20,20  # 파일전체에 영향을 미침 !!!!\n",
    "plot_tree(model, num_trees = 0) \n",
    "plt.show()\n",
    "\n",
    "# xgboost 자체 plot_tree 함수를 제공합니다.\n",
    "# plot_tree(model, num_trees = 0)\n",
    "# num_trees : 전체 트리 5개짜리 모델이므로 각각 0~4까지 인덱스로 조회해 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f2c63-94d3-4652-982d-d4bbf308f063",
   "metadata": {},
   "source": [
    "## 성능비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7fa2d-6358-4b95-b3d1-d030c05b573e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RMSE, MAE, MAPE = [],[],[]\n",
    "model_desc = ['lr_selected', 'lr_all','knn','dt','rf','xgb','svm']\n",
    "pred = [p1, p2, p3, p4, p5, p6,p7]\n",
    "\n",
    "for i, p in enumerate(pred) :\n",
    "    RMSE.append(mean_squared_error(y_val, p, squared=False))\n",
    "    MAE.append(mean_absolute_error(y_val, p))\n",
    "    MAPE.append(mean_absolute_percentage_error(y_val, p))\n",
    "\n",
    "result = pd.DataFrame({'model_desc':model_desc,'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543c4d3-d6f5-415f-abb8-251ab9fec147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_val, pred1))\n",
    "print(classification_report(y_val, pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5921def8-9b58-4f63-94d5-5fd1579763dd",
   "metadata": {},
   "source": [
    "## Decision Tree 시각화 및 변수중요도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74512328-ac56-4f62-8165-a1b739bd2c2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10)) # 그림 사이즈 조절\n",
    "plot_tree(m1, feature_names = list(x_train), \n",
    "               class_names= ['Stay', 'Leave'], filled = True, fontsize = 10);  # class_names = [target의 내용]\n",
    "plt.show()\n",
    "print('-'*88)\n",
    "# 변수 중요도\n",
    "print(list(x_train))\n",
    "print(m1.feature_importances_)\n",
    "print('-'*88)\n",
    "print(classification_report(y_val, p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eaf2ba-8169-49c1-ab4e-c2503d7049fe",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀를 위한 전진선택법 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219f321f-66db-420b-80f0-047e9d7bad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant\n",
    "\n",
    "def forward_stepwise_linear(x_train, y_train):\n",
    "\n",
    "    # 변수목록, 선택된 변수 목록, 단계별 모델과 AIC 저장소 정의\n",
    "    features = list(x_train)\n",
    "    selected = []\n",
    "    step_df = pd.DataFrame({ 'step':[], 'feature':[],'aic':[]})\n",
    "\n",
    "    # \n",
    "    for s in range(0, len(features)) :\n",
    "        result =  { 'step':[], 'feature':[],'aic':[]}\n",
    "\n",
    "        # 변수 목록에서 변수 한개씩 뽑아서 모델에 추가\n",
    "        for f in features :\n",
    "            vars = selected + [f]\n",
    "            x_tr = x_train[vars]\n",
    "            model = OLS(y_train, add_constant(x_tr)).fit(disp=False)\n",
    "            result['step'].append(s+1)\n",
    "            result['feature'].append(vars)\n",
    "            result['aic'].append(model.aic)\n",
    "        \n",
    "        # 모델별 aic 집계\n",
    "        temp = pd.DataFrame(result).sort_values('aic').reset_index(drop = True)\n",
    "\n",
    "        # 만약 이전 aic보다 새로운 aic 가 크다면 멈추기\n",
    "        if step_df['aic'].min() < temp['aic'].min() :\n",
    "            break\n",
    "        step_df = pd.concat([step_df, temp], axis = 0).reset_index(drop = True)\n",
    "\n",
    "        # 선택된 변수 제거\n",
    "        v = temp.loc[0,'feature'][s]\n",
    "        features.remove(v)\n",
    "\n",
    "        selected.append(v)\n",
    "    \n",
    "    # 선택된 변수와 step_df 결과 반환\n",
    "    return selected, step_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f64df06-f8e2-4d8a-b2ef-97d362534801",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars, result = forward_stepwise_logistic(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0df5b-343a-4271-9877-0ac0cef5ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선택된 변수\n",
    "lr_m1 = LinearRegression()\n",
    "lr_m1.fit(x_train[vars], y_train)\n",
    "p1 = lr_m1.predict(x_val[vars])\n",
    "\n",
    "print('RMSE : ', mean_squared_error(y_val, p1, squared=False))\n",
    "print('MAE  : ', mean_absolute_error(y_val, p1))\n",
    "print('MAPE : ', mean_absolute_percentage_error(y_val, p1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf20d7b-664d-4370-8e00-55b8e409fb21",
   "metadata": {},
   "source": [
    "## cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c24999-fa58-4b07-b323-450de6a1323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# train + validation set을 이용하여 학습, 예측, 평가를 한번에. (여기서는 .fit 이 아님!)\n",
    "dt_result = cross_val_score(model, x, y, cv=10)\n",
    "print(dt_result)\n",
    "print(dt_result.mean(), dt_result.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0876d-960f-44ea-b0a5-8a2acb251549",
   "metadata": {},
   "source": [
    "## Permutation Feature Importance(그 외 변수중요도 그래프)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd55a3-cfe5-4fcb-ace0-1b399f315a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d30d2c-6d7c-4780-9492-1a021fc4c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfi1 = permutation_importance(model4, x_val_s, y_val, n_repeats=10, scoring = 'r2', random_state=2022)\n",
    "# scoring = 'accuracy' default[분류 모델]\n",
    "# deep learning 모델에 대해서는 명시적으로 scoring = 'r2'을 지정해 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58147e8f-a372-4b2c-a951-7df3e66ce309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature별 Score 분포\n",
    "plt.figure(figsize = (10,8))\n",
    "for i,vars in enumerate(list(x)) :\n",
    "    sns.kdeplot(pfi1.importances[i], label = vars)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820c5a2-0ff6-4c62-b698-2feac8aa0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = pfi1.importances_mean.argsort()\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.boxplot(pfi1.importances[sorted_idx].T, vert=False, labels=x.columns[sorted_idx])\n",
    "plt.axvline(0, color = 'r')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288d3e93-fd33-4813-b864-ad186951fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균값으로 변수중요도 그래프 그리기\n",
    "pfi1 = permutation_importance(model1, x_val_s, y_val, n_repeats=10, scoring = 'r2', random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc70a2-80be-4adf-a371-f3b0cd35c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 중요도 plot(가져오기)\n",
    "result = plot_feature_importance(pfi1.importances_mean, list(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa99e68-a6f0-4011-b165-60185b89f33d",
   "metadata": {},
   "source": [
    "## Partial Dependence Plot(개별 변수별 관계)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25612daa-29b7-4292-a14c-4f611064b304",
   "metadata": {},
   "source": [
    "* 변수 중요도,PDP : train 데이터로 살펴보는 것이 기본이다. 그리고 결과가 유사해야 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524504e-98c0-4b99-85f4-ddc30f355d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import plot_partial_dependence, partial_dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b994514-5ba4-44e8-8c6f-d3514006025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개별 변수 분석\n",
    "var = 'MonthlyIncome'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "plot_partial_dependence(model, features = [var], X = x_val)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac85a3e-a9a0-407d-9b5a-5b6c28609b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터로 확인\n",
    "var = 'MonthlyIncome'\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 12, 8\n",
    "plot_partial_dependence(model, features = [var], X = x_val, kind = 'both') #kind = 'average'\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d35f900-19bf-48d5-b435-3be786601d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두개 변수 비교\n",
    "plot_partial_dependence(model, features = ['rm','lstat'], X = x_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9cea2-ca6a-4260-8783-c65510ac347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두개 변수 비교\n",
    "plot_partial_dependence(model2, features = [('CreditAmount','Age')], X = x_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13acb0-6d39-4e2f-a7ad-b98442f5d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일링한거 데이터 프레임에 넣어서 뽑아야함\n",
    "x_train_s = pd.DataFrame(x_train_s, columns = list(x))  # 칼럼이름 지정 필요!!\n",
    "x_val_s = pd.DataFrame(x_val_s, columns = list(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd92fd6-e143-48c7-839e-96ab575b40ff",
   "metadata": {},
   "source": [
    "## SHAP 값으로 모델의 예측 설명하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26bd8b4-529c-4568-a7e7-b8167cbef6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형회귀는 회귀계수로 변수 기여도 해석해도 무방"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd050132-df09-4291-bc29-658ad30df832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (회귀모델)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer1.shap_values(x_train)\n",
    "\n",
    "# (분류모델)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777c0f6-5481-49e9-a1d9-5dcce0d4076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측하기 위해서는 입력데이터(x)가 2차원이어야 합니다.\n",
    "pred = model1.predict(x_train.iloc[0:1,:])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361de6de-deb7-4cf6-8ce8-6f1a3a522cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train의 평균(회귀)\n",
    "explainer1.expected_value\n",
    "\n",
    "# train의 평균(분류)\n",
    "explainer1.expected_value[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e9a1ae-2275-452e-b6c6-567d2b419c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스별 데이터\n",
    "shap.initjs() # javascript 시각화 라이브러리 --> colab에서는 모든 셀에 포함시켜야 함.-> 아나콘다에서는 한번만 !\n",
    "index=371\n",
    "# force_plot(전체평균, shapley_values, input)\n",
    "shap.force_plot(explainer1.expected_value, shap_values1[index,:], x.iloc[index,:])\n",
    "\n",
    "# 분류모델\n",
    "shap.initjs() # javascript 시각화 라이브러리 --> colab에서는 모든 셀에 포함시켜야 함.-> 아나콘다에서는 한번만 !\n",
    "index=932\n",
    "# force_plot(전체평균, shapley_values, input)\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][index,:], x.iloc[index,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a1d2a-6bf7-46e5-86da-61fda96261e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터\n",
    "shap.initjs() # javascript 시각화 라이브러리 --> colab에서는 모든 셀에 포함시켜야 함.-> 아나콘다에서는 한번만 !\n",
    "\n",
    "# force_plot(전체평균, shapley_values, input)\n",
    "shap.force_plot(explainer1.expected_value, shap_values1[0, :], x_train.iloc[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edf225d-cdd8-4e25-a11e-d96ef4fadca5",
   "metadata": {},
   "source": [
    "## class balance를 맞추기 위한 resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f098a68-04d1-45a1-9473-e893b5fca852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3a8f7-42da-4cb1-a3b2-a0d0644bf8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a0459-dca1-4b10-9c53-79827e1a9fb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 비지도 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010b59d6-b431-4063-9d80-7500cf3a167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1,50)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    model = KMeans(n_clusters=k)\n",
    "    model.fit(mobile_x)\n",
    "    inertias.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816bcac3-b88c-456d-b8d6-846b97a4e51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34af48-e397-48a1-aaa7-c27279a6a29a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모델링[KNN]\n",
    "model = KMeans(n_clusters=5)\n",
    "model.fit(mobile_x)\n",
    "pred = model.predict(mobile_x)\n",
    "pred = pd.DataFrame(pred, columns = ['predict'])\n",
    "# 결과 보기\n",
    "mobile_y.reset_index(inplace=True, drop=True)\n",
    "result = pd.concat([mobile_x, mobile_y, pred], axis =1)\n",
    "result.CHURN = result.CHURN.astype('int')\n",
    "# 클러스터 별 고객이탈율\n",
    "result.groupby('predict')['CHURN'].mean()\n",
    "# 전체 평균\n",
    "result['CHURN'].value_counts() / result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e88bc6-48aa-43a0-bf61-6195f7628f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN 모델을 만들어 봅시다.\n",
    "model = DBSCAN(eps=0.1, min_samples=3)\n",
    "model.fit(x)\n",
    "# fitting한 후에 모델의 labels_ 값이 찾아낸 군집 종류입니다.\n",
    "clusters = model.labels_\n",
    "# 군집 번호 중 -1은 이상치를 의미합니다.(어느 군집에도 포함 안되는 값들!)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126cc2cd-68ba-4e0c-b45f-2cf050a208b6",
   "metadata": {},
   "source": [
    "## 시계열 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359fee4-1fa7-45ce-94c0-0b540f19c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore')\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1b49b-b28b-4008-b77d-f1ef70702d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 시계열로 확인 \n",
    "plt.figure(figsize = (20,8))\n",
    "plt.plot(data['sales'])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "temp = data[-100:]\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.plot(temp['sales'], marker ='o')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa23b2-c179-4c13-a838-d655a9e750e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잔차 분석\n",
    "def residual_diag(residuals, lags = 20) :\n",
    "\n",
    "    print('* 정규성 검정(> 0.05) : ', round(spst.shapiro(residuals)[1],5))\n",
    "    print('* 정상성 검정(< 0.05) : ', round(sm.tsa.stattools.adfuller(residuals)[1],5))\n",
    "    print('* 자기상관성 확인(ACF, PACF)')\n",
    "    fig,ax = plt.subplots(1,2, figsize = (15,5))\n",
    "    plot_acf(residuals, lags = lags, ax = ax[0])\n",
    "    plot_pacf(residuals, lags = lags, ax = ax[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733fa1d-5bec-493d-98c1-78d906b7dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 타입으로 변경\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "# 날짜를 인덱스로 변환\n",
    "data['DT'] = data['date']\n",
    "data.set_index('DT', inplace=True)\n",
    "data.head()\n",
    "## 날짜단위 지정하기 : freq / 인덱스 조회시, 마지막에 있는 freq 옵션\n",
    "# 일단위\n",
    "data.asfreq('D').head()\n",
    "# 월(말)단위\n",
    "data.asfreq('M').head()\n",
    "# 월초 단위\n",
    "data.asfreq('MS').head()\n",
    "#(추가) 빠진값 찾기\n",
    "temp = data.asfreq('D')\n",
    "temp.isna().sum()\n",
    "# 채우기\n",
    "data.asfreq('D', method = 'ffill')\n",
    "df = data.asfreq('D') # 일단위 데이터이므로 이걸로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c782bfe-33b8-4ed8-ba1e-14cd98bc0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y 만들기\n",
    "df['y'] = df['sales'].shift(-1)\n",
    "display(df.head())\n",
    "display(df.tail())\n",
    "# 제일 마지막 행은 삭제\n",
    "df.dropna(axis = 0, inplace = True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077b5ee-bae1-4267-9902-c3c237830513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "# 1) x, y 나누기\n",
    "# .values(넘파이 어레이)로 변환해서 저장하는 이유 ➡ 데이터 스플릿 index를 적용해서 데이터를 가져오기 위해서\n",
    "target = 'y'\n",
    "\n",
    "x = df.drop([target, 'date'], axis = 1)\n",
    "y = df.loc[:, target]\n",
    "\n",
    "# 시계열 데이터 분할\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "x.shape\n",
    "# validation set size\n",
    "val_size = 30\n",
    "nfold = 3\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = nfold, test_size = val_size)\n",
    "tscv\n",
    "\n",
    "#참조\n",
    "# .split을 이용하여 fold 하나씩 인덱스들을 뽑아 낼 수 있음.\n",
    "for train_index, val_index in tscv.split(x):\n",
    "    print(\"Train:\", train_index, \"Val:\", val_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7509d9-02fa-4c4d-9963-d83d88be275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델링\n",
    "# loop 돌며 모델링(cross-validation) 수행\n",
    "rmse, mae, mape = [],[],[]\n",
    "residuals = []\n",
    "pred = []\n",
    "model = LinearRegression()\n",
    "\n",
    "for train_index, val_index in tscv.split(x):\n",
    "\n",
    "    # 인덱스로 데이터 분할\n",
    "    x_train, y_train = x.iloc[train_index], y.iloc[train_index]\n",
    "    x_val, y_val = x.iloc[val_index], y.iloc[val_index]\n",
    "\n",
    "    # 학습\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # 예측\n",
    "    pr = model.predict(x_val)\n",
    "    pred += list(pr)\n",
    "\n",
    "    # 평가\n",
    "    rmse.append(mean_squared_error(y_val, pr, squared = False))\n",
    "    mae.append(mean_absolute_error(y_val, pr))\n",
    "    mape.append(mean_absolute_percentage_error(y_val, pr))\n",
    "\n",
    "    # 잔차 : 각 fold의 결과를 리스트로 변환하여 추가\n",
    "    residuals += list(y_val - pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969db4bd-ac0c-4fbb-adb9-1e023faaf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 평가\n",
    "print('RMSE : ',round(np.mean(rmse),4))\n",
    "print('MAE  : ',round(np.mean(mae),4))\n",
    "print('MAPE : ',round(np.mean(mape),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3ccda9-2ee3-442d-aa60-215ffc227846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 비교\n",
    "n = val_size * nfold\n",
    "pred = pd.Series(pred, index = y[-n:].index)\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.plot(y[:-n], label = 'train')\n",
    "plt.plot(y[-n:], label = 'val')\n",
    "plt.plot(pred, label = 'predicted')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (20,8))\n",
    "plt.plot(y[-n:], label = 'val')\n",
    "plt.plot(pred, label = 'predicted')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb6b063-297d-4b19-90ca-b535597d6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가: 잔차분석\n",
    "## 시각화\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.plot(residuals)\n",
    "plt.axhline(0, color = 'r', ls = '--')\n",
    "plt.axhline(np.mean(residuals), color = 'g', ls = '--')\n",
    "plt.show()\n",
    "\n",
    "## ACF, PACF(자기상관성 여부 확인)\n",
    "lags = 20\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize = (15,5))\n",
    "plot_acf(residuals, lags = lags, ax = ax[0])\n",
    "plot_pacf(residuals, lags = lags, ax = ax[1])\n",
    "plt.show()\n",
    "\n",
    "## 검정\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "stats.shapiro(residuals)[1]  # 정규성 검정 : Shapiro-Wilk 검정\n",
    "sm.tsa.stattools.adfuller(residuals)[1]  # 정상성 검정 : ADF 검정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d21e45a-b164-4278-96bb-e2922bb8410f",
   "metadata": {},
   "source": [
    "## 시계열 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a079f9-4e0d-49d0-b774-c97f26ab228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 시각화\n",
    "def plot_model_result(y_train, y_val, pred) :\n",
    "    pred = pd.Series(pred, index = y_val.index)\n",
    "\n",
    "    # 전체 시각화\n",
    "    plt.figure(figsize = (20,12))\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.plot(y_train, label = 'train')\n",
    "    plt.plot(y_val, label = 'val')\n",
    "    plt.plot(pred, label = 'pred')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.plot(y_val, label = 'val')\n",
    "    plt.plot(pred, label = 'pred')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9739f4e-d804-4514-9beb-94847e7120d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y값 살펴보기\n",
    "residual_diag(y_train, lags = 30)\n",
    "\n",
    "# ARIMA 모델링\n",
    "m1_1 = sm.tsa.SARIMAX(y_train, order=(1,0,1)).fit() # ARMA\n",
    "m1_2 = sm.tsa.SARIMAX(y_train, order=(1,1,1)).fit() # ARIMA\n",
    "\n",
    "# SARIMA 모델링 : P, D, Q, m = 1,1,1,7 로 모델을 생성합시다.\n",
    "m2_1 = sm.tsa.SARIMAX(y_train, order=(5,1,4), seasonal_order=(1,1,1,7)).fit()\n",
    "\n",
    "m3_1 = sm.tsa.SARIMAX(y_train, order=(5,1,4), seasonal_order=(1,1,1,7), exog=x_train).fit()\n",
    "\n",
    "# 평가[잔차 진단]\n",
    "residuals = m1_1.resid  # y_train과 예측값 차이\n",
    "residual_diag(residuals)\n",
    "# 평가[AIC] ->선형 모델에서의 적합도와, feature가 과도하게 늘어나는 것을 방지하도록 설계된 통계량이 AIC 입니다.\n",
    "# 값이 작을 수록 좋은 모델\n",
    "# 공식 : 𝐴𝐼𝐶=−2 ln⁡(𝐿)+2𝑘 ➡ - 모델의 적합도 + 변수의 갯수\n",
    "print('model1 AIC :', m1_1.aic)\n",
    "# 평가[Validation]\n",
    "pred = m1_1.forecast(30)   # SARIMAX 모델을 생성하고, 예측할 때는 exog=x_val 옵션이 들어가야 함. \n",
    "print('MAE :', mean_absolute_error(y_val, pred))\n",
    "print('MAPE:', mean_absolute_percentage_error(y_val, pred))\n",
    "# 평가[결과 시각화]\n",
    "plot_model_result(y_train, y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc011a5-ea27-4a08-bce6-dd151e82c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 튜닝\n",
    "\n",
    "# 학습\n",
    "from itertools import product\n",
    "# product 함수를 이용하여 값의 조합을 구성\n",
    "p = [1,2,3,4,5]\n",
    "q = [1,2,3,4,5]\n",
    "d = [1]\n",
    "iter = list(product(p,d,q))\n",
    "iter\n",
    "\n",
    "# 튜닝 \n",
    "mae, aic = [],[]\n",
    "for i in iter :\n",
    "    model_fit = sm.tsa.SARIMAX(y_train, order=(i[0],i[1],i[2])).fit()\n",
    "    pred = model_fit.forecast(30)\n",
    "    mae.append( mean_absolute_error(y_val, pred))\n",
    "    aic.append(model_fit.aic)\n",
    "    print(i)\n",
    "    \n",
    "result = pd.DataFrame({'params(p,d,q)' : iter, 'mae' : mae, 'aic':aic})\n",
    "\n",
    "display(result.loc[result['mae'] == result.mae.min()])\n",
    "display(result.loc[result['aic'] == result.aic.min()])\n",
    "\n",
    "# 가장 성능이 좋은 p, d, q 값으로 모델을 생성합니다.\n",
    "m1_3 = sm.tsa.SARIMAX(y_train, order=(5,1,4)).fit()\n",
    "\n",
    "# 평가[잔차진단]\n",
    "residuals = m1_3.resid\n",
    "residual_diag(residuals) # seasonallity \n",
    "# 평가[AIC]\n",
    "print('model2 AIC :', m1_3.aic)\n",
    "# 평가[validation]\n",
    "p1 = m1_3.forecast(30)\n",
    "print('MAE :', mean_absolute_error(y_val, p1))\n",
    "print('MAPE:', mean_absolute_percentage_error(y_val, p1))\n",
    "# 결과 시각화\n",
    "plot_model_result(y_train, y_val, p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841415ba-a113-49b3-a213-0f2df244b0af",
   "metadata": {},
   "source": [
    "## 딥러닝[keras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63aa4c-1211-43fd-a84b-c6ffb57254d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0ad5b2-0eec-4277-8908-5af216a9dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape , y.shape  # 확인해볼것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb9d68f-a6f4-4763-8fef-6d406055849b",
   "metadata": {},
   "source": [
    "* Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56367fdb-b8cf-4b4b-b174-7834b14485f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ [Linear Regression]\n",
    "# 혹시 이미 그려둔 그래프가 있다면 날려줘!\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# 레이어들을 사슬로 연결하 듯이 연결!\n",
    "input_layer = keras.layers.Input(shape=(1,))\n",
    "output_layer = keras.layers.Dense(1)(input_layer)\n",
    "\n",
    "# 모델의 시작과 끝을 지정\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "model.fit(x, y, epochs=10, verbose=1)\n",
    "# 결과 출력\n",
    "print(model.predict(x).reshape(-1,) )\n",
    "\n",
    "@ [Logistic Regression]\n",
    "keras.backend.clear_session()\n",
    "input_layer = keras.layers.Input(shape=(1,))\n",
    "output_layer = keras.layers.Dense(1, activation='sigmoid')(input_layer)\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "model.fit(x, y, epochs=10, verbose=1)\n",
    "print(y)\n",
    "print(model.predict(x).reshape(-1,) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb0919-cb10-4b52-b673-e043cccdd6c2",
   "metadata": {},
   "source": [
    "* Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67fd5e0-82fb-42c9-a314-9715ab7552b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ [Linear Regression]\n",
    "# 1번 청소 : 이미 만들어진 모델이 있다면 그 모델을 없애줘\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# 2번 모델 선언\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# 3번 모델 블록 조립\n",
    "model.add( keras.layers.Input(shape=(1,)) )\n",
    "model.add( keras.layers.Dense(1) )\n",
    "\n",
    "## 오리지널 Sequential API\n",
    "# model.add( keras.layers.Dense(1, input_shape=(1,)) )\n",
    "\n",
    "# 4번 컴파일 \n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.fit(x[:15], y[:15], epochs=10, verbose=1)\n",
    "print(y[15:])\n",
    "print(model.predict(x[15:]))\n",
    "\n",
    "@ [Logistic Regression]\n",
    "# 혹시 이미 그려둔 그래프가 있다면 날려줘!\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# model에 순차적으로 레이어를 쌓아가겠다는 의도!\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# model에 인풋 값을 받는 레이어를 넣음\n",
    "model.add( keras.layers.Input(shape=(1,)) )\n",
    "# model에 Dense 레이어를 넣을거야 (최초의 레이어) : weight를 곱하고, bias를 더해주는 과정\n",
    "model.add( keras.layers.Dense(1, activation='sigmoid') )\n",
    "\n",
    "# 오리지널 Sequential API\n",
    "# model.add( keras.layers.Dense(1, input_shape=(1,), activation='sigmoid') )\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics=['accuracy'])\n",
    "# keras.losses.binary_crossentropy 이걸로도 가능\n",
    "\n",
    "model.fit(x[:15], y[:15], epochs=10, verbose=1)\n",
    "print(y[15:])\n",
    "print(model.predict(x[15:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91289ec3-9fbe-42a4-bb08-76d33084e753",
   "metadata": {},
   "source": [
    "* 멀티클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60420175-9abb-4a4d-aa65-a20f60569a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미래 데이터를 먼제 떼어내야하기 때문에 먼저 분리해준다.(미래데이터는 건드리지 않는다)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.2, random_state=2022)\n",
    "x_train.shape, y_train.shape\n",
    "# One-Hot Encoding  (get_dummies= x에만 적용)\n",
    "class_n = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, class_n)\n",
    "y_test = to_categorical(y_test, class_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e62134-a6df-4a34-bd32-92d177f7835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ y 확인하기\n",
    "iris.target_names\n",
    "\n",
    "# One-Hot Encoding  (get_dummies= x에만 적용)\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "y = to_categorical(y, 3) # 반복 실행 주의!!(계속 생성됨)\n",
    "x.shape, y.shape\n",
    "\n",
    "@ [Sequential]\n",
    "keras.backend.clear_session()\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input(shape=(4,)))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs=10,verbose=1)\n",
    "model.predict(x_test).reshape(-1)\n",
    "y.argmax(axis=1)\n",
    "\n",
    "@ [Functional]\n",
    "keras.backend.clear_session()\n",
    "il = keras.layers.Input(shape=(4,))\n",
    "ol = keras.layers.Dense(3,activation='softmax')(il)\n",
    "model = keras.models.Model(il,ol)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "model.fit(x_train,y_train,epochs=10, verbose=1)\n",
    "pred = model.predict(x_test)\n",
    "pred[:5] # 합치면 확률값 =1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b4936-56b0-4659-a35c-f24f016b555f",
   "metadata": {},
   "source": [
    "* Hidden layer 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab5d1f-3c3d-4be5-80ae-f6c1a261c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ [Sequential API]\n",
    "\n",
    "keras.backend.clear_session()\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input(shape=(13,)))\n",
    "model.add(keras.layers.Dense(32,activation='relu'))\n",
    "model.add(keras.layers.Dense(32,activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "@ [Functional]\n",
    "keras.backend.clear_session()\n",
    "il = keras.layers.Input(shape=(30,))\n",
    "hl = keras.layers.Dense(32, activation='relu', name='hidden1')(il)\n",
    "hl = keras.layers.Dense(32, activation='relu', name='hidden2')(hl)\n",
    "ol = keras.layers.Dense(1,activation='sigmoid')(hl)\n",
    "model = keras.models.Model(il,ol)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "@ [multi-Functional]\n",
    "keras.backend.clear_session()\n",
    "il = keras.layers.Input(shape=(4,))\n",
    "hl = keras.layers.Dense(32, activation='relu', name='hidden1')(il)\n",
    "hl = keras.layers.Dense(32, activation='relu', name='hidden2')(hl)  # 변수명 같아도 상관없음\n",
    "ol = keras.layers.Dense(3,activation='softmax')(hl)\n",
    "model = keras.models.Model(il,ol)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " # optimizer=keras.optimizers.Adam(0.01)-> eta\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8066a064-fb8b-457c-93af-792304ffbdb9",
   "metadata": {},
   "source": [
    "## 딥러닝[ANN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6361238-7234-4dd4-937c-62c28c1ad4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참조코드\n",
    "'''\n",
    "matplolib inline 명령어를 통해서\n",
    "matplot으로 그리는 플롯들을 주피터 노트북 내에서 볼 수 있게 해준다.\n",
    "포맷을 retina로 바꾸면 그래프의 화질이 훨씬 좋아진다.\n",
    "'''\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20445e-b94f-47b6-b20a-c81a42dedc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) 전처리\n",
    "x = wine.data\n",
    "y = wine.target\n",
    "x.shape, y.shape\n",
    "data.target_names\n",
    "\n",
    "## reshape -> flatten하면 필요없음\n",
    "train_x.shape\n",
    "train_x = train_x.reshape([train_x.shape[0],-1])\n",
    "test_x = test_x.reshape([test_x.shape[0],-1])\n",
    "train_x.shape # 28*28\n",
    "## min-max scaling\n",
    "max_n, min_n = train_x.max(), train_x.min()\n",
    "max_n, min_n\n",
    "train_x = (train_x - min_n) / (max_n - min_n)\n",
    "test_x = (test_x - min_n) / (max_n - min_n)\n",
    "print(f'max : {train_x.max()} / min : {train_x.min()}')\n",
    "## target feature : One-hot Encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "len_y = len(set(train_y))\n",
    "train_y = to_categorical(train_y, len_y)\n",
    "test_y = to_categorical(test_y, len_y)\n",
    "\n",
    "2) 모델링\n",
    "train_x.shape, train_y.shape\n",
    "keras.backend.clear_session()\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input(train_x.shape[1]))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(0.01),metrics='accuracy')\n",
    "model.summary()\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss',  # 무얼 관찰할지(관측대상)\n",
    "                   min_delta=0,         # 최소한 나빠지지 않으면 괜찮아\n",
    "                   patience=5,          # 중요! 몇번이나 참을지(거기까지 개선 안되면 멈출거야)\n",
    "                   verbose=1,            \n",
    "                   restore_best_weights=True)  # (반드시 사용)학습이 멈췄을 때, 최적의 가중치로 전환해줌\n",
    "model.fit(train_x, train_y, \n",
    "          validation_split=0.2,  # Train data의 20%를 Validation data로!\n",
    "          callbacks=[es],        # Early Stopping 적용\n",
    "          verbose=1, epochs=50)\n",
    "\n",
    "3) 예측\n",
    "pred_train = model.predict(train_x)\n",
    "pred_test = model.predict(test_x)\n",
    "single_pred_train = pred_train.argmax(axis=1)\n",
    "single_pred_test = pred_test.argmax(axis=1)\n",
    "logi_train_accuracy = accuracy_score(train_y.argmax(axis=1), single_pred_train)\n",
    "logi_test_accuracy = accuracy_score(test_y.argmax(axis=1), single_pred_test)\n",
    "print('트레이닝 정확도 : {:.2f}%'.format(logi_train_accuracy*100))\n",
    "print('테스트 정확도 : {:.2f}%'.format(logi_test_accuracy*100))\n",
    "\n",
    "4) 확인\n",
    "mnist_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "figure = plt.figure(figsize=(20, 10))\n",
    "for i, index in enumerate(np.random.choice(test_x.shape[0], size=15, replace=False)):\n",
    "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
    "    # Display each image\n",
    "    ax.imshow(np.squeeze(test_x[index].reshape([28,-1])), cmap='gray' )\n",
    "    \n",
    "    predict_index = pred_test[index].argmax(axis=0)\n",
    "    true_index = test_y[index].argmax(axis=0)\n",
    "    # Set the title for each image\n",
    "    ax.set_title(f\"{mnist_labels[predict_index]} ({mnist_labels[true_index]})\",\n",
    "                 color=(\"green\" if predict_index == true_index else \"red\"))\n",
    "    \n",
    "5) 오답확인\n",
    "true_false = (test_y.argmax(axis=1) == single_pred_test)\n",
    "f_id = np.where(true_false == False)[0]\n",
    "f_n = len(f_id)\n",
    "id = f_id[rd.randrange(0,f_n)]\n",
    "print(f'id = {id}' )\n",
    "print(f'다음 그림은 숫자 {test_y.argmax(axis=1)[id]} 입니다.')\n",
    "print(f'모델의 예측 : {single_pred_test[id]}')\n",
    "print(f'모델의 카테고리별 확률 : {np.floor(pred_test[id]*100)}')\n",
    "if test_y.argmax(axis=1)[id] == single_pred_test[id] :\n",
    "    print('===============')\n",
    "    print('정답입니다')\n",
    "    print('===============')\n",
    "else : \n",
    "    print('===============')\n",
    "    print('틀렸어요')\n",
    "    print('===============')\n",
    "plt.imshow(test_x[id].reshape([28,-1]), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "6) 평가\n",
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe819b5d-4d0f-4b45-af16-e768a4fb3bff",
   "metadata": {},
   "source": [
    "## Modeling : multi-input & Concatenate layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7af6d6-ab4f-49b4-b2fa-3bf9e6b15d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) 데이터 불러오기\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "print(data.DESCR) \n",
    "df_x = pd.DataFrame(x, columns=iris.feature_names)\n",
    "# null값 확인 필요\n",
    "iris.target_names\n",
    "\n",
    "2) train set, test set 구분하기\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(df_x, y, test_size=0.1, random_state=2022)\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape\n",
    "\n",
    "추가) Scaling (데이터 수치->너무 편차가 커서(min_max 필요))\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm_scaler = MinMaxScaler()\n",
    "train_x = mm_scaler.fit_transform(train_x)\n",
    "test_x = mm_scaler.transform(test_x)\n",
    "pd.DataFrame(train_x, columns=iris.feature_names).describe()\n",
    "train_x = pd.DataFrame(train_x, columns=iris.feature_names)\n",
    "test_x = pd.DataFrame(test_x, columns=iris.feature_names)\n",
    "\n",
    "3) length끼리, width끼리\n",
    "print(df_x.columns)\n",
    "tr_x_l = train_x.loc[:, ['sepal length (cm)', 'petal length (cm)'] ]\n",
    "tr_x_w = train_x.loc[:, ['sepal width (cm)', 'petal width (cm)'] ]\n",
    "tr_x_l.shape, tr_x_w.shape\n",
    "te_x_l = test_x.loc[:, ['sepal length (cm)', 'petal length (cm)'] ]\n",
    "te_x_w = test_x.loc[:, ['sepal width (cm)', 'petal width (cm)'] ]\n",
    "\n",
    "4) One-hot Encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "train_y = to_categorical(train_y, 3)\n",
    "test_y = to_categorical(test_y, 3)\n",
    "train_y.shape\n",
    "\n",
    "5-1) Modeling : multi-input & Concatenate layer\n",
    "@ Functional API만 가능!!!!\n",
    "# 1. 세션 클리어\n",
    "clear_session()\n",
    "# 2. 레이어 사슬처럼 엮기 : input 2개!\n",
    "il_l = Input( shape=(2,) )\n",
    "hl_l = Dense(2, activation=relu)(il_l)\n",
    "il_w = Input( shape=(2,) )\n",
    "hl_w = Dense(2, activation=relu)(il_w)\n",
    "cl = Concatenate()([hl_l, hl_w])\n",
    "ol = Dense(3, activation=softmax)(cl)\n",
    "# 3. 모델 시작과 끝 지정\n",
    "model = Model([il_l, il_w], ol)\n",
    "# 4. 모델 컴파일\n",
    "model.compile(loss=categorical_crossentropy, metrics=['accuracy'],\n",
    "              optimizer=Adam())\n",
    "model.summary()\n",
    "\n",
    "5-2) Modeling : multi-input & Add layer\n",
    "tr_x_p.shape, train_y.shape\n",
    "keras.backend.clear_session()\n",
    "il_s = keras.layers.Input(shape=(2,))\n",
    "hl_s = keras.layers.Dense(6,activation='swish')(il_s)\n",
    "il_p = keras.layers.Input(shape=(2,))\n",
    "hl_p = keras.layers.Dense(6,activation='swish')(il_p)\n",
    "add_l = keras.layers.Add()([hl_s, hl_p])\n",
    "ol = keras.layers.Dense(3,activation='softmax')(add_l)\n",
    "model = keras.models.Model([il_s,il_p],ol)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "6) 모델 시각화\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)\n",
    "\n",
    "7) 학습\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, restore_best_weights=True)\n",
    "model.fit([tr_x_l, tr_x_w], train_y, validation_split=0.1, epochs=1000, verbose=1, callbacks=[es])\n",
    "        # 모델에 붓는 순서 지켜야됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c88908-5b03-4c91-a051-dc4506ec43ed",
   "metadata": {},
   "source": [
    "## 시각지능 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ddd25f-c347-485b-ab54-d37c107c3a0c",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727bd33f-550a-4886-a70d-99e29451d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = io.loadmat(\"notMNIST_small.mat\")\n",
    "\n",
    "# transform data\n",
    "X = data['images']\n",
    "y = data['labels']\n",
    "resolution = 28\n",
    "classes = 10\n",
    "\n",
    "X = np.transpose(X, (2, 0, 1))\n",
    "\n",
    "y = y.astype('int32')\n",
    "X = X.astype('float32') #/ 255.\n",
    "\n",
    "# shape: (sample, x, y, channel)\n",
    "X = X.reshape((-1, resolution, resolution, 1))\n",
    "\n",
    "# looking at data; some fonts are strange\n",
    "i = np.random.randint(0, 18723)\n",
    "print(i)\n",
    "plt.imshow( X[i,:,:,0] )\n",
    "plt.title( \"ABCDEFGHIJ\"[y[i]] )\n",
    "\n",
    "# random letters\n",
    "rows = 6\n",
    "fig, axs = plt.subplots(rows, classes, figsize=(classes, rows))\n",
    "\n",
    "for letter_id in range(10):\n",
    "    letters = X[y == letter_id]\n",
    "    for i in range(rows):\n",
    "        ax = axs[i, letter_id]\n",
    "        ax.imshow(letters[np.random.randint(len(letters)),:,:,0],\n",
    "                  cmap='Greys', interpolation='none')\n",
    "        ax.axis('off')\n",
    "        \n",
    "# splitting data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2022)\n",
    "\n",
    "x_train.shape, y_train.shape\n",
    "\n",
    "max_n, min_n = x_train.max(),x_train.min()\n",
    "\n",
    "x_train = (x_train - min_n)/ (max_n - min_n)\n",
    "x_test = (x_test - min_n)/ (max_n - min_n)\n",
    "\n",
    "len(np.unique(y_train))\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test,10)\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324770b9-89db-4637-81b7-f9369f2567ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "il = keras.layers.Input(shape=(28,28,1,))\n",
    "hl = keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu',)(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "\n",
    "hl = keras.layers.Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu')(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "\n",
    "hl = keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2))(hl)\n",
    "hl = keras.layers.Dropout(.25)(hl)\n",
    "\n",
    "hl = keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "\n",
    "hl = keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "\n",
    "hl = keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2))(hl)\n",
    "hl = keras.layers.Dropout(.25)(ml)\n",
    "\n",
    "hl = keras.layers.Flatten()(hl)\n",
    "hl = keras.layers.Dense(512,)(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl) \n",
    "ol = keras.layers.Dense(10, activation='softmax')(hl)\n",
    "\n",
    "model = keras.models.Model(il,ol)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4a828-c809-4b9f-9b7e-409f10f1c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "il = keras.layers.Input(shape=(28,28,1))\n",
    "hl = keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(il)\n",
    "hl = keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(hl)\n",
    "hl = keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2))(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "hl = keras.layers.Dropout(.25)(hl)\n",
    "\n",
    "hl = keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(hl)\n",
    "hl = keras.layers.Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(hl)\n",
    "hl = keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2))(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "hl = keras.layers.Dropout(.25)(hl)\n",
    "\n",
    "hl = keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(hl)\n",
    "hl = keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(hl)\n",
    "hl = keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(hl)\n",
    "hl = keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2))(hl)\n",
    "hl = keras.layers.BatchNormalization()(hl)\n",
    "hl = keras.layers.Dropout(.25)(hl)\n",
    "\n",
    "hl = keras.layers.Flatten()(hl)\n",
    "hl = keras.layers.Dense(1024, activation='relu')(hl)\n",
    "hl = keras.layers.Dense(1024, activation='relu')(hl)\n",
    "hl = keras.layers.Dense(1024, activation='relu')(hl)\n",
    "ol = keras.layers.Dense(10, activation='softmax')(hl)\n",
    "\n",
    "model = keras.models.Model(il ,ol)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(0.01), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3af6c7-0138-4488-acee-7d699c10745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True)\n",
    "model.fit(x_train, y_train,, verbose=1, validation_split=.2, batch_size=1024, epochs=100, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e329d13-50dd-461f-8122-7937a58416e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "pred_array = np.zeros(shape=(y_pred.shape[0], y_pred.shape[1]))\n",
    "idx = 0\n",
    "\n",
    "for arr_val in y_pred :\n",
    "    # print(arr_val)\n",
    "    pred_array[idx][arr_val.argmax()] = 1\n",
    "    idx += 1\n",
    "    \n",
    "pred_array.shape\n",
    "\n",
    "@ 성능평가\n",
    "from sklearn.metrics import accuracy_score\n",
    "print( f'{accuracy_score(y_test, pred_array):.4f}' )\n",
    "\n",
    "@ 문자 이미지 시각화\n",
    "import random as rd\n",
    "character = {0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:'F', 6:'G', 7:'H', 8:'I', 9:'J'}\n",
    "\n",
    "rand_n = rd.randrange(0, 3744)\n",
    "\n",
    "print(f'id = {rand_n}')\n",
    "print(f'실제 문자 : {character[y_test[rand_n].argmax()]}')\n",
    "print(f'모델의 문자 예측 : {character[y_pred[rand_n].argmax()]}' )\n",
    "print(f'모델의 문자별 예측 확률 : {np.round(y_pred[rand_n]*100)}')\n",
    "# print(f'모델의 문자들 총 확률 : {sum(np.round(y_pred[rand_n]*100))}')\n",
    "\n",
    "print('====================================================')\n",
    "\n",
    "if y_test[rand_n].argmax() == y_pred[rand_n].argmax() :\n",
    "    print('정답')\n",
    "else :\n",
    "    print('오답')\n",
    "\n",
    "print('====================================================')\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(x_test[rand_n].reshape(28, 28), cmap='gray')\n",
    "plt.title(\"ABCDEFGHIJ\"[y_test[rand_n].argmax()] )\n",
    "plt.show()\n",
    "\n",
    "# 틀린 문자만 확인\n",
    "t_f = ( y_test.argmax(axis=1) == y_pred.argmax(axis=1) )\n",
    "false_id = np.where(t_f==False)[0]\n",
    "false_n = len(false_id)\n",
    "\n",
    "id = false_id[rd.randrange(0, false_n)]\n",
    "\n",
    "print(f'id = {id}')\n",
    "print(f'실제 문자 : {character[y_test[id].argmax()]}')\n",
    "print(f'모델의 문자 예측 : {character[y_pred[id].argmax()]}' )\n",
    "print(f'모델의 문자별 예측 확률 : {np.round(y_pred[id]*100)}')\n",
    "# print(f'모델의 문자들 총 확률 : {sum(np.round(y_pred[rand_n]*100))}')\n",
    "\n",
    "print('====================================================')\n",
    "\n",
    "if y_test[id].argmax() == y_pred[id].argmax() :\n",
    "    print('정답')\n",
    "else :\n",
    "    print('오답')\n",
    "\n",
    "print('====================================================')\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(x_test[id].reshape(28, 28), cmap='gray')\n",
    "plt.title(\"ABCDEFGHIJ\"[y_pred[id].argmax()] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc456f-3a99-42de-b0e2-7f01c39f4370",
   "metadata": {},
   "source": [
    "### 동영상CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838adee-514a-4c66-b0f9-0fda84315f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 불러오기.\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839bd74c-adb0-424b-ab8a-f7c80192c633",
   "metadata": {},
   "source": [
    "1) 코랩 사용 시 구글 드라이브 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03339166-1cd2-43d1-b971-52c49af0379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd1a77-c415-4c7d-83d2-ffd6e2eeebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(TUTORIAL_PATH + \"/tutorial.mp4\")\n",
    "video.isOpened()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec18179-c804-4f34-9963-371c1ce99650",
   "metadata": {},
   "source": [
    "2. 경로 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a44879-e174-405d-8a08-abe6f7239bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT_PATH 확인 \n",
    "import os\n",
    "\n",
    "# 구글 드라이브 내 프로젝트 압축해제된 영역 (구글 드라이브 최상위에 압축해제 시 그대로 실행 수정 X)\n",
    "WORK_SPACE = \"\"\n",
    "\n",
    "if os.getcwd() == '/content' :\n",
    "  # 구글 드라이브 사용 시 \n",
    "  ROOT_PATH = \"/content/drive/MyDrive/\"+WORK_SPACE+\"/AIVLE3rd_individual\"\n",
    "else :\n",
    "  ROOT_PATH = os.path.abspath('..')\n",
    "\n",
    "# 모델 예측을 위한 test 데이터가 저장되는 경로\n",
    "TEST_PATH = ROOT_PATH + \"/test\"\n",
    "# 테스트 영상 test.mp4 경로 \n",
    "\n",
    "TEST_VIDEO = TEST_PATH + \"/test.mp4\"\n",
    "\n",
    "# 모델 예측을 위해 테스트 영상을 프레임 이미지로 자르고 저장하는 경로 (testGenerator 생성 시 PATH )   \n",
    "TEST_IMAGE = TEST_PATH + \"/image\"\n",
    "\n",
    "# 모델(.h5) 파일이 저장된 경로 (본인이 생성한 모델)\n",
    "MODEL_PATH = ROOT_PATH + \"/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7ffa93-58c1-45a5-8ec1-13048331848c",
   "metadata": {},
   "source": [
    "3) test data 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee20513-07ba-465a-a4fb-fa525112f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import gdown\n",
    "import shutil\n",
    "import zipfile\n",
    " \n",
    "google_path = 'https://drive.google.com/uc?id='\n",
    "file_id = \"10iKCHLPx-YFgkJcDqCgJPl83ZKYHvXmr\"\n",
    "output_name = 'test.zip'\n",
    "\n",
    "# 파일 다운로드\n",
    "gdown.download(google_path+file_id,output_name,quiet=False)\n",
    "\n",
    "# 파일 위치 이동\n",
    "shutil.move(\"./\" + output_name, ROOT_PATH)\n",
    "\n",
    "zip_file = ROOT_PATH + \"/\" + output_name\n",
    "\n",
    "# 압축해제 \n",
    "with zipfile.ZipFile(zip_file) as z:\n",
    "    z.extractall(ROOT_PATH)\n",
    "\n",
    "os.remove(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f4b0f2-2b64-4fe9-bcb1-b323e1b0b0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 환경 확인하기\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee88abf-0502-41ee-9f70-be30d66f6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_width = video.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "video_height = video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "video_length = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "video_fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(\"가로 :\", video_width)\n",
    "print(\"세로 :\", video_height)\n",
    "print(\"총 프레임 수 :\" , video_length)\n",
    "print(\"FPS :\", video_fps)\n",
    "print(\"영상 길이 : %d 초 :\"  %round(video_length/video_fps))\n",
    "\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd0bc8-39df-4714-8181-435200064a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참조 \n",
    "for x_data, t_data in train_generator:\n",
    "\n",
    "    print(x_data.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb449ef2-80f9-4486-a803-091a7ce703ba",
   "metadata": {},
   "source": [
    "* 이미지 만들기(CROP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b24df3-31c9-473f-b862-a3e6a6564528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습해보세요.\n",
    "test_IMAGE = \"/content/drive/MyDrive/AIVLE3rd_individual/train/samang\"\n",
    "\n",
    "TIME_MEASUERMENT_UNIT = 1 #TIME MEASUREMENT UNIT을 통해 몇 초 단위로 이미지를 저장할 지 선택\n",
    "\n",
    "if not os.path.exists(test_IMAGE):\n",
    "  os.mkdir(test_IMAGE)\n",
    "\n",
    "video = cv2.VideoCapture(\"/content/drive/MyDrive/AIVLE3rd_individual/video/train/220206_samang.mp4\")\n",
    "\n",
    "while video.isOpened():\n",
    "  ret,frame = video.read()\n",
    "  if ret:\n",
    "    # 현재 프레임 위치 (msec) \n",
    "    frame_sec = video.get(cv2.CAP_PROP_POS_MSEC)/1000\n",
    "    print(frame_sec%1)\n",
    "    # if (frame_sec % TIME_MEASUERMENT_UNIT == 0.6796875):\n",
    "    #  filename = test_IMAGE + \"/\" + str(round(frame_sec)) + \".jpg\"\n",
    "    #  cv2.imwrite(filename, frame) \n",
    "  else:\n",
    "    break\n",
    "\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba312c-9857-4099-8bd8-76d18226f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_IMAGE = \"/content/drive/MyDrive/AIVLE3rd_individual/train/boss\"\n",
    "\n",
    "# TIME_MEASUERMENT_UNIT = 1 #TIME MEASUREMENT UNIT을 통해 몇 초 단위로 이미지를 저장할 지 선택\n",
    "\n",
    "if not os.path.exists(test_IMAGE):\n",
    "  os.mkdir(test_IMAGE)\n",
    "\n",
    "video = cv2.VideoCapture(\"/content/drive/MyDrive/AIVLE3rd_individual/train/06boss\")\n",
    "\n",
    "count = 0\n",
    "\n",
    "while video.isOpened():\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    if ret:\n",
    "        if(int(video.get(1)) % video.get(cv2.CAP_PROP_FPS) == 0):\n",
    "            count += 1\n",
    "            # filename = test_IMAGE + \"/\" + str(count) + \".jpg\"\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # 흑백 변환\n",
    "            cv2.imwrite(filename, frame[55:80, 35:165]) # 코너 로고만 크롭해서 저장\n",
    "            # cv2.imwrite(filename, frame)\n",
    "    else: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1875e6-c30a-4c8f-ab14-314d54f6667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_IMAGE = \"/content/drive/MyDrive/AIVLE3rd_individual/train/20samang\"\n",
    "\n",
    "TIME_MEASUERMENT_UNIT = 1 #TIME MEASUREMENT UNIT을 통해 몇 초 단위로 이미지를 저장할 지 선택\n",
    "\n",
    "if not os.path.exists(test_IMAGE):\n",
    "  os.mkdir(test_IMAGE)\n",
    "\n",
    "video = cv2.VideoCapture(\"/content/drive/MyDrive/AIVLE3rd_individual/video/train/220220_samang.mp4\")\n",
    "\n",
    "count = 0\n",
    "\n",
    "while video.isOpened():\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    if ret:\n",
    "        if(round(int(video.get(1)) % video.get(cv2.CAP_PROP_FPS)) == 0):\n",
    "            count += 1\n",
    "            filename = test_IMAGE + \"/\" + str(count) + \".jpg\"\n",
    "            # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # 흑백 변환\n",
    "            # cv2.imwrite(filename, frame[55:80, 35:165]) # 코너 로고만 크롭해서 저장\n",
    "            cv2.imwrite(filename, frame)\n",
    "    else: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcb2717-f177-4980-9849-20b6e28841e0",
   "metadata": {},
   "source": [
    "### CROP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a91c9-1968-45fa-8c41-9acf6c60f912",
   "metadata": {},
   "source": [
    "* ①'매뉴얼하게 일일이 확인하는 방법'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631ae076-b09d-4450-a591-43becea6812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습해보세요.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class_map = {\n",
    "    0: 'jung',\n",
    "    1: 'park',    \n",
    "    2: 'sam',\n",
    "    3: 'sayuri',\n",
    "}\n",
    "\n",
    "image_class = glob.glob(IMAGE_PATH + \"*/*\")\n",
    "image_class.sort()\n",
    "\n",
    "fig, axes = plt.subplots(4, 5,figsize=(16, 9))\n",
    "class_count = 0 \n",
    "for c in image_class:  \n",
    "  for i in range(5):\n",
    "    original_image = cv2.imread(c + \"/\" + str(i*200) + \".jpg\")\n",
    "    rgb_image = cv2.cvtColor(original_image,cv2.COLOR_BGR2RGB)\n",
    "    # 특정 영역만 잘라서 이미지 비교\n",
    "    cropped_image = rgb_image[0:96, 0:320]\n",
    "    axes[class_count, i].imshow(cropped_image)\n",
    "    axes[class_count, i].set_title(class_map[class_count] , fontsize=15)    \n",
    "    axes[class_count, i].axis('off')\n",
    "  class_count += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f01bb7-bcae-4650-ab05-57600f0c64fe",
   "metadata": {},
   "source": [
    "* ②'이미지 속 동일한 규칙을 찾아내는 방법'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c4d9a-c42a-4400-b60d-966eb5d4cf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Tip] \n",
    "# 이미지 전체를 학습할 필요가 없다. \n",
    "# 코너를 구분하는데 필요한 영역은 좌측 상단에 고정되어 있음으로 해당 영역만 잘라내서 학습 데이터를 생성하는 것도 가능\n",
    "\n",
    "def croppedImage(IMAGE_PATH):\n",
    "  #이미지를 잘라낼 영역 \n",
    "  bounding_box = {\n",
    "      'start_x' : 0, \n",
    "      'start_y' : 0,\n",
    "      'end_x' : 320,    \n",
    "      'end_y' : 96\n",
    "  }\n",
    "  from tqdm.auto import tqdm, trange\n",
    "  image_list = glob.glob(IMAGE_PATH + '/*/*.jpg')\n",
    "  for image in tqdm(image_list, desc=\"IMAGE CROP PROGRESS\"):\n",
    "    original_image = cv2.imread(image)\n",
    "    cropped_image = original_image[bounding_box['start_y']:bounding_box['end_y'], bounding_box['start_x']:bounding_box['end_x']]    \n",
    "    cv2.imwrite(image, cropped_image) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc2481-4b46-4fc9-95a2-470d9db0f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "croppedImage(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a373b77-419a-4b9d-91e2-0eaec85bfe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = ROOT_PATH + \"/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df711c9c-24bf-408c-b46c-2043e0d407e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copytree(IMAGE_PATH, TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72fb6c4-e9e1-42c0-8320-3646609c4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 logo 부분만 crop하신다면 굳이 flip을 이용해서 데이터를 증강할 필요는 없습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243eaa7c-cbf7-4a6e-9a8b-86609b38c00d",
   "metadata": {},
   "source": [
    "4) 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c9bc9e-cf34-4b03-bd4f-02a0e9d29b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 96\n",
    "img_width = 320\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. /255,\n",
    "    validation_split=0.2,\n",
    "    # rotation_range=40,\n",
    "    # width_shift_range=0.2,\n",
    "    # height_shift_range=0.2,\n",
    "    # zoom_range=0.2,\n",
    "    # horizontal_flip=True,\n",
    "    # fill_mode='nearest'\n",
    ")\n",
    "# train_genrator 생성\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    batch_size=16,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "   )\n",
    "\n",
    "# validation_generator 생성 (train와 동일하게 해야함)\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    batch_size= batch_size,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c6919-874d-4fff-80ea-8c7f1d82c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preprocessing(TEST_VIDEO):\n",
    "  from tensorflow import keras\n",
    "  from keras.preprocessing.image import ImageDataGenerator\n",
    "  import os\n",
    "  import cv2\n",
    "\n",
    "  TEST_IMAGE = TEST_PATH + \"/image\"\n",
    "\n",
    "  TIME_MEASUERMENT_UNIT =1 #TIME MEASUREMENT UNIT을 통해 몇 초 단위로 이미지를 저장할 지 선택\n",
    "\n",
    "  if not os.path.exists(TEST_IMAGE):\n",
    "    os.mkdir(TEST_IMAGE)\n",
    "\n",
    "  video = cv2.VideoCapture(TEST_PATH + \"/test.mp4\")\n",
    "\n",
    "  while video.isOpened():\n",
    "    ret,frame = video.read()\n",
    "    if ret:\n",
    "      # 현재 프레임 위치 (msec) \n",
    "      frame_sec = video.get(cv2.CAP_PROP_POS_MSEC)/1000\n",
    "      if frame_sec.is_integer():\n",
    "        if (frame_sec % TIME_MEASUERMENT_UNIT == 0):\n",
    "          filename = TEST_IMAGE + \"/\" + str(round(frame_sec)) + \".jpg\"\n",
    "          cv2.imwrite(filename, frame) \n",
    "    else:\n",
    "      break\n",
    "\n",
    "  video.release()\n",
    "\n",
    "  batch_size=8\n",
    "  img_height=240\n",
    "  img_width=427\n",
    "\n",
    "  test_datagen = ImageDataGenerator(\n",
    "    rescale=1. /255,)\n",
    "  \n",
    "# test_genrator 생성\n",
    "  test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_PATH,\n",
    "    batch_size=batch_size,\n",
    "    target_size=(img_height, img_width),\n",
    "    class_mode='categorical',\n",
    "    subset='test',\n",
    "    )      \n",
    "\n",
    "  return test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e92e06-f8ba-4e36-b606-02a583212958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인 (수정하지 마세요.) - Found 1220 images belonging to 1 classes. 와 같은 메시지가 출력되어야 합니다. \n",
    "test_generator = my_preprocessing(TEST_VIDEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942ce55-3a9f-4a73-b2b5-ea8e7b2cc4ce",
   "metadata": {},
   "source": [
    "5) 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81450e10-cc92-416e-9a2f-fa01d29fae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape찍어보기\n",
    "from PIL import Image\n",
    "\n",
    "imagel = Image.open('/content/drive/MyDrive/AIVLE3rd_individual/train/park/1000.jpg')\n",
    "imagel.show()\n",
    "imagl_size = imagel.size\n",
    "print(imagl_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17673475-52bf-4a63-9e54-143ec127578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2D, MaxPooling2D 조합으로 층을 쌓습니다. 첫번째 입력층의 input_shape은 imageDataGenerator target size로 지정합니다.\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Input(shape=(img_height, img_width,3,)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu',))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(.25))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100,))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16597587-2290-4976-9035-35bdf5f06db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[img_height, img_width,3,]))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Adding convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Step 4 - Full Connection\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "# Step 5 - Output Layer\n",
    "cnn.add(tf.keras.layers.Dense(units=8, activation='softmax'))\n",
    "\n",
    "# Compiling the CNN\n",
    "cnn.compile(optimizer = 'adam', \n",
    "            loss = 'categorical_crossentropy', \n",
    "            metrics = ['accuracy'])\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e54641-1d04-4cb2-bdee-97fe54188108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "MODEL_PATH = '/content/drive/MyDrive/AIVLE3rd_individual/weights'\n",
    "# early_stopping \n",
    "cp = ModelCheckpoint(filepath= MODEL_PATH, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)\n",
    "\n",
    "# early_stopping\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=20,\n",
    "                    epochs=200,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=10,\n",
    "                    callbacks=[cp,es], \n",
    "                    batch_size=10,)\n",
    "\n",
    "@ 모델저장하기\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('/content/drive/MyDrive/AIVLE3rd_individual/model/[개인]미니프로젝트3차_A021087.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f56d1b0-3078-4449-9c90-1233860fe5ad",
   "metadata": {},
   "source": [
    "* 모델 SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad56627-fd47-4d79-a8f3-726387f095b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "name = '김나현'\n",
    "model.save(MODEL_PATH + '/미니프로젝트3차_' + name + '.h5')\n",
    "print(MODEL_PATH + '/미니프로젝트3차_' + name + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a5665-9eba-4ab7-9f50-cb2b30bd8393",
   "metadata": {},
   "source": [
    "6) 모델적용 예측결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da179aab-91a4-4a19-861e-b70bbd25f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model_predict(test_generator, model):\n",
    "  import datetime as df\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "\n",
    "  class_map = {\n",
    "    0: 'jung',\n",
    "    1: 'park',    \n",
    "    2: 'sam',\n",
    "    3: 'sayuri',\n",
    "  }\n",
    "\n",
    "  keys = ['predict_v1','predict_v2','predict_v3']\n",
    "  predict_verList = dict.fromkeys(keys)\n",
    "\n",
    "  predict = model.predict(test_generator)\n",
    "\n",
    "  np.set_printoptions(formatter={'float':lambda x:\"{0:0.3f}\".format(x)})\n",
    "  predict_v1 = pd.DataFrame({\n",
    "      'file' : test_generator.filenames,\n",
    "      class_map[0] : predict[:,0],\n",
    "      class_map[1] : predict[:,1],\n",
    "      class_map[2] : predict[:,2],\n",
    "      class_map[3] : predict[:,3],\n",
    "      })\n",
    "  \n",
    "  pd.options.display.float_format = '{:.2f}'.format\n",
    "  predict_verList['predict_v1'] = predict_v1\n",
    "\n",
    "\n",
    "  return predict_verList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55c6fd-37a9-4645-ba80-8c76c6f51f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('/content/drive/MyDrive/AIVLE3rd_individual/model/[개인]미니프로젝트3차_A021087.h5', \n",
    "                                custom_objects=None, compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732517c9-ff74-4c51-a7b1-21ba7385e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_verList = my_model_predict(test_generator, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5b0fe-1215-42c9-b4de-ba20fc750ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('모델결과_v1')\n",
    "predict_verList['predict_v1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ede200a-525c-41ed-a79d-26350e318f38",
   "metadata": {},
   "source": [
    "7) 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53f474-0ac5-4048-8381-5e54157c00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pred = model.predict(validation_generator)\n",
    "\n",
    "pred = np.argmax(pred, axis = 1 )\n",
    "\n",
    "f1 = f1_score(validation_generator.labels, pred, average = None)\n",
    "print('f1 score :', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b43e5-6962-40e3-bb9b-544dcd3cb59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "if not isinstance(history, dict):\n",
    "    history = history.history\n",
    "\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.title('Accuracy : Training vs Validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "if not isinstance(history, dict):\n",
    "    history = history.history\n",
    "\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('Loss : Training vs Validation')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b244a83-691d-4558-9251-8e9552c8c508",
   "metadata": {},
   "source": [
    "### UltraLytics YOLO v3 Image Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d32db-9414-4d3c-bbfc-1086f5e495d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) UltraLytics git에서 복사하기\n",
    "!git clone https://github.com/ultralytics/yolov3.git\n",
    "\n",
    "2) yolov3 폴더 이동 및 requirements.txt 내부 패키지 설치\n",
    "!cd yolov3; pip install -r /content/yolov3/requirements.txt\n",
    "\n",
    "3) Image Detection\n",
    "3-1) 예제 이미지 다운로드 => !wget -O [저장할 파일명] [파일 주소]\n",
    "!wget -O /content/yolov3/data/images/14th_street.jpg\\\n",
    "https://raw.githubusercontent.com/DrKAI/image/main/14th_Street_2005.jpg\n",
    "\n",
    "3-2) COCO Dataset(성능지표)으로 pretrained 된 weights 다운로드 [weights가 없으면 자동 다운로드] \n",
    "=> !mkdir [경로/디렉토리 명]\n",
    "=> pretrained weights 다운로드\n",
    "!mkdir /content/yolov3/pretrained\n",
    "!wget -O /content/yolov3/pretrained/yolov3-tiny.pt\\\n",
    "https://github.com/ultralytics/yolov3/releases/download/v9.6.0/yolov3-tiny.pt\n",
    "\n",
    "3-3) detect.py를 python으로 직접 호출하여 수행 => 명령어 도움말 : !cd yolo3; python detect.py -h\n",
    "!cd yolov3; python detect.py -h\n",
    "!cd yolov3; python detect.py \\\n",
    "    --weights '/content/yolov3/pretrained/yolov3.pt' \\\n",
    "    --source '/content/yolov3/data/images' \\\n",
    "    --project '/content/yolov3/detected' \\\n",
    "    --name 'images' \\\n",
    "    --img 640 \\\n",
    "    --conf-thres 0.1 \\\n",
    "    --iou-thres 0.4 \\\n",
    "    --line-thickness 2 \\\n",
    "    --exist-ok \\# 덮어쓰기\n",
    "    --device CPU\n",
    "\n",
    "@ Detect Image 살펴보기\n",
    "from IPython.display import Image\n",
    "from google.colab import files\n",
    "\n",
    "# Image(filename=[파일 경로])\n",
    "Image(filename='/content/yolov3/detected/images/14th_street.jpg', width=850)\n",
    "\n",
    "# files.download(filename=[파일 경로])\n",
    "files.download(filename='/content/yolov3/detected/images/14th_street.jpg')\n",
    "\n",
    "# zip\n",
    "!zip -r /content/detected_image.zip /content/yolov3/detected/images2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b633ee0-0881-4d27-b266-086b66b5e510",
   "metadata": {},
   "source": [
    "### UltraLytics_YOLOv3_VideoDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5675720-bd1b-4f39-83fa-f24b59aae4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov3.git\n",
    "\n",
    "!cd yolov3; pip install -r /content/yolov3/requirements.txt\n",
    "\n",
    "!mkdir /content/yolov3/data/videos\n",
    "!wget -O /content/yolov3/data/videos/noway.mp4\\\n",
    "https://github.com/DrKAI/image/raw/main/No_Way_This_Happened.mp4\n",
    "\n",
    "!mkdir /content/yolov3/pretrained\n",
    "!wget -O /content/yolov3/pretrained/yolov3.pt\\\n",
    "https://github.com/ultralytics/yolov3/releases/download/v9.6.0/yolov3.pt\n",
    "\n",
    "# !cd yolov3; python detect.py -h\n",
    "!cd yolov3; python detect.py \\\n",
    "    --weights '/content/yolov3/pretrained/yolov3.pt' \\\n",
    "    --source '/content/yolov3/data/videos/' \\\n",
    "    --project '/content/yolov3/detected' \\\n",
    "    --name 'videos' \\\n",
    "    --img 640 \\\n",
    "    --conf-thres 0.5 \\\n",
    "    --iou-thres 0.4 \\\n",
    "    --line-thickness 2 \\\n",
    "    --exist-ok\n",
    "    # --device CPU\n",
    "    \n",
    "from google.colab import files\n",
    "## colab은 멀티 다운로드 지원X\n",
    "## 폴더 압축하여 파일 하나로 만들고 다운로드\n",
    "!zip -r /content/detected_videos.zip /content/yolov3/detected/videos/\n",
    "\n",
    "files.download(filename='/content/yolov3/detected/videos/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328f69e8-44c7-408a-b75b-1453c5f2fb68",
   "metadata": {},
   "source": [
    "### UltraLytics_YOLOv5_CustomData_ImageDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d2333-3ea6-476f-9ec0-068da8150e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5\n",
    "    \n",
    "!cd yolov5; pip install -r requirements.txt\n",
    "\n",
    "@ Image Detection\n",
    "1) 사전 작업된 CustomData yaml 다운로드\n",
    "!wget -O /content/yolov5/data/street.yaml\\\n",
    "https://raw.githubusercontent.com/DrKAI/CV/main/street_example.yaml\n",
    "\n",
    "2) pretrained 된 weights 다운로드 => weights가 없으면 자동 다운로드\n",
    "!mkdir /content/yolov5/pretrained\n",
    "!wget -O /content/yolov5/pretrained/yolov5s.pt\\\n",
    "https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt\n",
    "\n",
    "3) train용 이미지 다운로드\n",
    "!mkdir /content/datasets; mkdir /content/datasets/street\n",
    "!mkdir /content/datasets/street/images; mkdir /content/datasets/street/images/train\n",
    "!mkdir /content/datasets/street/labels; mkdir /content/datasets/street/labels/train\n",
    "\n",
    "!wget -O /content/street_images1.zip https://github.com/DrKAI/CV/raw/main/street_images1.zip\n",
    "!wget -O /content/street_images2.zip https://github.com/DrKAI/CV/raw/main/street_images2.zip\n",
    "!wget -O /content/street_images3.zip https://github.com/DrKAI/CV/raw/main/street_images3.zip\n",
    "\n",
    "!wget -O /content/street_labels1.zip https://github.com/DrKAI/CV/raw/main/street_labels1.zip\n",
    "!wget -O /content/street_labels2.zip https://github.com/DrKAI/CV/raw/main/street_labels2.zip\n",
    "!wget -O /content/street_labels3.zip https://github.com/DrKAI/CV/raw/main/street_labels3.zip\n",
    "    \n",
    "!unzip /content/street_images1.zip -d /content/datasets/street/images/train\n",
    "!unzip /content/street_images2.zip -d /content/datasets/street/images/train\n",
    "!unzip /content/street_images3.zip -d /content/datasets/street/images/train\n",
    "\n",
    "!unzip /content/street_labels1.zip -d /content/datasets/street/labels/train\n",
    "!unzip /content/street_labels2.zip -d /content/datasets/street/labels/train\n",
    "!unzip /content/street_labels3.zip -d /content/datasets/street/labels/train\n",
    "\n",
    "4) train.py 실행\n",
    "!cd yolov5; python train.py -h\n",
    "!cd yolov5; python train.py \\\n",
    "    --data '/content/yolov5/data/street.yaml' \\\n",
    "    --cfg '/content/yolov5/models/yolov5s.yaml' \\\n",
    "    --weights '/content/yolov5/pretrained/yolov5s.pt' \\\n",
    "    --epochs 1000 \\\n",
    "    --patience 7 \\\n",
    "    --img 640 \\\n",
    "    --project 'trained' \\\n",
    "    --name 'train_street' \\\n",
    "    --exist-ok\n",
    "    # --device cpu\n",
    "\n",
    "5) detect용 이미지 다운로드\n",
    "!wget -O /content/yolov5/data/images/street01.jpeg https://github.com/DrKAI/image/raw/main/001.jpeg\n",
    "!wget -O /content/yolov5/data/images/street02.jpg https://github.com/DrKAI/image/raw/main/14th_Street_2005.jpg\n",
    "!wget -O /content/yolov5/data/images/street03.jpg https://github.com/DrKAI/image/raw/main/street02.jpg\n",
    "!wget -O /content/yolov5/data/images/street04.jpg https://github.com/DrKAI/image/raw/main/street03.jpg\n",
    "!wget -O /content/yolov5/data/images/street05.jpg https://github.com/DrKAI/image/raw/main/street04.jpg\n",
    "!wget -O /content/yolov5/data/images/street06.jpg https://github.com/DrKAI/image/raw/main/street05.jpg\n",
    "\n",
    "\n",
    "6) detect.py 실행\n",
    "!cd yolov5; python detect.py -h\n",
    "\n",
    "!cd yolov5; python detect.py \\\n",
    "    --weights '/content/yolov5/trained/train_street/weights/best.pt' \\\n",
    "    --source '/content/yolov5/data/images/' \\\n",
    "    --project '/content/yolov5/detected' \\\n",
    "    --name 'images' \\\n",
    "    --img 640 \\\n",
    "    --conf-thres 0.25 \\\n",
    "    --iou-thres 0.5 \\\n",
    "    --line-thickness 2 \\\n",
    "    --exist-ok \n",
    "    # --device CPU\n",
    "\n",
    "7) Detect Image 살펴보기\n",
    "from IPython.display import Image\n",
    "from google.colab import files\n",
    "\n",
    "Image(filename='/content/yolov5/detected/images/street01.jpeg', width=640)\n",
    "\n",
    "## colab은 멀티 다운로드를 지원하지 않는다\n",
    "## 폴더를 압축하여 파일 하나로 만들고 다운로드 한다\n",
    "\n",
    "!zip -r /content/detected_images.zip /content/yolov5/detected/images\n",
    "\n",
    "files.download(filename='/content/detected_images.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0706d8f3-326a-4946-891b-a470b77cfac7",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57fddd-9cc4-40fe-b90a-47dd69c00c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests 이용\n",
    "받아오는 문자열에 따라 두가지 방법으로 구분\n",
    "json 문자열로 받아서 파싱하는 방법 : 주로 동적 페이지 크롤링할때 사용\n",
    "html 문자열로 받아서 파싱하는 방법 : 주로 정적 페이지 크롤링할때 사용\n",
    "selenium 이용\n",
    "브라우져를 직접 열어서 데이터를 받는 방법\n",
    "크롤링 방법에 따른 속도\n",
    "requests json > requests html > selenium\n",
    "# summary\n",
    "# web : server-client : url\n",
    "# request, response : get,post\n",
    "# 웹서비스의 구조\n",
    "# 웹페이지의 종류\n",
    "# - 동적페이지 : URL 변경 X > 데이터 수정 : JSON : API\n",
    "# - 정적페이지 : URL 변경 O > 데이터 수정 : HTML : css selector > BeautifulSoup : select(), select_one()\n",
    "\n",
    "\n",
    "# html : 웹페이지에서 레이아웃, 텍스트 등의 데이터를 작성\n",
    "# 구성요소: document, element, tag, attr, text\n",
    "# element 계층적 구조\n",
    "# tag 종류 : p, span, ul, li, table, a, img, iframe, div\n",
    "\n",
    "# css selector : html의 element에 style을 적용시킬때 element를 선택하는 방법\n",
    "# element 선택 : tag(span), class(.), id(#), attr([value=\"no1\"])\n",
    "# n번째 element 선택 : .py:nth-child(2) : 2번째 엘리먼트중에 클래스가 py인 엘리먼트\n",
    "# 계층적 element 선택 : 모든 하위 엘리먼트 선택(.wrap p), 한단계 하위 엘리먼트 선택(.wrap > p), 여러개 선택(.no1, .no2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49620ab6-88c5-4ddc-8858-69529f3ea12a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Selenium\n",
    "- 브라우져의 자동화 목적으로 만들어진 다양한 브라우져와 언어를 지원하는 라이브러리\n",
    "- 브라우져를 파이썬 코드로 컨트롤 해서 브라우져에 있는 데이터를 수집\n",
    "\n",
    "#### 크롤링 방법\n",
    "- 1. requests : json : 웹페이지의 API 트래픽을 분석해서 데이터 수집 : naver stock\n",
    "- 2. requests : json : 공식적으로 제공하는 API를 application key 받아서 데이터 수집 : naver api(papago, trend)\n",
    "- 3. requests : html, BeautifulSoup(css selector) : 웹페이지의 html 코드 받아서 데이터 수집 : gmarket\n",
    "- 4. selenium : browser - python : 브라우져를 파이썬 코드로 컨트롤 해서 데이터 수집 : ted\n",
    "- 크롤링할때 좋은 순서 : 2 > 1 > 3 > 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6973869f-8ad4-408b-86f5-05fb2098994b",
   "metadata": {},
   "source": [
    "### 동적페이지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0ee8b-075f-48c3-b624-7f1eb7f9f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동적 페이지 데이터 수집 프로세스\n",
    "# 1. 웹서비스 분석(개발자도구) : URL\n",
    "# 2. request(url, params, header) > response(json) : JSON(str)\n",
    "# 3. JSON(str) > list, dict > DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be0f089-4c10-41a8-8129-205f475a03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "1) pc 웹페이지가 복잡하면 mobile 웹페이지에서 수집\n",
    "2) 서버에 데이터 요청 : request(url) > response : json(str)\n",
    "- response의 status code가 200이 나오는지 확인\n",
    "- 403이나 500이 나오면 request가 잘못되거나 web server에서 수집이 안되도록 설정이 된것임\n",
    "  -> header 설정 또는 selenium 사용\n",
    "- 200이 나오더라도 response 안에 있는 내용을 확인 > 확인하는 방법 : response.text[:200]\n",
    "3) 서버에서 받은 데이터 파싱(데이터 형태를 변경) : json(str) > list, dict > DataFrame\n",
    "4) 함수로 만들기\n",
    "\n",
    "def stock_price(pagesize, page, code=\"KOSPI\"):\n",
    "    \"\"\"This function is crawling stock price form naver webpage.\n",
    "    Params\n",
    "    ------\n",
    "    pagesize : int : one page size\n",
    "    page : int : page number\n",
    "    code : str : KOSPI or KOSDAQ\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    type : DataFrame : display date, price columns\n",
    "    \"\"\"\n",
    "    url = f\"https://m.stock.naver.com/api/index/{code}/price?pageSize={pagesize}&page={page}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return pd.DataFrame(data)[[\"localTradedAt\",\"closePrice\"]]\n",
    "\n",
    "kospi = stock_price(60,1,\"KOSPI\")\n",
    "kosdaq = stock_price(60,1,\"KOSDAQ\")\n",
    "\n",
    "# 데이터 전처리\n",
    "df = kospi.copy()\n",
    "df[\"kosdaq\"]=kosdaq[\"closePrice\"]\n",
    "df[\"usd\"] = usd['closePrice']\n",
    "df = df.rename(columns={\"closePrice\":\"kospi\"})\n",
    "df\n",
    "\n",
    "# docstring[\"\"\"\"\"\"] : 함수를 사용하는 방법을 문자열로 작성 \n",
    "#help()이용, Shift + tab\n",
    "help(stock_price)\n",
    "\n",
    "df = pd.DataFrame([{'age': 23},{'age': 36},{'age': 27}])  # 데이터프레임 만들기\n",
    "df\n",
    "\n",
    "@ 시각화\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 데이터 수집\n",
    "page_size = 60\n",
    "kospi_df = stock_price(\"KOSPI\", page_size=page_size)\n",
    "kosdaq_df = stock_price(\"KOSDAQ\", page_size=page_size)\n",
    "usd_df = exchage_rate(\"FX_USDKRW\", page_size=page_size)\n",
    "eur_df = exchage_rate(\"FX_EURKRW\", page_size=page_size)\n",
    "\n",
    "# 데이터 전처리 1 : 데이터 타입 변경\n",
    "print(kospi_df.dtypes)\n",
    "kospi_df[\"kospi\"] = kospi_df[\"closePrice\"].apply(lambda data: float(data.replace(\",\", \"\")))\n",
    "kospi_df = kospi_df.drop(columns=[\"closePrice\"])\n",
    "print(kospi_df.dtypes)\n",
    "\n",
    "kosdaq_df[\"kosdaq\"] = kosdaq_df[\"closePrice\"].apply(lambda data: float(data.replace(\",\", \"\")))\n",
    "usd_df[\"usd\"] = usd_df[\"closePrice\"].apply(lambda data: float(data.replace(\",\", \"\")))\n",
    "eur_df[\"eur\"] = eur_df[\"closePrice\"].apply(lambda data: float(data.replace(\",\", \"\")))\n",
    "\n",
    "kosdaq_df = kosdaq_df.drop(columns=[\"closePrice\"])\n",
    "usd_df = usd_df.drop(columns=[\"closePrice\"])\n",
    "eur_df = eur_df.drop(columns=[\"closePrice\"])\n",
    "\n",
    "# 데이터 전처리 2 : 날짜 데이터 맞추기 : merge\n",
    "merge_df_1 = pd.merge(kospi_df, kosdaq_df, on=\"localTradedAt\")\n",
    "merge_df_2 = pd.merge(merge_df_1, usd_df, on=\"localTradedAt\")\n",
    "merge_df_3 = pd.merge(merge_df_2, eur_df, on=\"localTradedAt\")\n",
    "merge_df = merge_df_3.copy()\n",
    "merge_df.tail(2)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(20, 5))\n",
    "columns = merge_df.columns[1:]\n",
    "for column in columns:\n",
    "    plt.plot(merge_df[\"localTradedAt\"], merge_df[column], label=column)\n",
    "\n",
    "xticks_count = 11\n",
    "plt.xticks(merge_df[\"localTradedAt\"][::int(len(merge_df) // xticks_count) + 1])\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "\n",
    "corr_df = merge_df[merge_df.columns[1:]].corr()\n",
    "corr_df\n",
    "\n",
    "# 결정계수 : r-squared \n",
    "# 1과 가까울수록 강한 관계, 0과 가까울수록 약한 관계\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.heatmap(corr_df**2, cmap=\"YlGnBu\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f92a181-e776-4208-91a2-032e56cf19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼의 데이터 타입 변경 : str > float\n",
    "# df[column].apply() : 모든 데이터를 함수에 대입한 결과를 출력\n",
    "# apply(func) : 모든 데이터를 func을 적용시킨 결과 출력\n",
    "df.dtypes\n",
    "\n",
    "# 피어슨 상관계수 : df.corr()\n",
    "# 1과 가까울수록 강한 양의 상관관계를 갖는다.\n",
    "# -1과 가까울수록 강한 음의 상관관계를 갖는다.\n",
    "# 0과 가까울수록 관계가 없다.\n",
    "df[['kospi','kosdaq','usd']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b41e3-a071-43ec-9c16-9f4c6503ddc6",
   "metadata": {},
   "source": [
    "### 카카오 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedbb599-8ffc-487d-8cec-629122da977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "REST_API_KEY = '9a2d4ed549676750779492f1f9f89956'\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://dapi.kakao.com/v2/local/geo/coord2regioncode.json\"\n",
    "#params = {query': '}\n",
    "params = {'x' : '127.177375','y' : '37.240666'}\n",
    "headers = {'Authorization': f'KakaoAK {REST_API_KEY}'}\n",
    "\n",
    "\n",
    "\n",
    "response = requests.get(url, params = params, headers=headers)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58329ff-519b-4c70-bfbd-262752375141",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b48f4b-7dc6-485e-82b1-45e3e360447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "application programing interface\n",
    "api 를 사용해서 데이터를 수집하는것은 서비스에 데이터를 제공한는 공식적인 방법으로 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1f72e-7afc-42ce-a3ec-d487dbdb901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API를 이용한 데이터 수집\n",
    "# 1. APP 등록 : application key \n",
    "# 2. API 문서 : URL\n",
    "# 3. request(url, params, header(application key)) > response(json) : JSON(str)\n",
    "# 4. JSON(str) > list, dict > DataFrame or Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c258821c-0a5d-4f7f-88f2-523a3a5139de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d68d416-17b4-40ce-9268-a47074cd698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID, CLIENT_SECRET = \"KE2M4YcO4Sv9P7HrFXJ1\", \"9X0zM51ZRf\"\n",
    "\n",
    "# json.dumps() : 인터넷 트래픽에서는 영문, 숫자, 특수문자만 사용가능\n",
    "# 한글과 같은 문자를 인코딩(영문,숫자,특수문자)\n",
    "txt= \"파이썬은 재미있습니다.\"\n",
    "url = \"https://openapi.naver.com/v1/papago/n2mt\" \n",
    "params = {\"source\": \"ko\", \"target\": \"en\", \"text\": txt}\n",
    "headers = {\"Content-Type\": \"application/json\", \n",
    "           \"X-Naver-Client-Id\": CLIENT_ID,\n",
    "           \"X-Naver-Client-Secret\": CLIENT_SECRET}\n",
    "\n",
    "response = requests.post(url, json.dumps(params),headers=headers)\n",
    "response\n",
    "\n",
    "response.text\n",
    "\n",
    "txt_en = response.json()[\"message\"][\"result\"][\"translatedText\"]\n",
    "txt_en\n",
    "\n",
    "# 이렇게 가져올수 있다[다음 환율]\n",
    "datas = response.json()[\"data\"]\n",
    "df = pd.DataFrame(datas)\n",
    "df.head(1)\n",
    "\n",
    "def translate(txt):\n",
    "    CLIENT_ID, CLIENT_SECRET = \"KE2M4YcO4Sv9P7HrFXJ1\",\"9X0zM51ZRf\" \n",
    "    url = \"https://openapi.naver.com/v1/papago/n2mt\"\n",
    "    params = {\"source\": \"ko\", \"target\": \"en\", \"text\": txt}\n",
    "    headers = {\"Content-Type\": \"application/json\", \n",
    "           \"X-Naver-Client-Id\": CLIENT_ID,\n",
    "           \"X-Naver-Client-Secret\": CLIENT_SECRET}\n",
    "    response = requests.post(url, json.dumps(params), headers=headers)\n",
    "    txt_en = response.json()[\"message\"][\"result\"][\"translatedText\"]\n",
    "    return txt_en\n",
    "\n",
    "txt= \"웹크롤링은 재미있습니다.\"\n",
    "txt_en = translate(txt)\n",
    "txt_en\n",
    "\n",
    "@ 한글 excel 파일을 영문 excel 파일로 변경\n",
    "\n",
    "%ls\n",
    "\n",
    "covid = pd.read_excel(\"covid.xlsx\")[[\"category\",\"title\"]]\n",
    "covid.tail(2)\n",
    "\n",
    "covid_en = covid[\"title\"].apply(translate)\n",
    "\n",
    "covid[\"title_en\"] = covid_en\n",
    "covid\n",
    "\n",
    "# utf-8-sig : excel에서 사용하는 인코딩 방식과 호환이 되는 utf-8인 인코딩 방식 \n",
    "covid.to_excel(\"covid_en.xlsx\", index = False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d8888-7d77-4ea1-9b61-173c3796e550",
   "metadata": {},
   "source": [
    "### 통합검색어 트렌드 api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ee8a0-4758-428d-a301-f2dbe2437d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. URL\n",
    "url = \"https://openapi.naver.com/v1/datalab/search\"\n",
    "\n",
    "# 2. request > response\n",
    "params = {\n",
    "    \"startDate\": \"2018-01-01\",\n",
    "    \"endDate\": \"2022-01-31\",\n",
    "    \"timeUnit\": \"month\",\n",
    "    \"keywordGroups\": [\n",
    "        {\"groupName\": \"트위터\", \"keywords\": [\"트위터\", \"트윗\"]},\n",
    "        {\"groupName\": \"페이스북\", \"keywords\": [\"페이스북\", \"페북\"]},\n",
    "        {\"groupName\": \"인스타그램\", \"keywords\": [\"인스타그램\", \"인스타\"]},\n",
    "    ]\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"X-Naver-Client-Id\": CLIENT_ID,\n",
    "    \"X-Naver-Client-Secret\": CLIENT_SECRET,    \n",
    "}\n",
    "\n",
    "response = requests.post(url, data=json.dumps(params), headers=headers)\n",
    "response\n",
    "\n",
    "# 3. parsing\n",
    "datas = response.json()[\"results\"]\n",
    "\n",
    "# 4. preprocessing\n",
    "result_df = pd.concat(dfs, ignore_index=True)\n",
    "result_df.tail(2)\n",
    "\n",
    "pivot_df = result_df.pivot(\"period\", \"title\", \"ratio\")\n",
    "pivot_df.columns = [\"instagram\", \"twitter\", \"facebook\"]\n",
    "pivot_df.tail(2)\n",
    "\n",
    "# 5. visualization\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pivot_df.plot(figsize=(20, 5))\n",
    "plt.legend(loc=0)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8271533-a62e-415f-9a5b-7ac6fd1aa285",
   "metadata": {},
   "source": [
    "### 위도 경도로 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a584b0-0446-4bd9-a872-034d0a336b16",
   "metadata": {},
   "source": [
    "- 절차\n",
    "    - 동이름으로 위도 경도 구하기\n",
    "    - 위도 경도로 geohash 알아내기\n",
    "    - geohash로 매물 아이디 가져오기\n",
    "    - 매물 아이디로 매물 정보 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784bbc48-ffad-4cfd-9dd9-94e3a2c8a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 동이름으로 위도 경도 구하기\n",
    "addr = \"망원동\"\n",
    "url = f\"https://apis.zigbang.com/v2/search?leaseYn=N&q={addr}&serviceType=원룸\"\n",
    "response = requests.get(url)\n",
    "data = response.json()[\"items\"][0]\n",
    "lat, lng = data[\"lat\"], data[\"lng\"]\n",
    "lat, lng\n",
    "\n",
    "# 2. 위도 경도로 geohash 알아내기\n",
    "# install geohash2\n",
    "# !pip install geohash2\n",
    "import geohash2\n",
    "\n",
    "# precision이 커질수록 영역이 작아짐\n",
    "geohash = geohash2.encode(lat, lng, precision=5)\n",
    "geohash\n",
    "\n",
    "# 3. geohash로 매물 아이디 가져오기\n",
    "url = f\"https://apis.zigbang.com/v2/items?deposit_gteq=0&domain=zigbang\\\n",
    "&geohash={geohash}&needHasNoFiltered=true&rent_gteq=0&sales_type_in=전세|월세\\\n",
    "&service_type_eq=원룸\"\n",
    "response = requests.get(url)\n",
    "datas = response.json()[\"items\"]\n",
    "# len(datas), datas[0]\n",
    "ids = [data[\"item_id\"] for data in datas]\n",
    "len(ids), ids[:5]\n",
    "\n",
    "# 4. 매물 아이디로 매물 정보 가져오기\n",
    "# 1000개 넘어가면 나눠서 수집해야 함\n",
    "url = \"https://apis.zigbang.com/v2/items/list\"\n",
    "params = {\n",
    "    \"domain\": \"zigbang\",\n",
    "    \"withCoalition\": \"true\",\n",
    "    \"item_ids\": ids\n",
    "}\n",
    "response = requests.post(url, params)\n",
    "response\n",
    "\n",
    "datas = response.json()[\"items\"]\n",
    "df = pd.DataFrame(datas)\n",
    "df.tail(2)\n",
    "\n",
    "# 필요한 컬럼만 필터링\n",
    "columns = [\"item_id\", \"sales_type\", \"deposit\", \"rent\", \"size_m2\", \"floor\", \"building_floor\",\n",
    "           \"address1\", \"manage_cost\"]\n",
    "filtered_column_df = df[columns]\n",
    "filtered_column_df.tail(2)\n",
    "\n",
    "# 주소에 망원동이 있는 데이터만 필터링\n",
    "result_df = filtered_column_df[filtered_column_df[\"address1\"].str.contains(\"망원동\")]\n",
    "result_df = result_df.reset_index(drop=True)\n",
    "result_df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f877474-7e76-4bf1-aa09-127db92a7c4b",
   "metadata": {},
   "source": [
    "### 이미지 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b357db-f6fb-4011-adf9-5c7f01eb8734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests, os\n",
    "\n",
    "# 1. 디렉토리 생성 : data\n",
    "path = \"data\"\n",
    "if not os.path.exists(\"data\"): # 디렉토리 존재 유무 확인\n",
    "    os.makedirs(path)\n",
    "    \n",
    "%ls\n",
    "# 2. csv 파일을 로드 : image link\n",
    "df = pd.read_csv(\"gmarket.csv\")\n",
    "df.tail(2)\n",
    "\n",
    "img_link = df.loc[0, 'img']\n",
    "img_link\n",
    "\n",
    "# 3. download images : requests\n",
    "response = requests.get(img_link)\n",
    "response\n",
    "\n",
    "with open(f'{path}/test.png', \"wb\") as file: # 파일저장 wb, 읽기는 rd\n",
    "    file.write(response.content)\n",
    "    \n",
    "%ls data\n",
    "\n",
    "# 4. display image : pillow\n",
    "from PIL import Image as pil\n",
    "pil.open(f'{path}/test.png') \n",
    "\n",
    "# 5. 여러개의 이미지 다운로드\n",
    "df[:3]\n",
    "for idx, data in df[:5].iterrows():\n",
    "    filename = '0' * (3 - len(str(idx))) + str(idx) + '.png'\n",
    "    print(idx, filename, data['img'])\n",
    "    response = requests.get(data['img'])\n",
    "    with open(f'{path}/{filename}', \"wb\") as file: # 파일저장 wb, 읽기는 rd\n",
    "        file.write(response.content)\n",
    "\n",
    "pil.open(f'{path}/004.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ed5d2-a022-4cd4-ba4a-2c5291398087",
   "metadata": {},
   "source": [
    "## 언어지능 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65aa6aa-c623-4d87-9823-048c4ec57f27",
   "metadata": {},
   "source": [
    "### MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f23eb8-7d2a-4838-9529-e3894a6db0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#<MeCab = 형태소 분석기>\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# MeCab 설치하기\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "    \n",
    "!pwd\n",
    "%cd Mecab-ko-for-Google-Colab\n",
    "\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh \n",
    "\n",
    "# 실행 테스트\n",
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "\n",
    "sentence = '코로나 감염 뒤 4주 이상 후유증이 이어지는 현상을 롱코비드라고 부른다.'\n",
    "print (tagger.parse(sentence))\n",
    "\n",
    "# 설치 후 Tagger 에러가 발생한 경우\n",
    "# https://github.com/konlpy/konlpy/issues/144\n",
    "#!git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git\n",
    "#import os\n",
    "#os.chdir('mecab-python-0.996')\n",
    "#!python setup.py build\n",
    "#!python setup.py install\n",
    "\n",
    "!pwd\n",
    "\n",
    "@ MeCab 사용자 사전 추가하기\n",
    "\n",
    "# Mecab의 사전 디렉토리\n",
    "%cd /content/mecab-ko-dic-2.1.1-20180720/\n",
    "ls\n",
    "\n",
    "!ls user-dic/\n",
    "\n",
    "# 사용자 사전에 미리 등록되어있는 고유명사\n",
    "!cat user-dic/nnp.csv\n",
    "\n",
    "1.Colab 안에서 직접 단어 추가하기\n",
    "# 사전에 등록할 단어 추가\n",
    "!echo \"코로나,,,,NNP,*,T,코로나,*,*,*,*,*\" >> user-dic/nnp.csv\n",
    "!cat user-dic/nnp.csv\n",
    "\n",
    "mv user-dic/nnp.csv /content/gdrive/'My Drive'/'aivle'\n",
    "\n",
    "2.텍스트 파일로 로컬에서 수정 후 추가하기\n",
    "# nnp.csv 파일 이동\n",
    "!mv /content/mecab-ko-dic-2.1.1-20180720/user-dic/nnp.csv /content/gdrive/'My Drive'/'aivle'\n",
    "\n",
    "로컬에서 nnp.csv 파일에 단어를 추가한 후에 구글드라이브에 업로드\n",
    "# 로컬에서 파일 수정 후 재 업로드\n",
    "#!cat /content/gdrive/'My Drive'/'Colab Notebooks'/nng.csv\n",
    "#!cat /content/gdrive/'My Drive'/'Colab Notebooks'/nnp.csv\n",
    "!cat /content/gdrive/'My Drive'/'Colab Notebooks'/aivle/DAY1/nng.csv\n",
    "!cat /content/gdrive/'My Drive'/'Colab Notebooks'/aivle/DAY1/nnp.csv\n",
    "\n",
    "sentence = '토트넘이 리그 4위로 21-22 프리미어리그 시즌을 마무리해서 UCL에 진출했다.'\n",
    "print (tagger.parse(sentence))\n",
    "\n",
    "# 업로드된 수정 파일 이동\n",
    "!mv /content/gdrive/'My Drive'/'Colab Notebooks'/aivle/DAY1/nng.csv /content/mecab-ko-dic-2.1.1-20180720/user-dic/\n",
    "!mv /content/gdrive/'My Drive'/'Colab Notebooks'/aivle/DAY1/nnp.csv /content/mecab-ko-dic-2.1.1-20180720/user-dic/\n",
    "!ls user-dic/\n",
    "\n",
    "!cat /content/mecab-ko-dic-2.1.1-20180720/user-dic/nnp.csv\n",
    "!cat /content/mecab-ko-dic-2.1.1-20180720/user-dic/nng.csv\n",
    "\n",
    "# 사용자사전 업데이트\n",
    "!bash ./tools/add-userdic.sh\n",
    "\n",
    "# 사전 리빌드\n",
    "!sudo make install\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "# 사용자 사전 업데이트 확인\n",
    "sentence = '코로나 감염 뒤 4주 이상 후유증이 이어지는 현상을 롱코비드라고 부른다.'\n",
    "print (tagger.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdeefec-81da-4f24-a235-01eadb8a90f8",
   "metadata": {},
   "source": [
    "### word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12574e-2655-46fa-99f8-66c8f0241808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab 환경에 Nanum 폰트를 설치\n",
    "!sudo apt-get install -y fonts-nanum\n",
    "# scans the font directories and build font cache\n",
    "!sudo fc-cache -fv\n",
    "# matplotlib의 font cache를 clear\n",
    "!rm ~/.cache/matplotlib -rf\n",
    "# 실행 후 런타임을 다시 시작하세요\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "!ls /content/gdrive/'My Drive'/aivle/data/\n",
    "\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "    \n",
    "#!cd Mecab-ko-for-Google-Colab\n",
    "!pwd\n",
    "%cd Mecab-ko-for-Google-Colab\n",
    "\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh\n",
    "\n",
    "import MeCab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "with open('/content/gdrive/My Drive/aivle/data/extreme_job_review.txt','r',encoding='utf-8') as f:\n",
    "    my_sentence=f.read()\n",
    "my_sentence\n",
    "\n",
    "print(tagger.parse(my_sentence))\n",
    "\n",
    "# Mecab 형태소분석 결과에서 단어부분(스트링)와 품사태그 부분을 분리해서 반환해주는 함수\n",
    "# 예) 영화배우/NNG -> ('영화배우', 'NNG')\n",
    "def mecabsplit(mecab_tagger,inputs, pos):\n",
    "    r=[]\n",
    "    inputs = mecab_tagger.parse(inputs)\n",
    "    t = inputs.split('\\n')[:-2]\n",
    "    for i in t:\n",
    "        field = i.split('\\t')\n",
    "        if field[1].split(',')[-1] is not '*':\n",
    "            r.extend( [ (x.split('/')[0],x.split('/')[1]) for x in field[1].split(',')[-1].split('+') ] )\n",
    "        else:\n",
    "            r.append( (field[0],field[1].split(',')[0]) )\n",
    "    if pos:\n",
    "        return r\n",
    "    else:\n",
    "        return [ x[0] for x in r ]\n",
    "    \n",
    "# tag가 NNG(일반 명사)인 단어 (토큰, 형태소) 리스트\n",
    "nng = [word for word, tag in mecabsplit(tagger, my_sentence, True) if tag=='NNG']\n",
    "print(nng)\n",
    "\n",
    "# tag가 NNP(고유 명사)인 단어 (토큰, 형태소) 리스트\n",
    "nnp = [word for word, tag in mecabsplit(tagger, my_sentence, True) if tag=='NNP']\n",
    "print(nnp)\n",
    "\n",
    "from collections import Counter,OrderedDict\n",
    "\n",
    "# matplotlib의 폰트를 Nanum 폰트로 지정\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "\n",
    "count_list=Counter(nng)\n",
    "print(len(count_list))\n",
    "\n",
    "# counter안의 value를 sort해줌\n",
    "sorted_list=count_list.most_common(20)\n",
    "# 다시 dictionary 형태로 변환\n",
    "sorted_list=OrderedDict(sorted_list)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.xticks(rotation=90)\n",
    "plt.plot(list(sorted_list.keys()), list(sorted_list.values()))\n",
    "plt.title(\"일반 명사 빈도수\")\n",
    "plt.ylabel(\"# of words\")\n",
    "plt.xlabel(\"word\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd05d9fe-4069-488f-bb65-4da18b7d81aa",
   "metadata": {},
   "source": [
    "### sentiment_BoW[Naive Bayes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da065920-2153-4f2a-b67f-832c0dd374ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeCab 설치\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "    \n",
    "%cd Mecab-ko-for-Google-Colab/\n",
    "\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh \n",
    "\n",
    "#colab 을 이용한 실행시\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# Mecab 형태소분석 결과에서 단어부분(스트링)와 품사태그 부분을 분리해서 반환해주는 함수\n",
    "# 예) 영화배우/NNG -> ('영화배우', 'NNG')\n",
    "def mecabsplit(mecab_tagger,inputs, pos):\n",
    "    r=[]\n",
    "    inputs = mecab_tagger.parse(inputs)\n",
    "    t = inputs.split('\\n')[:-2]\n",
    "    for i in t:\n",
    "        field = i.split('\\t')\n",
    "        if field[1].split(',')[-1] is not '*':\n",
    "            r.extend( [ (x.split('/')[0],x.split('/')[1]) for x in field[1].split(',')[-1].split('+') ] )\n",
    "        else:\n",
    "            r.append( (field[0],field[1].split(',')[0]) )\n",
    "    if pos:\n",
    "        return r\n",
    "    else:\n",
    "        return [ x[0] for x in r ]\n",
    "    \n",
    "# utf-8 인코딩으로 된 한글 파일을 읽어들임\n",
    "import codecs\n",
    "def read_data(filename):\n",
    "    with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]\n",
    "    return data\n",
    "\n",
    "# 데이터 파일 위치 확인하세요\n",
    "train_data = read_data('/content/gdrive/My Drive/aivle/data/nsm/small_ratings_train.txt')\n",
    "test_data = read_data('/content/gdrive/My Drive/aivle/data/nsm/small_ratings_test.txt')\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print (train_data[0])\n",
    "print (test_data[0])\n",
    "\n",
    "\n",
    "%%time\n",
    "# 형태소 분석기 Mecab import\n",
    "import MeCab\n",
    "tagger = MeCab.Tagger()\n",
    "\n",
    "# Mecab의 출력 스트링을 token+space+token+space ... 포맷으로 변환해줌 (pos=False일 때)\n",
    "def tokenize(doc):\n",
    "    return ' '.join(mecabsplit(tagger, doc, False))\n",
    "\n",
    "# train data의 첫번째 줄\n",
    "print (train_data[0])\n",
    "# test data의 첫번째 줄\n",
    "print (test_data[0])\n",
    "\n",
    "# train data의 document 부분\n",
    "train_docs_X = [tokenize(row[1]) for row in train_data]\n",
    "# train data의 label 부분\n",
    "train_Y = [row[2] for row in train_data]\n",
    "\n",
    "test_docs_X = [tokenize(row[1]) for row in test_data]\n",
    "test_Y = [row[2] for row in test_data]\n",
    "\n",
    "# train data 확인\n",
    "print(train_docs_X[0])\n",
    "print(train_Y[0])\n",
    "print(train_docs_X[1])\n",
    "print(train_Y[1])\n",
    "print('\\n')\n",
    "# test data 확인\n",
    "print(test_docs_X[0])\n",
    "print(test_Y[0])\n",
    "print(test_docs_X[1])\n",
    "print(test_Y[1])\n",
    "print('\\n')\n",
    "\n",
    "@ CountVectorizer 변환\n",
    "데이터 : ['I am happy', 'I am sad']\n",
    "단어 모음 : ['I', 'am', 'happy', 'sad']\n",
    "변환 결과 : [[1, 1, 1, 0], [1, 1, 0, 1]]\n",
    "    \n",
    "# CountVectorizer 기능: \n",
    "# 1. 문서를 토큰 리스트로 변환한다\n",
    "# 2. 각 문서에서 토큰의 빈도를 센다\n",
    "# 3. 각 문서를 BoW 벡터로 변환한다.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# max_features : 전체 문서에서 빈도수 높은 순으로 top max_features 갯수의 단어만 포함하여\n",
    "# fit : vocabulary를 생성한다\n",
    "vec = CountVectorizer(max_features = 1000).fit(train_docs_X)\n",
    "\n",
    "# 학습데이터(train_docs_X), 테스트데이터(test_docs_X)를 BoW 벡터로 변환\n",
    "train_X = vec.transform(train_docs_X).toarray()\n",
    "test_X = vec.transform(test_docs_X).toarray()\n",
    "\n",
    "# 첫번째 문서(리뷰 문장)의 벡터\n",
    "print(train_X[0])\n",
    "# 학습데이터 문서(리뷰 문장) 갯수\n",
    "print(len(train_X))\n",
    "\n",
    "\n",
    "@ Naive Bayes 모델 이용하기\n",
    "%%time\n",
    "\n",
    "# sklearn 팩키지에서 GaussianNB 모델을 import\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# GaussianNB 객체를 생성\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# train_X: 학습데이터, train_Y: 정답레이블을 받아서 학습\n",
    "gnb.fit(train_X, train_Y)\n",
    "\n",
    "# score(test_data, true_labels) returns the mean accuracy on the given test data and labels.\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gnb.score(train_X, train_Y)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gnb.score(test_X, test_Y)))\n",
    "\n",
    "\n",
    "pos = [ [test_docs_X[i], test_Y[i], gnb.predict([test_X[i]]) ] for i in range(len(test_Y)) if test_Y[i] == '1' ]\n",
    "neg = [ [test_docs_X[i], test_Y[i], gnb.predict([test_X[i]]) ] for i in range(len(test_Y)) if test_Y[i] == '0' ]\n",
    "#crr= []\n",
    "#for i in range(len(test_Y)):\n",
    "#    if test_Y[i] == '1':\n",
    "#        crr.append([test_docs_X[i], test_Y[i], gnb_predict([test_X[i]]) ])\n",
    "#print (crr)\n",
    "print ('>긍정 리뷰에 대한 예측:')\n",
    "for i in pos[:5]:\n",
    "    print ('입력:', i[0])\n",
    "    print ('정답:', i[1])\n",
    "    print ('예측:', i[2])\n",
    "print ('\\n>부정 리뷰에 대한 예측:')\n",
    "for i in neg[:5]:\n",
    "    print ('입력:', i[0])\n",
    "    print ('정답:', i[1])\n",
    "    print ('예측:', i[2])\n",
    "\n",
    "    \n",
    "@  KNN 모델 이용하기\n",
    "# (연습 1) KNN으로 긍부정 분류를 하세요. (sklearn 라이브러리에서 제공하는 KNeighborsClassifier를 사용)\n",
    "# write code here\n",
    "# \n",
    "#\n",
    "#\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(train_X, train_Y)\n",
    "\n",
    "# (연습 2) score 함수를 사용하여 학습데이터의 정확도, 테스트데이터의 정확도를 출력하세요.\n",
    "# 소수점 이하 3자리까지 출력하세요.\n",
    "#\n",
    "#\n",
    "print(\"Accuracy on training set: {:.3f}\".format(neigh.score(train_X, train_Y)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(neigh.score(test_X, test_Y)))\n",
    "\n",
    "# KNN으로 예측한 결과를 확인하기\n",
    "pos = [ [test_docs_X[i], test_Y[i], neigh.predict([test_X[i]]) ] for i in range(len(test_Y)) if test_Y[i] == '1' ]\n",
    "neg = [ [test_docs_X[i], test_Y[i], neigh.predict([test_X[i]]) ] for i in range(len(test_Y)) if test_Y[i] == '0' ]\n",
    "\n",
    "print ('>긍정 리뷰에 대한 예측:')\n",
    "for i in pos[:5]:\n",
    "    print ('\\n입력:', i[0])\n",
    "    print ('정답:', i[1])\n",
    "    print ('예측:', i[2])\n",
    "print ('\\n>부정 리뷰에 대한 예측:')\n",
    "for i in neg[:5]:\n",
    "    print ('\\n입력:', i[0])\n",
    "    print ('정답:', i[1])\n",
    "    print ('예측:', i[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982313d8-02c0-4511-ab77-b88f813fd10b",
   "metadata": {},
   "source": [
    "### sentiment_vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec1f13-82ec-41ac-8696-60b104be3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# 사전 기반 감성분석 툴 vaderSentiment 설치\n",
    "!pip install vaderSentiment\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(score)))\n",
    "    \n",
    "sentiment_analyzer_scores(\"The phone is super cool.\")\n",
    "\n",
    "# 아마존 상품리뷰\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/aivle/data/data_sentiment.csv',encoding='utf-8')\n",
    "print(df.head())\n",
    "\n",
    "# vader 결과\n",
    "vaderresult=[]\n",
    "# 상품리뷰 점수와 같은 경우\n",
    "truecount = 0\n",
    "\n",
    "for i in range(df.id.count()):\n",
    "    # compound결과가 0.05보다 크면 긍정, -0.05보다 작으면 부정\n",
    "    if analyser.polarity_scores(df.textcontent[i]).get(\"compound\")>0.05:\n",
    "        vaderresult.append(1)\n",
    "    elif analyser.polarity_scores(df.textcontent[i]).get(\"compound\")< -0.05:\n",
    "        vaderresult.append(0)\n",
    "    else:\n",
    "        vaderresult.append(3)\n",
    "\n",
    "    print(df.textcontent[i], df.reviewrating[i], vaderresult[i])\n",
    "\n",
    "    #자세히 보고 싶다면..\n",
    "    #print(df.textcontent[i],analyser.polarity_scores(df.textcontent[i]))\n",
    "    \n",
    "for i in range(df.id.count()):\n",
    "    if vaderresult[i]==df.reviewrating[i]:\n",
    "        truecount=truecount+1\n",
    "# 정확도\n",
    "print(\"정확도 : \", truecount/(df.id.count()))\n",
    "print(\"정확도 : \", format(truecount/(df.id.count()), \".3f\"))\n",
    "#print(\"정확도 : \", round(truecount/(df.id.count()), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e512c3a-a49b-4c65-99b6-00c053e06f20",
   "metadata": {},
   "source": [
    "### clustering_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5333c20-b76c-43c2-b375-870477c27d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#구글드라이브 마운트\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 입력 데이터 위치\n",
    "Path = '/content/drive/MyDrive/aivle/data/clustering/' \n",
    "\n",
    "# 각 주제별 100건의 뉴스 데이터\n",
    "# Read an Excel file into a pandas DataFrame\n",
    "# DataFrame : 2차원 데이터구조. Row, Column, Series(각 Column에 있는 데이터들)로 구성\n",
    "climate = pd.read_excel(Path+'news_climate change.xlsx')\n",
    "mobility = pd.read_excel(Path+'news_mobility.xlsx')\n",
    "probiotics = pd.read_excel(Path+'news_probiotics.xlsx')\n",
    "\n",
    "# 주제 column을 추가\n",
    "climate['주제'] = '기후위기'\n",
    "mobility['주제'] = '모빌리티'\n",
    "probiotics['주제'] = '유산균'\n",
    "\n",
    "# 각 주제별 70건의 뉴스데이터를 가져와서 입력용 데이터를 생성\n",
    "# pandas.concat(objs, axis=0, ignore_index=Faslse, ...)\n",
    "# axis=0, 0: 위+아래로 합치기, 1: 왼쪽+오른쪽으로 합치기\n",
    "# ignore_index=True : 기존 index를 무시. The resulting axis will be labeled 0, …, n - 1. \n",
    "data = pd.concat([climate[:70], mobility[:70], probiotics[:70]], ignore_index=True)\n",
    "\n",
    "# 뉴스기사 body, title을 기준으로 중복을 제거\n",
    "# DataFrame.drop_duplicates : Returns DataFrame with duplicate rows removed.\n",
    "data_unique = data.drop_duplicates(['body']).drop_duplicates(['title'])\n",
    "\n",
    "# 중복 제거된 데이터 건수를 확인\n",
    "# DataFrame.shape : Returns a tuple representing the dimensionality of the DataFrame.\n",
    "data.shape, data_unique.shape\n",
    "\n",
    "@ 중복으로 들어있는 기사를 제거하고 기사들의 순서를 무작위로 섞습니다.\n",
    "\n",
    "# 데이터 순서를 셔플링\n",
    "# frac: 랜덤 추출할 비율 (1 == 전체 데이터를 셔플링)\n",
    "# random_state: 랜덤 추출할 값에 seed 설정하면, 항상 같은 결과를 생성\n",
    "# reset_index: Reset the index. Use drop parameter to avoid the old index being added as a column\n",
    "data_shuffled = data_unique.sample(frac=1, random_state=16).reset_index(drop=True) \n",
    "data_shuffled.tail(5)\n",
    "\n",
    "# DataFrame.iloc : integer-location based indexing for selection by position\n",
    "data_shuffled.iloc[0]['title']\n",
    "data_shuffled.iloc[0]['body']\n",
    "\n",
    "# 한국어 형태소 분석기를 포함하고 있는 파이썬 패키지 설치\n",
    "!pip install konlpy\n",
    "\n",
    "< 뉴스 기사 단어 빈도 분석 >\n",
    "\n",
    "# 데이터에서 첫번째 기사 내용을 가져와서 명사만 추출합니다\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()  # 카이스트 연구실에서 개발에서 오픈 \n",
    "\n",
    "temp = hannanum.nouns(data_shuffled.loc[0]['body'])  # nouns-> 일반명사/ 고유명사만 추출\n",
    "print(temp)\n",
    "\n",
    "# 명사만 추출된 리스트\n",
    "word_list=temp\n",
    "\n",
    "# 문서에 출현한 각 명사로부터 Series 객체를 생성 (index는 0부터 시작하는 정수값)\n",
    "# 0 차량\n",
    "# 1 고장\n",
    "# 2 제동\n",
    "# 3 차량\n",
    "# 4 고장\n",
    "# 5 ...\n",
    "word_list=pd.Series([x for x in word_list if len(x)>1])  # 객체생성\n",
    "\n",
    "# pd.Series.value_counts: Returns a Series containing counts of unique values.\n",
    "# The resulting object will be in descending order so that the first element is the most frequently-occurring element.  \n",
    "word_list.value_counts().head(10), data_shuffled.iloc[0]['주제']\n",
    "\n",
    "< DBSCAN 예제 : 기사 분석하기 >\n",
    "    \n",
    "# 기사 내용중 명사만을 추출하여 docs 리스트에 저장합니다.\n",
    "docs = []\n",
    "for i in data_shuffled['body']:\n",
    "    docs.append(hannanum.nouns(i))\n",
    "\n",
    "# 공백 문자를 넣어서 각 단어를 합칩니다.\n",
    "for i in range(len(docs)):\n",
    "    docs[i] = ' '.join(docs[i])\n",
    "\n",
    "docs[0]\n",
    "\n",
    "# 처음 한나눔 분석기를 이용해서 명사만 추출한 docs의 구조는 다음과 같은 이중 리스트로 구성되어 있습니다.\n",
    "\n",
    "# [[문장1], [문장2], ... , [문장70]]\n",
    "# 두번째 for문을 이용하면 docs는 다음과 같이 변형되고\n",
    "\n",
    "# [문장1, 문장2, ... , 문장70]\n",
    "# 각 리스트의 원소가 명사들 사이에 공백이 삽입된 텍스트로 바뀝니다.\n",
    "\n",
    "# sklearn.feature_extraction.text.TfidfVectorizer :\n",
    "# Converts a collection of raw documents to a matrix of TF-IDF features (document-term matrix)\n",
    "# ngram_range(min_n, max_n): all values of n such that min_n <= n <= max_n will be used. \n",
    "# min_df : ignore terms that have a document frequency lower than the given threshold.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# n(1 to 5)gram 을 사용하여 위 기사의 명사들을 tfidf vector로 변환\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 3, ngram_range=(1,5))\n",
    "tfidf_vectorizer.fit(docs)\n",
    "vector = tfidf_vectorizer.transform(docs).toarray()\n",
    "print(vector.shape) # num_of_documents X vocab   DTM (Document -Term Matrix)\n",
    "\n",
    "# sklearn.cluster.DBSCAN: Performs DBSCAN clustering from vector array\n",
    "# eps: maximum distance between two samples (default 0.5)\n",
    "# min_samples: The number of samples in a neighborhood for a point to be considered as a core point (default 5) \n",
    "# metric: The metric to use when calculating distance between instances in a feature array. (default 'euclidean')\n",
    "# 'cosine' distance == (1 - cosine similarity)\n",
    "# ex) If 2 vectors are perfectly the same then the similarity is 1 (angle=0 hence 𝑐𝑜𝑠(𝜃)=1), and the distance is 0 (1–1=0).\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "vector = np.array(vector)\n",
    "model = DBSCAN(eps=0.6, min_samples=3, metric = \"cosine\") # 얘네를 바꿔서 하면 됨 (노이즈 줄어듦)\n",
    "result = model.fit_predict(vector) # Computes clusters from a data and predict labels.\n",
    "\n",
    "# 클러스터 번호 -1에는 노이즈 데이터로 판별되어 클러스터링이 안된 문서들이 들어 있음\n",
    "print('클러스터 번호: 클러스터에 속한 기사의 수')\n",
    "result_dict = {str(i):list(result).count(i) for i in set(result)} # result의 각 클러스터에 속한 기사수를 카운트\n",
    "result_dict # 마지막 159 = 노이즈\n",
    "\n",
    "# 클러스터 번호를 column으로 추가\n",
    "data_shuffled['result'] = result\n",
    "\n",
    "sentences = []\n",
    "max_cluster_num = 0\n",
    "for cluster_num in set(result):\n",
    "    sentence = ''\n",
    "\n",
    "    # -1은 노이즈 판별이 났거나 클러스터링이 안된 경우\n",
    "    if(cluster_num == -1): \n",
    "        continue\n",
    "    else:\n",
    "        max_cluster_num = cluster_num         # 생성된 클러스터의 갯수를 카운트\n",
    "        temp_df = data_shuffled[data_shuffled['result'] == cluster_num] # cluster num 별로 조회\n",
    "\n",
    "        print(\"cluster num : {}\".format(cluster_num))\n",
    "        # zip: 동일한 개수로 이루어진 자료형을 묶어 준다\n",
    "        for data in zip(temp_df['title'], temp_df['body'], temp_df.iloc): \n",
    "            title, body, my_data = data\n",
    "            print(my_data['주제'], ':', title) # 주제, 기사 제목을 출력\n",
    "            sentence += body + ' '\n",
    "        print()\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "# 각 클러스터 마다 데이터 출력해주기\n",
    "for i in range(max_cluster_num+1):\n",
    "  data_cluster = data_shuffled[data_shuffled.result==i]\n",
    "  data_num = len(data_cluster)\n",
    "\n",
    "  if len(data_cluster) > num_to_display:\n",
    "    data_cluster = data_cluster[0:num_to_display]\n",
    "\n",
    "  print('----- 클러스터 %i -----' %i)\n",
    "  print('총 데이터 개수: %i' % data_num)\n",
    "  print(data_cluster)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f382540-a136-4fc7-8ffd-89da9679e737",
   "metadata": {},
   "source": [
    "### Word2Vec_Wiki_Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4525c00-27a7-4b1d-a3f0-a949c0a34f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "<모델!>\n",
    "(주의) word2vec 모델 학습에 장시간이 소요되므로 강의 중에 실행하지 않습니다.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import codecs\n",
    "import os\n",
    "import gensim\n",
    "\n",
    "%%time\n",
    "def read_data(filename):\n",
    "    with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "        data = [line.split(' ') for line in f.read().splitlines()]\n",
    "    return data\n",
    "        \n",
    "data = read_data('/content/gdrive/My Drive/aivle/data/wiki/wiki.txt')\n",
    "\n",
    "print(len(data))\n",
    "print(data[0])\n",
    "\n",
    "%%time\n",
    "import gensim\n",
    "# Word2Vec 생성\n",
    "# sg=1이면 skip-gram 0이면 cbow 방식 사용, min_count 단어 빈도수, size 벡터차원\n",
    "model = gensim.models.Word2Vec(data, min_count=1, size=100, window=5, sg=1, seed=10)\n",
    "model.save('/content/gdrive/My Drive/Colab Notebooks/aivle/data/wiki/wiki.w2v')\n",
    "\n",
    "%%time\n",
    "model = gensim.models.Word2Vec.load(os.path.join('/content/gdrive/My Drive/aivle/data/wiki', 'wiki.w2v'))\n",
    "\n",
    "model.wv['고양이']\n",
    "\n",
    "result = model.wv.accuracy(os.path.join('/content/gdrive/My Drive/aivle/data/wiki','word_analogy_korean.txt')) \n",
    "for r in result:\n",
    "    print('%s\\t%s\\t%s' % (r['section'], len(r['correct']), len(r['incorrect'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756fd9d8-ef14-4d1e-9671-b4fc05fa4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# gensim은 텍스트를 벡터로 변환하는 데 필요한 함수들 제공\n",
    "# gensim is an open-source library for unsupervised topic modeling, document indexing, \n",
    "# retrieval by similarity, and other natural language processing functionalities. \n",
    "import os\n",
    "import gensim\n",
    "import codecs\n",
    "\n",
    "model = gensim.models.Word2Vec.load(os.path.join('/content/gdrive/My Drive/aivle/data/wiki', 'wiki.w2v'))\n",
    "\n",
    "model.wv[\"강아지\"]\n",
    "\n",
    "# Word Analogy Test (임베딩된 단어벡터들이 의미론적/문법적 관계를 잘 반영하는지 테스트)\n",
    "# Find the top-N most similar words. Positive words contribute positively towards the similarity, negative words negatively.\n",
    "for t in model.wv.most_similar(positive=[\"파리\", \"일본\"], negative=[\"도쿄\"], topn=10):\n",
    "    print('%s\\t%f' % (t[0], t[1]))\n",
    "    \n",
    "result = model.wv.accuracy(os.path.join('/content/gdrive/My Drive/Colab Notebooks/aivle/data/wiki','word_analogy_korean.txt'))\n",
    "for r in result:\n",
    "    print('%s\\t%s\\t%s' % (r['section'], len(r['correct']), len(r['incorrect'])))\n",
    "    \n",
    "# most_similar 함수 top-N개 유사 단어를 반환\n",
    "for t in model.wv.most_similar(\"강아지\", topn=5):\n",
    "    print(t[0])\n",
    "    \n",
    "model.wv.similarity(\"호랑이\", \"표범\") # 두 단어의 similar점수 뽑아줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d803e-f42c-4c71-bc29-a0ea419d732b",
   "metadata": {},
   "source": [
    "### PyTorch_tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e366a8d-eeac-47df-a741-c719643badb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The torch package contains data structures for multi-dimensional tensors and \n",
    "# defines mathematical operations over these tensors. \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "@ 텐서초기화 하기\n",
    "# The torch package contains data structures for multi-dimensional tensors and \n",
    "# defines mathematical operations over these tensors. \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 랜덤값으로 텐서 생성. 정수값으로 텐서의 차원을 전달\n",
    "# 랜덤값으로 채워진 (3, 4) 차원의 텐서를 생성 \n",
    "tensor = torch.rand(3, 4)   \n",
    "print (\"tensor :\", tensor)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")     \n",
    "print(f\"Datatype of tensor: {tensor.dtype}\") \n",
    "print(f\"Device tensor is stored on: {tensor.device}\") \n",
    "\n",
    "# Returns a tensor filled with the scalar value 1,with the shape defined by the argument size.\n",
    "tensor = torch.ones(4, 4)\n",
    "print(tensor)\n",
    "# 모든 행에 대해서 1번열에 0값을 대입\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n",
    "\n",
    "# Concatenate : 텐서를 연결하기\n",
    "# 딥러닝에서는 모델의 입력 또는 중간 연산 단계에서 두 개의 텐서를 연결하는 경우가 있습니다.\n",
    "# 두 텐서를 연결해서 입력으로 사용하는 것은 두 텐서에 담긴 정보를 모두 사용한다는 의미입니다. \n",
    "# dim : 텐서를 연결하여 어느 차원을 늘릴 것인지를 표시\n",
    "t1 = torch.cat([tensor, tensor], dim=0) # 두 텐서를 연결하여 0번째 차원을 늘리라는 의미\n",
    "print(\"tensor shape:\", tensor.shape)\n",
    "print(\"->\", t1)\n",
    "print(\"t1 shape:\", t1.shape) # 0번째 dimension이 늘어난 것을 확인\n",
    "print(\"----------------------------\")\n",
    "t2 = torch.cat([tensor, tensor], dim=1) # 두 텐서를 연결하여 1번째 차원을 늘리라는 의미\n",
    "print(\"tensor shape:\", tensor.shape)\n",
    "print(\"->\", t2)\n",
    "print(\"t2 shape:\", t2.shape) # 1번째 dimension이 늘어난 것을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3e03a-1cec-473f-ac38-870e0d86ac9b",
   "metadata": {},
   "source": [
    "* 신경망 모델 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ede84-1241-4450-b79c-692d4b468206",
   "metadata": {},
   "outputs": [],
   "source": [
    "모든 신경망 클래스는 torch.nn 패키지를 통해서 생성합니다.\n",
    "torch.nn.Module은 PyTorch의 모든 신경망의 Base Class이며 새로운 신경망 모델은 torch.nn.Module 클래스를 상속하여 정의해야 합니다.\n",
    "새로운 클래스 내에서 __init()__함수와 forward()함수를 반드시 override 해야 합니다.\n",
    "__init()__함수에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function(ReLU 등)를 정의합니다.\n",
    "forward()에서는 모델에서 실행되어야 하는 연산을 정의합니다.\n",
    "backward 연산은 backward() 함수를 호출하면 PyTorch가 자동으로 수행하므로 forward()만 정의합니다.\n",
    "forward()에서는 input 데이터에 대해 어떤 연산을 진행하여 output이 나올지를 정의해 주는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e782de5-10a3-46db-bc0a-12043b20c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn           # 신경망 구현을 위한 데이터 구조, 신경망 레이어 등이 정의되어 있음 \n",
    "import torch.nn.functional as F # Convolution, Pooling, Activation, Linear 함수 등이 정의되어 있음\n",
    "\n",
    "# torch.nn.Module은 PyTorch의 모든 신경망의 Base Class\n",
    "# __init()__과 forward()를 반드시 override 해야 한다.\n",
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    # 입력 이미지 채널 1개, 출력 채널 6개, 5x5의 정사각 컨볼루션 행렬\n",
    "    # 컨볼루션 커널 정의\n",
    "    # nn.Conv2d(in_channels, out_channels, kernel_size, stride=1)\n",
    "    self.conv1 = nn.Conv2d(1, 6, 5)   # 입력 채널 크기, 출력 채널 크기, 커널 크기\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)  # 입력 채널 크기, 출력 채널 크기, 커널 크기\n",
    "    \n",
    "    # Fully Connected Layer: y = Wx + b\n",
    "    self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5은 이미지 차원에 해당\n",
    "    self.fc2 = nn.Linear(120, 60)\n",
    "    self.fc3 = nn.Linear(60, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # convolution을 거치게 되면 이미지의 크기는 kernel - 1만큼 감소함\n",
    "    # 현재 입력되는 이미지의 크기가 32 X 32\n",
    "    # conv1을 통해 출력되는 이미지의 크기는 (32 - 5 + 1) X (32 - 5 + 1) = 6 X 28 X 28\n",
    "    # (2, 2) 크기 윈도우에 대해 맥스 풀링 -> 이미지의 크기는 2분의 1이 되므로 -> 최종 출력 이미지 크기는 14 X 14\n",
    "    # torch.nn.functional.max_pool2d(input, kernel_size)\n",
    "    x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # [batch size, 채널 크기, 이미지 가로 크기, 이미지 세로 크기] == [1, 6, 14, 14] \n",
    "\n",
    "    # conv2을 통해 출력되는 이미지의 크기는 (14 - 5 + 1) X (14 - 5 + 1) = 10 X 10\n",
    "    # (2, 2) 크기 윈도우에 대해 맥스 풀링 -> 이미지의 크기는 2분의 1이 되므로 -> 최종 출력 이미지 크기는 5 X 5\n",
    "    # 크기가 제곱수라면, 하나의 숫자만을 특정(specify)\n",
    "    x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2)) # [1, 16, 5, 5]\n",
    "    # torch.flatten(input, start_dim, end_dim) : flattens input by reshaping it into a one-dimensional tensor. \n",
    "    x = torch.flatten(x, 1) # batch 차원을 제외한 모든 차원을 하나로 평탄화(flatten) [1, 16 * 5 * 5], Dimension =1 \n",
    "    x = F.relu(self.fc1(x)) # [1, 120]\n",
    "    x = F.relu(self.fc2(x)) # [1, 60]\n",
    "    x = self.fc3(x) # [1, 10]\n",
    "    return x\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32) # [batch size, 입력 채널 크기, 가로 길이, 세로 길이]\n",
    "out = net(input)\n",
    "print(\"out shape:\", out.shape)\n",
    "print(out)  # out shape: (1, 10)\n",
    "\n",
    "# view 함수 : reshape 함수. 텐서의 형태(Shape)를 변경함. 변경 전과 후에 원소의 갯수는 유지되어야 한다.\n",
    "# view(1, -1) : 첫번째 차원은 1이 되도록 하되,-1로 표시된 2번째 차원은 파이토치가 알아서 계산하라는 의미\n",
    "output = net(input)               # 입력 데이터를 신경망 모델에 전달하여 예측값을 얻음 shape: (1, 10)\n",
    "print(\"output shape:\", output.shape)\n",
    "target = torch.randn(10)          # 임의의 텐서를 생성하여 정답값으로 가정\n",
    "print (\"target shape:\", target.shape)\n",
    "target = target.view(1, -1)       # 모델의 출력 텐서와 동일한 shape로 변경 : (1, 10)\n",
    "print (\"after reshape(view) -> target shape:\", target.shape)\n",
    "\n",
    "@ 손실함수 (Loss Function) 설정\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 어떤 텐서가 학습에 필요한 텐서라면 backpropagation을 통하여 gradient를 구해야 합니다.\n",
    "# 텐서의 옵션 requireds_grad를 True로 설정하면 텐서에 실행되는 모든 연산들을 트랙킹하여 자동으로 gradient를 계산합니다.\n",
    "input = torch.randn(3, 5, requires_grad=True) \n",
    "target = torch.randn(3, 5)\n",
    " # 손실함수로 MSE(Mean Squared Error)를 설정\n",
    " # 평균제곱오차(MSE)는 오차를 제곱한 값의 평균. 오차란 모델이 예측한 값과 실제 정답과의 차이\n",
    "criterion = nn.MSELoss()          \n",
    "output = criterion(input, target) # Loss 계산\n",
    "output.backward()\n",
    "print('input: ', input)\n",
    "print('target: ', target)\n",
    "print('output: ', output)\n",
    "\n",
    "@ 옵티마이저 설정\n",
    "\n",
    "# torch.optim : 신경망 학습을 위한 파라미터 최적화 알고리즘이 구현되어 있는 클래스\n",
    "import torch.optim as optim\n",
    "# Adam Optimizer 객체 생성\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "# 학습 과정(training loop)\n",
    "# Pytorch에서는 gradients 값들을 backward를 할 때 계속 누적하기 때문에 \n",
    "# 학습을 시작하기 전에 gradients 버퍼를 zero로 reset해야 한다.\n",
    "optimizer.zero_grad()\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "output = net(input)\n",
    "target = torch.randn(10)          # 예시를 위한 임의의 정답\n",
    "target = target.view(1, -1)       # 출력과 같은 shape로 만듬\n",
    "loss = criterion(output, target)  # Loss 계산\n",
    "print(loss)\n",
    "\n",
    "loss.backward()         # 역전파 함수 실행을 통해 gradient 계산\n",
    "optimizer.step()        # gradient를 기반으로 실제 파라미터 업데이트를 실행\n",
    "\n",
    "# torch.optim : 신경망 학습을 위한 파라미터 최적화 알고리즘들이 구현되어 있는 팩키지\n",
    "import torch.optim as optim\n",
    "# Optimizer 객체를 생성하고 모델의 파라미터를 전달, learning rate 설정\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01) \n",
    "\n",
    "# 학습 과정(training loop)\n",
    "# Pytorch에서는 gradients 값들을 backward를 할 때 계속 누적하기 때문에 \n",
    "# 학습을 시작하기 전에 gradients 버퍼를 zero로 reset해야 한다.\n",
    "optimizer.zero_grad()\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "output = net(input)\n",
    "target = torch.randn(10)          # 예시를 위한 임의의 정답\n",
    "target = target.view(1, -1)       # 출력과 같은 shape로 만듬\n",
    "loss = criterion(output, target)  # Loss 계산\n",
    "print(loss)\n",
    "\n",
    "loss.backward()         # 역전파 함수 실행을 통해 gradient 계산\n",
    "optimizer.step()        # 파라미터 업데이트를 실행\n",
    "\n",
    "@ GPU 가속 이용하기\n",
    "# 현재 개발환경에서 GPU 가속이 가능한지 확인\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# GPU 사용이 가능하면 cuda로 연산하도록 device를 설정 그렇지 않으면 cpu로 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "# 랜덤한 학습 데이터 생성\n",
    "train_inputs = torch.randn(100, 32, 128) # [전체 데이터 개수, 데이터의 최대 길이, 히든 벡터 크기]\n",
    "train_labels = torch.randn(100, 3) # [전체 데이터 개수, 예측할 클래스 개수]\n",
    "\n",
    "# Dataset에 필요한 패키지 임포트\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 12 # 매 학습 Step 마다 샘플링할 데이터 개수\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_labels) # 학습데이터와 레이블을 하나의 TensorDataset으로 결합 가능\n",
    "train_sampler = RandomSampler(train_data) # 데이터를 샘플링 할 함수(순차적으로 뽑아올지, 랜덤하게 뽑아올지)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) # 미니배치 단위로 데이터를 자동 로딩\n",
    "\n",
    "# dataloader를 통해서 학습 루프 생성\n",
    "for step, batch in enumerate(train_dataloader): # enumerate : 인덱스(index)와 데이터값에 동시에 접근\n",
    "  x_data, x_label = batch\n",
    "  # 데이터 로딩 step \n",
    "  print(step, x_data.shape, x_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84734415-ad67-49b5-b724-1be90b122dbe",
   "metadata": {},
   "source": [
    "### Sentiment_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd493a93-2dc8-4ce4-ac4a-0a339c8ba77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext.legacy를 사용할 수 있는 torchtext 버전 설치\n",
    "!pip install -U torchtext==0.10.0\n",
    "\n",
    "#colab 을 이용한 실행시\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "import torch\n",
    "# torch.nn : 신경망 구현을 위한 데이터 구조, 신경망 레이어, 관련함수들이 구현되어 있는 팩키지\n",
    "# torch.nn.functional: torch.nn 팩키지의 함수들이 정의되어 있음 (손실함수, 활성화함수, 풀링함수 등) \n",
    "# torch. autograd : 미분을 위한 함수들이 정의되어 있음\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#import torchtext.data as data\n",
    "#import torchtext.datasets as datasets\n",
    "#legacy 버전으로 변경\n",
    "\n",
    "# torchtext : text의 preprocessing 파이프라인 정의, \n",
    "# 토크나이징, Vocab 생성, dataset splits, 데이터 로더 등 지원\n",
    "from torchtext.legacy import data\n",
    "import torchtext.datasets as datasets\n",
    "import pickle\n",
    "\n",
    "# CNN 모델의 구조와 연산을 정의\n",
    "class CNN_Text(nn.Module):\n",
    "    # 생성자 : 모델의 구조와 동작을 정의\n",
    "    # 객체가 갖는 속성값을 초기화함. 객체가 생성될 때 자동으로 호출된다.\n",
    "    def __init__(self, embed_num, class_num):\n",
    "        super(CNN_Text, self).__init__() # nn.Module 클래스를 초기화(부모클래스를 초기화)\n",
    "        # V: 사전의 크기(vocavilary)\n",
    "        # D: embed_dim (단어 벡터의 차원)\n",
    "        # C: 분류하고자 하는 클래스의 개수(긍정 부정 2개 클래스)\n",
    "        # Co : 각 커널(필터)의 갯수\n",
    "        V = embed_num\n",
    "        D = 100 \n",
    "        C = class_num\n",
    "        Co = 50         # output channel 수 (필터의 갯수)\n",
    "        Ks = [2,3,4]\n",
    "\n",
    "        # 사전에 있는 모든 단어 벡터에 random 초기값\n",
    "        self.embed = nn.Embedding(V, D) \n",
    "        # torch.nn.Conv2d (in_channels, out_channels, kernel_size, stride=1)\n",
    "        # convs1에 컨볼루션 모듈의 리스트가 들어감 (필터(커널) 갯수만큼) \n",
    "        # forward에서 순차적으로 접근 가능\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(1, Co, (K, 100)) for K in Ks])\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        # nn.Linear 클래스. Fully Connected Layer \n",
    "        # Applies a linear transformation to the incoming data (y = Wx + b)\n",
    "        # torch.nn.Linear(in_features, out_features)\n",
    "        # in_features: size of each input sample, out_features: size of each output sample \n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C) # 150(3*50), 2 \n",
    "\n",
    "        # foward 함수 : 모델이 학습데이터를 입력받아서 forward 연산을 진행\n",
    "        # model 객체를 데이터와 함께 호출하면 자동으로 실행된다.\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)   # (N, W, D) 미니배치, 문장 최대길이, 단어벡터 차원\n",
    "        x = x.unsqueeze(1)  # (N x Ci x W x D) Conv2d를 사용하려면 입력채널 수 추가해야 함\n",
    "\n",
    "        # Convolution Layer\n",
    "        # Convolution -> ReLU -> 텐서의 dimension 3을 squeeze(max_pool1d는 3D 입력을 받음)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]\n",
    "\n",
    "        # Max Pooling\n",
    "        # F.max_pool1d(input, kernel_size): Applies a 1D max pooling over an input\n",
    "        # Tensor.size(dim=None) : Returns the size of the self tensor. If dim is specified, returns the size of that dimension.\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks) max pooling 후에 마지막 차원은 1 -> squeeze\n",
    "        x = torch.cat(x, 1) # torch.cat(tensors, dim), dim=1이면 두번째 차원이 늘어나게 concat (첫번째 차원은 N)\n",
    "        x = self.dropout(x) # (N, len(Ks)*Co), dropout을 적용\n",
    "        logit = self.fc1(x) # fully-connected layer 적용\n",
    "        return logit\n",
    "    \n",
    "class mydataset(data.Dataset):\n",
    "    @staticmethod  # 유틸리티 메소드 정의 시 선언\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "    def __init__(self, text_field, label_field, path=None, examples=None, **kwargs):\n",
    "        fields = [('text', text_field), ('label', label_field)] # text_field 레이블은 text, label_field 레이블은 label\n",
    "        if examples is None:\n",
    "            path = self.dirname if path is None else path\n",
    "            examples = []\n",
    "            for i,line in enumerate(open(path,'r',encoding='utf-8')):\n",
    "                if i==0:\n",
    "                    continue\n",
    "                line = line.strip().split('\\t')\n",
    "                txt = line[1].split(' ')               \n",
    "                                  \n",
    "                examples += [ data.Example.fromlist( [ txt, line[2]],fields ) ]\n",
    "        super(mydataset, self).__init__(examples, fields, **kwargs)\n",
    "        \n",
    "# Field : 텐서로 변환될 텍스트 데이터타입을 정의 \n",
    "# text_field, label_field : 전처리 관련된 field 객체를 각각 생성 \n",
    "# batch_first : 미니배치 차원을 맨 앞에 둔 텐서를 생성할 것인지\n",
    "# fix_length : 하나의 문장 내 max 토큰수 \n",
    "# sequential : 시퀀스 데이터 여부\n",
    "text_field = data.Field(batch_first = True, fix_length = 20)\n",
    "label_field = data.Field(sequential= False, batch_first = True, unk_token = None) # unk_token을 표현할 스트링\n",
    "\n",
    "train_data = mydataset(text_field,label_field,path='/content/gdrive/My Drive/aivle/data/nsm/small_ratings_train_tok.txt')\n",
    "\n",
    "test_data = mydataset(text_field,label_field,path='/content/gdrive/My Drive/aivle/data/nsm/small_ratings_test_tok.txt')\n",
    "#print(test_data.fields.items())\n",
    "\n",
    "# vocab 생성\n",
    "text_field.build_vocab(train_data)\n",
    "label_field.build_vocab(train_data)\n",
    "\n",
    "# Data Loader 생성 (train_data, test_data를 각각 100개, 1개씩 데이터 로딩)\n",
    "train_iter, test_iter = data.Iterator.splits(\n",
    "                            (train_data, test_data), \n",
    "                            batch_sizes=(100, 1))#, device= 'cuda')\n",
    "len(text_field.vocab)\n",
    "\n",
    "\n",
    "# CNN모델 객체를 생성 (embed_num, class_num)\n",
    "cnn = CNN_Text(len(text_field.vocab),2)\n",
    "\n",
    "# torch.optim : 신경망 학습을 위한 다양한 파라미터 최적화 알고리즘이 구현되어 있는 팩키지\n",
    "# Optimizer를 설정\n",
    "optimizer = torch.optim.Adam(cnn.parameters())\n",
    "cnn.train()\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    totalloss = 0\n",
    "    for batch in train_iter:\n",
    "        optimizer.zero_grad() # resets the gradient to 0\n",
    "        \n",
    "        txt = batch.text\n",
    "        label = batch.label\n",
    "                \n",
    "        #print(txt.size()) -> torch.Size([100, 20])\n",
    "        pred = cnn(txt)\n",
    "                \n",
    "        #print(pred.size(), label.size()) -> torch.Size([100, 2]) torch.Size([100])\n",
    "        #print(label)\n",
    "        loss = F.cross_entropy(pred, label)\n",
    "        totalloss += loss.data\n",
    "        \n",
    "        loss.backward() # backward 연산\n",
    "        optimizer.step() # 파라미터 업데이트\n",
    "        \n",
    "    print(epoch,'epoch')    \n",
    "    print('loss : {:.3f}'.format(totalloss.numpy()))\n",
    "\n",
    "torch.save(cnn,'/content/gdrive/My Drive/aivle/cnn_model.pt')\n",
    "\n",
    "%%time\n",
    "from sklearn.metrics import classification_report\n",
    "cnn.eval() # 모델을 evaluation mode로 설정. 정규화 기술(dropout 등)을 배제하여 온전한 모델로 평가\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "y_test = []\n",
    "prediction = []\n",
    "\n",
    "for batch in test_iter:\n",
    "    txt = batch.text\n",
    "    label = batch.label\n",
    "    y_test.append(label.data[0])\n",
    "\n",
    "    pred = cnn(txt)\n",
    "    _,ans = torch.max(pred,dim=1) # dimension을 기준으로 (최대값, 최대값이 있는 인덱스) 반환 \n",
    "    prediction.append(ans.data[0]) # ans.data[0]: 최대값이 들어있는 인덱스 (0 또는 1) \n",
    "  \n",
    "    if ans.data[0] == label.data[0]:        \n",
    "        correct += 1    \n",
    "    else:\n",
    "        incorrect += 1\n",
    "    \n",
    "print ('correct : ', correct)\n",
    "print ('incorrect : ', incorrect)\n",
    "print(classification_report(torch.tensor(y_test),     # 정답값\n",
    "                            torch.tensor(prediction), # 예측값\n",
    "                            digits=4,                 # 출력할 자리수\n",
    "                            target_names=['negative', 'positive'])) # display names matching the label\n",
    "\n",
    "# Weighted Avg는 클래스의 수치간의 평균 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f846a99-d807-4fe0-8717-6dee0470331b",
   "metadata": {},
   "source": [
    "### SonnyBot_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05847b54-2284-4293-b14f-4715543d4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "    \n",
    "cd Mecab-ko-for-Google-Colab\n",
    "\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh \n",
    "\n",
    "def mecabsplit(mecab_tagger,inputs, pos):\n",
    "    r=[]\n",
    "    inputs = mecab_tagger.parse(inputs)\n",
    "    t = inputs.split('\\n')[:-2]\n",
    "    for i in t:\n",
    "        field = i.split('\\t')\n",
    "        if field[1].split(',')[-1] is not '*':\n",
    "            r.extend( [ (x.split('/')[0],x.split('/')[1]) for x in field[1].split(',')[-1].split('+') ] )\n",
    "        else:\n",
    "            r.append( (field[0],field[1].split(',')[0]) )\n",
    "    if pos:\n",
    "        return r\n",
    "    else:\n",
    "        return [ x[0] for x in r ]\n",
    "    return r\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable # tensor로 해도됨\n",
    "import pickle\n",
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger()\n",
    "\n",
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_num, class_num, ):\n",
    "        super(CNN_Text, self).__init__()\n",
    "        \n",
    "        # 단어 사전 크기\n",
    "        V = embed_num\n",
    "        # 임베딩벡터 크기\n",
    "        D = 100 #args.embed_dim\n",
    "        # 분류하고자 하는 클래스의 개수\n",
    "        C = class_num\n",
    "        # 입력 채널 수\n",
    "        Ci = 1\n",
    "        # 출력 채널 수\n",
    "        Co = 20 #args.kernel_num\n",
    "        # 커널(필터) 사이즈 1단어, 2단어, 3단어\n",
    "        Ks = [1,2,3]\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        # padding numbers for (height,width)\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D), padding=(2,0)) for K in Ks])\n",
    "        # dropout 설정\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # FC 레이어\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # (B, W, D)\n",
    "        \n",
    "        # 입력 x를 4D로 변환\n",
    "        x = x.unsqueeze(1)  # (B(batch), Ci(input channel), W(sent), D(dimension))\n",
    "        # output = F.relu(x) -> B x Co x W x 1\n",
    "        # max_pool1D는 3D 입력만 받음 -> size 1인 차원을 제거(squeeze)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(B, Co, W), ...]*len(Ks)\n",
    "        \n",
    "        # (B x Co x 1) -> size 1인 차원을 제거(squeeze)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(B, Co), ...]*len(Ks)\n",
    "        \n",
    "        # concatenate\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        x = self.dropout(x)  # (B, len(Ks)*Co)\n",
    "        logit = self.fc1(x)  \n",
    "        return logit\n",
    "    \n",
    "# 학습데이터에 나타난 단어 사전\n",
    "content_vocab = {'unk':0}\n",
    "# 의도 레이블 사전\n",
    "intent_vocab={}\n",
    "# 의도 클래스\n",
    "intent_list=[]\n",
    "\n",
    "data_intent=''\n",
    "intent_idx=0\n",
    "vocab_idx=1\n",
    "\n",
    "for line in open('/content/gdrive/My Drive/aivle/data/sonny/mydata.txt','r',encoding='utf-8'):\n",
    "    line = line.strip().split('\\t')\n",
    "    if len(line)>1:\n",
    "        intent=line[1]\n",
    "        if intent not in intent_vocab:\n",
    "            intent_vocab[intent]=intent_idx\n",
    "            intent_list.append(intent)\n",
    "            intent_idx +=1\n",
    "    else:\n",
    "        line = mecabsplit(tagger,line[0],False)\n",
    "        for it in line:\n",
    "            if it not in content_vocab:\n",
    "                content_vocab[it] = vocab_idx\n",
    "                vocab_idx +=1\n",
    "                \n",
    "cnn = CNN_Text(vocab_idx,intent_idx)\n",
    "print(vocab_idx, intent_idx) # 파일에 나타난 단어수, 의도 갯수(Class 갯수)\n",
    "optimizer = torch.optim.Adam(cnn.parameters())\n",
    "cnn.train()\n",
    "\n",
    "epoch = 10\n",
    "for e in range(epoch):\n",
    "    totalloss = 0\n",
    "    for line in open('/content/gdrive/My Drive/aivle/data/sonny/mydata.txt','r',encoding='utf-8'):\n",
    "        line = line.strip().split('\\t')\n",
    "    \n",
    "        if len(line)> 1:\n",
    "            target = Variable(torch.LongTensor([intent_vocab[line[1]]]))\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cont = []\n",
    "        line = mecabsplit(tagger,line[0],False)\n",
    "        for it in line:\n",
    "            cont.append(content_vocab[it])\n",
    "        # view : 원소의 수를 유지하면서 텐서를 reshape, 텐서의 첫번째 차원을 1로 reshape\n",
    "        cont = Variable(torch.LongTensor(cont)).view(1,-1)\n",
    "        pred = cnn(cont)\n",
    "\n",
    "        loss = F.cross_entropy(pred,target)\n",
    "        totalloss += loss.data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print (e, 'epoch')\n",
    "    print('loss : {:.3f}'.format(totalloss.numpy()))\n",
    "    \n",
    "response = []\n",
    "for line in open('/content/gdrive/My Drive/aivle/data/sonny/response.txt','r',encoding='utf-8'):\n",
    "    line=line.strip()\n",
    "    response.append(line)\n",
    "    \n",
    "    \n",
    "cnn.eval()\n",
    "for line in open('/content/gdrive/My Drive/aivle/data/sonny/testdata.txt','r',encoding='utf-8'):\n",
    "    line = line.strip()\n",
    "    \n",
    "    line = mecabsplit(tagger,line,False)\n",
    "    cont = []\n",
    "    for it in line:\n",
    "        if it in content_vocab:\n",
    "            cont.append(content_vocab[it]) # cont에는 입력문에 나타난 단어들의 index 저장됨\n",
    "        else:\n",
    "            cont.append(content_vocab['unk'])\n",
    "    cont = Variable(torch.LongTensor(cont)).view(1,-1)\n",
    "    pred = cnn(cont)\n",
    "    v,i = torch.max(pred,1) # pred는 (p1, p2) 즉, 클래스별 확률 v: 둘중 큰값 i:큰값 클래스의 인덱스\n",
    "    \n",
    "    print('input : ',line)\n",
    "    # 3개 클래스의 확률값\n",
    "    probs = torch.nn.functional.softmax(pred,dim=-1).data.numpy()[0]\n",
    "    print ([probs[0], probs[1], probs[2]])\n",
    "    print('intent : ',intent_list[int(i)])\n",
    "    print(response[int(i)])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b816a2-7eb4-43c4-af4e-924f251533d5",
   "metadata": {},
   "source": [
    "### Sentiment_RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df4e17-d087-42af-9aed-ccb4ff5022af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchtext.legacy를 사용할 수 있는 torchtext 버전 설치\n",
    "!pip install -U torchtext==0.10.0\n",
    "\n",
    "#colab 을 이용한 실행시\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # 뉴럴 네트워크\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# torchtext.legacy : text의 preprocessing 파이프라인 정의\n",
    "# 1) 토크나이징(Tokenization)\n",
    "# 2) 단어장 생성(Build Vocabulary)\n",
    "# 3) 토큰의 수치화(Numericalize all tokens)\n",
    "# 4) 데이터 로더 생성(Create Data Loader)\n",
    "from torchtext.legacy import data\n",
    "import torchtext.datasets as datasets\n",
    "\n",
    "import pickle\n",
    "print (torch.__version__)\n",
    "\n",
    "class RNN_Text(nn.Module):   # init, forward를 꼭 재정의 해줘야함 \n",
    "    def __init__(self, embed_num, class_num):\n",
    "        # super()로 Base Class의 __init__() 호출 (nn.Module 클래스 생성자 호출)\n",
    "        # super(파생클래스, self).__init__() 파이썬 2.x 문법\n",
    "        # super().__init__() 파이썬 3.x 문법 둘다 사용 가능\n",
    "        super(RNN_Text, self).__init__()\n",
    "          \n",
    "        V = embed_num   # 단어 사전의 크기\n",
    "        C = class_num   # 분류하고자 하는 클래스 개수        \n",
    "        H = 256         # 히든 사이즈\n",
    "        D = 100         # 단어벡터 차원 100        \n",
    "        self.embed = nn.Embedding(V, D)        \n",
    "        \n",
    "        # LSTM Layer, bidirectional이므로 출력되는 벡터의 크기는 H * 2\n",
    "        self.rnn = nn.LSTM(D, H, bidirectional = True) # bidirectional = 양방향의 여부\n",
    "                 \n",
    "        # Linear Layer : (512, 2) # 긍정부정(2개)\n",
    "        self.out = nn.Linear(H*2, C) # 마지막 상태의 히든스테이트가 됨(양방향이 되기 때문에 256*2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)     # (N, W, D) 문장 x의 단어 벡터값 가져옴 (임베딩해줌) 차원을 나타냄, 한문장과 batch size를 곱해줌\n",
    "      \n",
    "        # LSTM 모듈 실행\n",
    "        # LSTM 입력데이터\n",
    "        # input x : torch.Size([30, 100, 100]) [시퀀스 길이, 배치 사이즈, Dimension] -> 문장하나다\n",
    "        x,(_,__) = self.rnn( x, ( self.h, self.c ) )  # 몇차원의 텐서가 들어가고 나오는지가 중요함\n",
    "\n",
    "        # output x : torch.Size([30, 100, 512]) [시퀀스 길이, 배치 사이즈, 256 * 2]# 마지막 상태의 히든스테이트가 됨\n",
    "        # print('output * size', x.size)\n",
    "        # 최종 Hidden Layer로 Linear 모듈 실행   \n",
    "        logit = self.out(x[-1])  # 첫번째 차원, 즉 시퀀스차원에 있는 마지막 단어의 배치값이 담겨있는 tensor를 가져옴\n",
    "\n",
    "        # 최종 예측 벡터 크기: [배치 사이즈, C], C: 클래스 개수\n",
    "        return logit       # logit : torch.Size([100, 2])\n",
    "\n",
    "    def inithidden(self, b): # 히든스테이트 초기화 함수(cell, hidden 둘다 랜덤으로 초기화)\n",
    "        #self.h = Variable(torch.randn(2, b, 256))\n",
    "        #self.c = Variable(torch.randn(2, b, 256))    \n",
    "        self.h = torch.randn(2, b, 256)   # [2, batch_size, 256]\n",
    "        self.c = torch.randn(2, b, 256)   # [2, batch_size, 256]\n",
    "        \n",
    "        \n",
    "# train, test dataset을 만들어준다\n",
    "class mydataset(data.Dataset):\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "    def __init__(self, text_field, label_field, path=None, examples=None, **kwargs):\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "        if examples is None:\n",
    "            path = self.dirname if path is None else path\n",
    "            examples = []\n",
    "            for i,line in enumerate(open(path,'r',encoding='utf-8')):\n",
    "                if i==0:      # 첫번째 라인은 skip\n",
    "                    continue\n",
    "                line = line.strip().split('\\t') # text, label 필드가 /tab으로 구분되어 있다                  \n",
    "                txt = line[1].split(' ')  # 공백을 기준으로 문자열을 나누어 토큰 리스트를 만든다. line[0]에는 ID\n",
    "               \n",
    "                # examples: 학습 텍스트, 라벨 텍스트\n",
    "                # data.Example : Defines a single training or test example.\n",
    "                examples += [ data.Example.fromlist( [txt, line[2]],fields ) ]\n",
    "        # Create a dataset from a list of Examples and Fields.\n",
    "        # fields : field name, field \n",
    "        super(mydataset, self).__init__(examples, fields, **kwargs) \n",
    "\n",
    "        \n",
    "# Field 객체는 다음과 같은 값을 통하여 데이터의 각 필드를 처리하는 방법을 지정\n",
    "# fix_length: A fixed length that all examples using this field will be padded to, or None for flexible sequence lengths. \n",
    "# sequential: Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.\n",
    "# batch_first: Whether to produce tensors with the batch dimension first. Default: False.\n",
    "##text_field = data.Field(fix_length=20)\n",
    "text_field = data.Field(fix_length=30)\n",
    "label_field = data.Field(sequential=False, batch_first = True, unk_token = None)\n",
    "\n",
    "# 학습데이터 Dataset\n",
    "train_data = mydataset(text_field,label_field,path='/content/gdrive/My Drive/aivle/data/nsm/small_ratings_train_tok.txt')\n",
    "# 테스트데이터 Dataset\n",
    "test_data = mydataset(text_field,label_field,path='/content/gdrive/My Drive/aivle/data/nsm/small_ratings_test_tok.txt')\n",
    "\n",
    "text_field.build_vocab(train_data)    # Construct the Vocab object \n",
    "label_field.build_vocab(train_data)   # Construct the Vocab object \n",
    "\n",
    "# Create Iterator objects for train data, test data\n",
    "train_iter, test_iter = data.Iterator.splits(\n",
    "                            (train_data, test_data), \n",
    "                            batch_sizes=(100, 1), repeat=False)#, device = -1)\n",
    "len(text_field.vocab)\n",
    "\n",
    "rnn = RNN_Text(len(text_field.vocab),2)     # embed_num, class_num\n",
    "optimizer = torch.optim.Adam(rnn.parameters())\n",
    "rnn.train()\n",
    "\n",
    "%%time\n",
    "bool_debug = True    # 텐서의 차원을 출력할 경우 True로 설정\n",
    "print_idx = 3        # 출력 횟수\n",
    "for epoch in range(10):\n",
    "    \n",
    "    totalloss = 0\n",
    "    for batch in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        txt = batch.text        # torch.Size([30, 100])\n",
    "        label = batch.label     # torch.Size([100])\n",
    "        \n",
    "        if bool_debug and print_idx > 0:\n",
    "          # print('txt.size():', txt.shape)\n",
    "          print (\"txt.shape:\", txt.shape)\n",
    "          print_idx -= 1\n",
    "\n",
    "        # inithiddend : hidden state, cell state 초기화 함수\n",
    "        rnn.inithidden(txt.size(1))   # 배치 사이즈를 전달\n",
    "        # 학습 실행\n",
    "        pred = rnn(txt)\n",
    "        \n",
    "        if bool_debug and print_idx > 0:\n",
    "          print(\"pred.shape:\", pred.shape)\n",
    "          print(\"label.shape:\", label.shape)\n",
    "          print_idx -= 1        \n",
    "\n",
    "        loss = F.cross_entropy(pred, label)\n",
    "        totalloss += loss.data\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(epoch,'epoch')  \n",
    "    print('loss : {:.3f}'.format(totalloss.numpy()))\n",
    "       \n",
    "torch.save(rnn,'/content/gdrive/My Drive/aivle/model/rnn_model.pt')\n",
    "\n",
    "%%time\n",
    "bool_debug = True    # 텐서의 차원을 출력할 경우 True로 설정\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "rnn.eval()\n",
    "y_test = []\n",
    "prediction = []\n",
    "\n",
    "# 텐서 차원 확인용\n",
    "print_tensor_shape = 2\n",
    "print_idx = 1\n",
    "\n",
    "for batch in test_iter:\n",
    "    txt = batch.text            # txt.shape: torch.Size([max_sent_len, 1])\n",
    "    label = batch.label         # label.shape: torch.Size([1])\n",
    "    y_test.append(label.data[0])\n",
    "    \n",
    "    rnn.inithidden(txt.size(1))\n",
    "   \n",
    "    pred = rnn(txt)               # pred.shape: torch.Size([1, 2])\n",
    "    \n",
    "    _ , ans = torch.max(pred,dim=1) # ans.shape: torch.Size([1])\n",
    "    prediction.append(ans.data[0])\n",
    "    \n",
    "    \n",
    "    #---------------------------------------\n",
    "    # 텐서 형태, 데이터를 출력\n",
    "    if bool_debug and print_tensor_shape > 0:\n",
    "      print(\"-----\", print_idx, \"-----\") \n",
    "      print(\"prediction:\", prediction)\n",
    "      print(\"y_test:\", y_test)\n",
    "      print(\"pred.shape:\", pred.shape)\n",
    "      #print(\"pred.data[0]:\", pred.data[0])\n",
    "      print(\"pred[0]:\", pred[0])\n",
    "      print(\"pred[0][0]:\", pred[0][0])\n",
    "      print(\"pred[0][1]:\", pred[0][1])\n",
    "      print(\"ans.data[0]:\", ans.data[0])\n",
    "      print(\"ans.shape:\", ans.shape)\n",
    "      print(\"txt.shape:\", txt.shape)\n",
    "      print(\"label.shape:\", label.shape)\n",
    "      print(\"label.data[0]:\", label.data[0])\n",
    "      \n",
    "      print()\n",
    "      print_tensor_shape -= 1\n",
    "      print_idx += 1\n",
    "      #---------------------------------------\n",
    "\n",
    "    if ans.data[0] == label.data[0]:  # ans.data[0]: tensor(0) 또는 tensor(1)\n",
    "        correct += 1    \n",
    "    else:\n",
    "        incorrect += 1\n",
    "    \n",
    "print ('correct : ', correct)\n",
    "print ('incorrect : ', incorrect)\n",
    "print(classification_report(torch.tensor(y_test), \n",
    "                            torch.tensor(prediction), \n",
    "                            digits=4, \n",
    "                            target_names=['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b004148-0bdf-40c8-bda7-59a467b53854",
   "metadata": {},
   "source": [
    "### Intent_analysis_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bdf9f-032c-4503-b3bc-9024db268821",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "%cd Mecab-ko-for-Google-Colab\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh \n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable \n",
    "import pickle \n",
    "\n",
    "import MeCab\n",
    "tagger = MeCab.Tagger()\n",
    "#local에서 실행할 경우 참고사항 : 사전 디렉토리를 인자로 전달할 수 있음\n",
    "#tagger = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "df=pd.read_csv('/content/gdrive/My Drive/aivle/data/college/college_FAQ.csv',encoding='utf-8', on_bad_lines='skip')\n",
    "df.tail()\n",
    "\n",
    "df.category.value_counts()\n",
    "\n",
    "subset_category=['등록','휴복학','수강신청','장학금','생활관']\n",
    "\n",
    "df=df[df.category.isin(subset_category)]\n",
    "# 학습데이터 원문 확인\n",
    "print (df.head())\n",
    "print()\n",
    "print (df.tail())\n",
    "print()\n",
    "\n",
    "def mecabsplit(mecab_tagger,inputs, pos):\n",
    "    r=[]\n",
    "    inputs = mecab_tagger.parse(inputs)\n",
    "    t = inputs.split('\\n')[:-2]\n",
    "    for i in t:\n",
    "        field = i.split('\\t')\n",
    "        if field[1].split(',')[-1].strip() is not '*':\n",
    "            r.extend( [ (x.split('/')[0],x.split('/')[1]) for x in field[1].split(',')[-1].split('+') ] )\n",
    "        else:\n",
    "            r.append( (field[0],field[1].split(',')[0]) )\n",
    "    if pos:\n",
    "        return r\n",
    "    else:\n",
    "        return [ x[0] for x in r ]\n",
    "    return r\n",
    "\n",
    "content_vocab = {'unk':0}\n",
    "intent_vocab={}\n",
    "intent_list=[]\n",
    "\n",
    "data_intent=''\n",
    "intent_idx=0\n",
    "vocab_idx=1\n",
    "\n",
    "for line in df.category.unique():\n",
    "    intent=line\n",
    "    if intent not in intent_vocab:\n",
    "        intent_vocab[intent]=intent_idx\n",
    "        intent_list.append(intent)\n",
    "        intent_idx +=1\n",
    "\n",
    "for line in list(df.question):\n",
    "    line = mecabsplit(tagger,line,False)\n",
    "    for it in line:\n",
    "        if it not in content_vocab:\n",
    "            content_vocab[it] = vocab_idx # it : 단어\n",
    "            vocab_idx +=1\n",
    "            \n",
    "print(\"vocab size:\", len(content_vocab))\n",
    "print(content_vocab)  # 단어 사전\n",
    "print(intent_vocab)   # intent 사전\n",
    "print(\"intent size:\", len(intent_list))\n",
    "print(intent_list)    # intent_list : FAQ 카테고리 리스트\n",
    "\n",
    "@ 모델링\n",
    "class CNN_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_num, class_num, ):\n",
    "        super(CNN_Text, self).__init__()\n",
    "\n",
    "        #--- <연습2> 아래 빈 코드를 채우세요\n",
    "        V = embed_num\n",
    "        D = 100            # 단어임베딩 차원 \n",
    "        C = class_num   # 분류할 카테고리 갯수\n",
    "        Ci = 1          # 입력 채널의 수\n",
    "        Co = 20           # 출력 채널의 수\n",
    "        Ks = [1,2,3]           # 커널(필터) 사이즈 정의\n",
    "        #--- end\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)  # 단어 벡터를 임의의 값으로 초기화\n",
    "\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)  # dropout 정의\n",
    "        \n",
    "        #--- <연습3> Fully-connected layer를 정의하세요\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)                     \n",
    "        #--- end\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : 학습데이터 파일의 한 문장\n",
    "        # 입력문장이 [대학, 원, 수료, 생, 논문, 제출] => \n",
    "        # x: [1, 2, 3, 4, 8, 9]의 입력이 생성됨 (content_vocab 출력결과 참고)\n",
    "        \n",
    "        # (N, W) Integer => (W) => view(1, -1) => (1, W)\n",
    "        x = self.embed(x)         # (N, W, D) (1문장, 최대 입력 단어 개수, D차원)\n",
    "        # 문장 x의 단어 벡터값 가져옴\n",
    "        #print (x.size())\n",
    "\n",
    "        #--- <연습4> unsqueeze 함수를 호출하세요\n",
    "        x = x.unsqueeze(1)      # (N, Ci, W, D) -> Ci를 추가하여 unsqueeze (Text는 채널 1개, 이미지는 채널 3개)\n",
    "        #--- end\n",
    "        \n",
    "        #print (x.size())\n",
    "        \n",
    "        # 컨볼루션 부분 : 입력문 별로 호출되어 커널을 적용하면서 컨볼루션 수행\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks)\n",
    "        \n",
    "        #print(len(x))\n",
    "        #print(x[0].size())\n",
    "        #print(x[1].size())\n",
    "        #print(x[2].size())\n",
    "        \n",
    "        # max pooling 부분. max pooling을 수행한 결과 벡터의 차원은? (=출력채널의 갯수)\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "                \n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        x = self.dropout(x)  # (N, len(Ks)*Co)\n",
    "        \n",
    "        # 결과값(logit)은 실수값 2개를 출력하며 이 중 큰값을 가진 쪽이 예측값\n",
    "        logit = self.fc1(x)  # (N, C) \n",
    "\n",
    "        #print(logit)\n",
    "        return logit\n",
    "\n",
    "    \n",
    "cnn = CNN_Text(vocab_idx,intent_idx)\n",
    "print(vocab_idx, intent_idx) # 파일에 나타난 단어수, 의도 갯수(클래스)\n",
    "optimizer = torch.optim.Adam(cnn.parameters())\n",
    "cnn.train()    # train mode로 준비\n",
    "\n",
    "@ Training\n",
    "epoch = 10\n",
    "for e in range(epoch):\n",
    "    totalloss = 0\n",
    "    \n",
    "    for line in df.values:\n",
    "        target=Variable(torch.LongTensor([intent_vocab[line[1]]])) # 딕셔너리 타입\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cont = []\n",
    "        # 입력 문장을 형태소 단위의 토큰으로 분리\n",
    "        line = mecabsplit(tagger,line[2],False)\n",
    "        \n",
    "        for it in line:\n",
    "            # 형태소 토큰을 content_vocab을 기준으로 id 값으로 변환하여 입력 생성\n",
    "            cont.append(content_vocab[it])\n",
    "        \n",
    "        # list 변수를 torch에서 사용가능한 data_type으로 변환\n",
    "        # view 함수는 입력 텐서를 reshape하는 함수\n",
    "        # [입력 토큰 개수] => [1, 입력 토큰 개수]\n",
    "        # \n",
    "        cont = Variable(torch.LongTensor(cont)).view(1,-1)\n",
    "        pred = cnn(cont)\n",
    "        \n",
    "        #--- <연습5> 정답값과 예측값의 오차를 계산하는 손실함수 cross_entropy를 호출하세요\n",
    "        loss = F.cross_entropy(pred,target)\n",
    "        #--- end\n",
    "\n",
    "        totalloss += float(loss.data)\n",
    "        loss.backward()  # 필터, 임베딩벡터 값, FC 레이어의 matrix 등 모든 값을 loss를 줄이는 방향으로 계산\n",
    "\n",
    "        \n",
    "        #--- <연습6> 파라미터값을 변경하는 함수를 호출하세요\n",
    "        # write code here\n",
    "        optimizer.step()\n",
    "        #--- end\n",
    "    \n",
    "    print(e,'epoch')    \n",
    "    print('loss : {:.3f}'.format(totalloss))\n",
    "    #print (totalloss)\n",
    "\n",
    "    \n",
    "@ Response\n",
    "response = df.answer\n",
    "with open('/content/gdrive/My Drive/aivle/data/college/college_FAQ_answer.txt', 'r', encoding='utf-8') as response_file:\n",
    "    response = [row.split('\\t')[1] for row in response_file.readlines()[1:]]\n",
    "    \n",
    "@ Test\n",
    "test=[]\n",
    "# intent_list는 FAQ 카테고리 리스트:\n",
    "# ['등록', '휴복학', '수강신청', '장학금', '생활관']\n",
    "for i in intent_list:\n",
    "    subset_df=df.query('category==\"{}\"'.format(i))\n",
    "    test.append(subset_df.question.values[0])\n",
    "    print(subset_df.question.values[0])\n",
    "    \n",
    "test\n",
    "\n",
    "# your test : input questions\n",
    "#test=['2학기 수강신청 기간 언제인가요?']\n",
    "#test=['장학금 받으려면 몇 학점 이상이어야 되나요?']\n",
    "test = ['재수강 들어가려면 어떻게?']\n",
    "\n",
    "for idx in list(test):\n",
    "    line = mecabsplit(tagger,idx, False)\n",
    "   \n",
    "    cont = []\n",
    "    for it in line:\n",
    "        if it in content_vocab:\n",
    "            cont.append(content_vocab[it])  # cont에는 입력문에 나타난 단어들의 index 저장됨\n",
    "        else:\n",
    "            cont.append(content_vocab['unk'])\n",
    "    \n",
    "    cont = Variable(torch.LongTensor(cont)).view(1,-1)\n",
    "    pred = cnn(cont)\n",
    "    #pred : (1, C)\n",
    "    v,i = torch.max(pred,1) # i: 가장 큰 값이 들어있는 위치 인덱스\n",
    "    \n",
    "    print('input : ', line)\n",
    "    print('intent : ', intent_list[int(i)])\n",
    "    #print(int(i))\n",
    "    print('answer : ', response[int(i)],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ccbc08-5522-403b-a0a8-bad097d4c070",
   "metadata": {},
   "source": [
    "### bert_for_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29209bd-c90b-4e8c-9f71-c2861a3e24fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "@ Import modules\n",
    "import pickle as pc\n",
    "import os\n",
    "import numpy as np\n",
    "# csv 모듈은 CSV 형식(쉼표로 구분된 표 형식) 데이터를 읽고 쓰는 클래스를 제공\n",
    "import csv\n",
    "import torch\n",
    "# torch 버전 확인\n",
    "print(\"Pytorch Version: \", torch.__version__)\n",
    "# GPU 사용 가능한지 여부 확인\n",
    "if torch.cuda.is_available():\n",
    "    # PyTorch 에게 GPU 사용할거라고 알려주기\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "@ Installing the Hugging Face Library\n",
    "# transformers 팩키지 설치\n",
    "!pip install transformers\n",
    "\n",
    "@ Configure the experiments\n",
    "train_filename = '/content/gdrive/My Drive/aivle/data/amazon/bert_train_data_all.csv'\n",
    "test_data = '/content/gdrive/My Drive/aivle/data/amazon/bert_balanced_data'\n",
    "test_label = '/content/gdrive/My Drive/aivle/data/amazon/bert_balanced_label'\n",
    "\n",
    "@ Load Amazon Review Dataset\n",
    "def load_data(filename):\n",
    "    data = list()\n",
    "    label = list()\n",
    "    \n",
    "    f = open(filename, 'r', encoding='utf-8')\n",
    "    reader = csv.reader(f)\n",
    "    for idx, line in enumerate(reader):\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        # line[2]에 label 1(긍정), 0(부정) line[5]에 review text가 있음 \n",
    "        data.append(line[5])\n",
    "        label.append(int(line[2]))\n",
    "\n",
    "    f.close() \n",
    "   \n",
    "    # data와 label 사이즈 일치 여부 확인\n",
    "    assert len(data) == len(label)  \n",
    "    return data, label\n",
    "\n",
    "# train_data : 리뷰문장 (text), train_lable : 1(긍정) 또는 0(부정)\n",
    "train_data, train_label = load_data(train_filename)\n",
    "\n",
    "print(\"Size of train data: {}\".format(len(train_data)))\n",
    "print(\"Size of train label: {}\".format(len(train_label)))\n",
    "\n",
    "@ Tokenization & Input Formatting\n",
    "## BERT Tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# BERT tokenizer 불러오기\n",
    "# do_lower_case : True이면 모두 소문자로 변환, False이면 대소문자 구분\n",
    "print(\"Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# 하나의 sentence에 대해 BertTokenizer 적용\n",
    "# Print the original sentence.\n",
    "print(\"Original: \", train_data[0])\n",
    "print(\"Original: \", train_data[1])\n",
    "print()\n",
    "# Print the sentence split into tokens.\n",
    "print(\"Tokenized: \", tokenizer.tokenize(train_data[0]))\n",
    "print(\"Tokenized: \", tokenizer.tokenize(train_data[1]))\n",
    "print()\n",
    "# Print the sentence mapped to token ids.\n",
    "print(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_data[0])))\n",
    "print(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_data[1])))\n",
    "print()\n",
    "\n",
    "## Required Formatting\n",
    "각 문장의 처음과 끝에 special token 더하기\n",
    "각 문장을 maximum length 만큼 자르고 padding token 채워주기\n",
    "각 문장에서 padding token 과 실제 token 들 구분하기 위한 attention masking 적용\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence\n",
    "for sent in train_data:\n",
    "    # 'encode' will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the '[CLS]' token to the start.\n",
    "    #   (3) Append the '[SEP]' token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   max_length : 문장의 최대길이\n",
    "    #   encoded_sent : token IDs\n",
    "    #---------------------------------------------------------------\n",
    "    #      연습 (1)     tokenizer의 encode 함수를 호출해 주세요.  \n",
    "    #---------------------------------------------------------------                \n",
    "  \n",
    "    encoded_sent = tokenizer.encode(sent, \n",
    "                                    add_special_tokens=True,\n",
    "                                    max_length = 64)\n",
    "    \n",
    "    # Add the encoded sentence to the list\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Print train data[0]\n",
    "print(\"Original: \", train_data[0])\n",
    "print()\n",
    "print(\"Token IDs: \", input_ids[0])\n",
    "\n",
    "# Print special tokens and tokenized sentence\n",
    "print(\"\\n[CLS] token: {:}, ID: {:}\".format(tokenizer.cls_token, tokenizer.cls_token_id))\n",
    "print(\"\\n[PAD] token: {:}, ID: {:}\".format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "print(\"\\n[SEP] token: {:}, ID: {:}\".format(tokenizer.sep_token, tokenizer.sep_token_id))\n",
    "print(\"\\nTokenized: \", tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
    "\n",
    "# Padding & Truncating\n",
    "print(\"Max length: \", max([len(each) for each in input_ids]))\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "MAXLEN = 64\n",
    "# post-sequence truncation, post-sequence padding\n",
    "# padding value 0, type of output sequences long\n",
    "input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, \n",
    "                                                          maxlen=MAXLEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print(\"\\nPadding is done.\")\n",
    "\n",
    "# Attention Masks\n",
    "# Create attention masks\n",
    "attention_masks = [] # 각 문장에 대한 attention mask 리스트를 저장\n",
    "\n",
    "# 토큰 시퀀스에서 패딩에 해당하는 부분은 0, 패딩이 아닌 부분은 1을 넣은 mask를 생성\n",
    "# 패딩 부분은 모델 내에서 Attention을 수행하지 않아 학습속도를 향상\n",
    "# For every sentence\n",
    "for sent in input_ids:\n",
    "    # Create the attention mask.\n",
    "    #  - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #  - If a token ID is not 0 ( > 0), then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)\n",
    "print(\"\\nAttention masking is done.\")\n",
    "\n",
    "# Training & Validation Split\n",
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_test_split(arrays, test_size, train_size, random_state, shuffle, stratify)\n",
    "#     arrays : 분할시킬 데이터\n",
    "#     test_size : 테스트 데이터셋의 비율 (default = 0.25)\n",
    "#     random_state : 데이터 셔플 시 seed value. 호출할 때마다 동일한 학습 데이터, 테스트 데이터 셋을 생성하기 위해 설정\n",
    "#     shuffle : 셔플 여부 (default = True)\n",
    "#     stratify : 지정한 Data의 비율을 유지한다. 예를 들어, Label Set인 Y가 25%의 0과 75%의 1로 이루어진 Binary Set일 때\n",
    "#     stratify=Y로 설정하면 나누어진 데이터셋들도 0과 1을 각각 25%, 75%로 유지\n",
    "# 반환값 : (학습 데이터, 테스트 데이터, 학습데이터 label, 테스트데이터 label)\n",
    "# test_size=0.1로 설정, Use 90% for training and 10% for validation\n",
    "train_inputs, valid_inputs, train_labels, valid_labels = train_test_split(input_ids, train_label, random_state=2018, test_size=0.1)\n",
    "# Do the same for the masks.\n",
    "train_masks, valid_masks, _, _ = train_test_split(attention_masks, train_label, random_state=2018, test_size=0.1)\n",
    "# print train_inputs, valid_inputs\n",
    "# 첫번째 학습데이터, 첫번째 attention mask를 출력해서 확인\n",
    "print(train_inputs[:1])\n",
    "print(train_masks[:1])\n",
    "\n",
    "# Converting to PyTorch Data Types\n",
    "# Convert all inputs and labels into torch tensors, the required data type for our model.\n",
    "# torch.tensor(data) -> tensor를 생성\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "valid_inputs = torch.tensor(valid_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "valid_labels = torch.tensor(valid_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "valid_masks = torch.tensor(valid_masks)\n",
    "\n",
    "# PyTorch 의 DataLoader class 를 이용하여 amazon review dataset 에 대한 iterator 를 생성합니다.\n",
    "# torch.utils.data : 파이토치의 데이터 로딩 유틸리티. 데이터셋, 데이터로더, 샘플러 등을 제공함\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# The DataLoader needs to know our batch size for training, so we specify it here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "#---------------------------------------------------------------\n",
    "#      연습 (2) train_data, valid_data의 데이터 로더를 생성해 주세요.\n",
    "#---------------------------------------------------------------   \n",
    "# Create the DataLoader for our training set.\n",
    "# torch.utils.data.TensorDataset(*tensors)\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "# Create the DataLoader for our validation set.\n",
    "valid_data = TensorDataset(valid_inputs, valid_masks, valid_labels)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)\n",
    "\n",
    "@ Train our classification model\n",
    "## BertForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassfication, the pretrained BERT model \n",
    "# with a single linear classification layer on top.\n",
    "# BERT 모델의 네트워크 형태를 출력\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab\n",
    "                                                      num_labels = 2, # The number of output labels 2 for binary classification\n",
    "                                                                      # You can increase this for multi-class tasks\n",
    "                                                      output_attentions = False, # whether the model returns attentions weight (correponding to multi-head self attentions)\n",
    "                                                      output_hidden_states = False) # whether the model returns all hidden states\n",
    "\n",
    "# Tell PyTorch to run this model on the GPU\n",
    "# (model의 모든 parameter를 GPU에 loading)\n",
    "model.cuda()\n",
    "\n",
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "print(\"The BERT model has {:} different named parameters.\\n\".format(len(params)))\n",
    "print(\"=== Embedding Layer ===\\n\")\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print(\"\\n==== First Transformer ====\\n\")\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print(\"\\n==== Output Layer====\\n\")\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "    \n",
    "## Optimizer & Learning Rate Scheduler\n",
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
    "# I believe the 'W' stands for \"Weight Decay fix\"\n",
    "# Weight Decay: weight들의 값이 증가하는 것을 제한함으로써 모델의 복잡도를 감소시켜 오버피팅을 방지하는 기법 \n",
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "# Number of training epochs (we recommend between 2 and 4)\n",
    "epochs = 4\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "print(\"number of batches:\", len(train_dataloader))\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "print(\"total_steps:\", total_steps)\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "## Training Loop\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "# numpy argmax : 해당 차원(axis)의 값 중에서 가장 큰 값의 인덱스를 반환\n",
    "# flatten() : 다차원 배열을 1차원으로 변환\n",
    "# sum() : 배열 내 전체 값들의 합\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "import time\n",
    "import datetime\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Take a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "import random\n",
    "# Set the seed value all over the place to make this reproducible\n",
    "def set_seed(seed_val):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "seed_val = 42\n",
    "set_seed(seed_val)\n",
    "# Store the average loss after each epoch so we can plot them\n",
    "loss_values = []\n",
    "# For each epoch\n",
    "for epoch in range(0, epochs):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0.\n",
    "    # Put the model into training mode.\n",
    "    # Don't be mislead -- the call to 'train' just changes the \"mode\", it doesn't \"perform\" the training.\n",
    "    # 'dropout' and 'bachnorm' layers behave differently during training vs test\n",
    "    model.train()\n",
    "    # For each batch of training data\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress\n",
    "            print(\"Batch {:>5,} of {:>5,}. Elapsed: {:}.\".format(step, len(train_dataloader), elapsed))\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        #---------------------------------------------------------------\n",
    "        #      연습 (3) 아래 라인에 그래디언트를 0으로 초기화하는 함수를 호출해 주세요.\n",
    "        #--------------------------------------------------------------- \n",
    "        model.zero_grad()\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value from the tensor.\n",
    "        total_loss += loss.item()\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.        \n",
    "        #---------------------------------------------------------------\n",
    "        #      연습 (4) 아래 라인에 파라미터를 업데이트하는 함수를 호출해 주세요.\n",
    "        #--------------------------------------------------------------- \n",
    "        optimizer.step() # 최적화, 업데이트 \n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on our validation set.\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode -- the dropout layers behave differently during evaluation\n",
    "    model.eval()\n",
    "    # Tracking variables\n",
    "    eval_loss, eval_acc = 0., 0.\n",
    "    # Evaluate data for one epoch\n",
    "    for valid_step, batch in enumerate(valid_dataloader):\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            #---------------------------------------------------------------\n",
    "            #      연습 (5) 아래 함수에서 파라미터를 채워주세요\n",
    "            #--------------------------------------------------------------- \n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        # Get the \"logits\" output by the model.\n",
    "        # The \"logits\" are the output values prior to applying an activation function like the softmax\n",
    "        # output 타입: class transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # (https://huggingface.co/docs/transformers/v4.22.2/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput)\n",
    "        # our model will return (outputs.loss(optional)=None, outputs.logits)\n",
    "        logits = outputs[0]\n",
    "        # Move logits and labels to CPU (Module을 통해 나온 tensor을 후처리에 사용하거나, 계산된 loss를 로깅 등)\n",
    "        # detach() : 파이토치는 tensor에서 이루어진 모든 연산을 추적해서 graph에 기록해두는데 이 연산 기록으로부터 \n",
    "        # 그래디언트가 계산되고 역전파가 이루어지게 된다. detach()는 이 연산 기록으로부터 분리한 tensor을 반환하는 method     \n",
    "        # cpu() : GPU 메모리에 올려져 있는 tensor를 cpu 메모리로 복사하는 method   \n",
    "        # numpy() : tensor를 numpy로 변환하여 반환\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_acc = flat_accuracy(logits, label_ids)\n",
    "        # Accumulate the total accuracy.\n",
    "        eval_acc += tmp_eval_acc        \n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"Accuracy: {0:.2f}\".format(eval_acc / (valid_step + 1)))\n",
    "    print(\"Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "print(\"\")\n",
    "print(\"Training complete!\")        \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o')\n",
    "# Label the plot.\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "@ Performance on Test set\n",
    "## Load Amazon Review Test Dataset\n",
    "# pickle 모듈로 테스트 파일을 load : bert_balanced_data, bert_balanced_label\n",
    "with open(test_data, 'rb') as f:\n",
    "    test_data = pc.load(f)\n",
    "with open(test_label, 'rb') as f:\n",
    "    test_label = pc.load(f)\n",
    "print(\"Size of test data: {}\".format(len(test_data)))\n",
    "print(\"Size of test label: {}\".format(len(test_label)))\n",
    "\n",
    "## Tokenization & Input Formatting\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "# For every sentence...\n",
    "for sent in test_data:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    input_ids.append(encoded_sent)\n",
    "# Pad our input tokens\n",
    "input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=MAXLEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks array\n",
    "attention_masks = []\n",
    "# For every sentence\n",
    "for sent in input_ids:\n",
    "    # Create the attention mask.\n",
    "    #  - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #  - If a token ID is not 0 ( > 0), then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)\n",
    "# Convert to tensors\n",
    "# input_ids : 테스트 문장에 포함된 단어 ids\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(test_label)\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "prediction_sampler = SequentialSampler(prediction_data) \n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "## Evaluate on Test Set\n",
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "# Tracking variables \n",
    "predictions, true_labels = [], []\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader (batch에서 데이터 추출)\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "  logits = outputs[0]\n",
    "  # Move logits and labels to CPU\n",
    "  # logist, labels 텐서를 CPU로 이동하여 정확도 계산\n",
    "  # detach() : 텐서에서 이루어진 연산 기록으로부터 분리한 텐서를 반환\n",
    "  # cpu() : GPU 메모리에 올려져 있는 tensor를 cpu 메모리로 복사\n",
    "  # numpy() : tensor를 numpy로 변환하여 반환\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  # Store predictions and true labels\n",
    "  # predictions : 예측값 array\n",
    "  # true_labels : 정답값 array\n",
    "  # logits array의 값들 중 가장 큰 값의 index를 반환\n",
    "  # when axis = 1, argmax identifies the maximum value for every row. \n",
    "  predictions.extend(np.argmax(logits, axis=1).flatten()) # [0, 1, 1, 0, ...]\n",
    "  true_labels.extend(label_ids.flatten()) # [0, 1, 0, 0, ...]\n",
    "print('DONE.')\n",
    "\n",
    "# accuracy, precision, recall, f1 score 성능 확인\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['negative', 'positive']\n",
    "# sklearn.metrics.classification_report(y_true, y_pred, digits, target_names)\n",
    "# support is the number of actual occurrences of the class in the specified dataset.\n",
    "print(classification_report(true_labels, predictions, digits=4, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51e2c4-20c1-4aed-a3ef-171619336eff",
   "metadata": {},
   "source": [
    "## 스팸분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192d122-43ec-41a1-8c27-b73b08fe1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4ae33-39ee-4707-abd6-958d2a8742f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import rich  # 출력을 예쁘게 꾸며주는 라이브러리\n",
    "from rich.table import Table\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "train_df = pd.read_csv('spam.csv')\n",
    "test_df = pd.read_csv('spam_test_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4b157-217c-4551-ae30-98a6ff024c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "train_df.isna().sum()\n",
    "train_df.reset_index(inplace=True)\n",
    "train_df.drop(columns={'index'},inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce781e3c-8ff8-4a2e-8966-53055c4f62a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(inplace=True)\n",
    "train_df['label'] = train_df['label'].replace(['ham','spam'],[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5fa70-fd5d-44b1-868b-0e623f1c5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipe(model, model_name: str) -> Pipeline:\n",
    "    \"TfidfVectorizer와 모델을 연결한 파이프라인을 반환하는 함수\"\n",
    "    tfidf = TfidfVectorizer(analyzer=\"char\", ngram_range=(1, 3))\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", tfidf),\n",
    "        (model_name, model)\n",
    "    ])\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63095e3-c415-48ac-bdf8-2eb68c824e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_kfold_accuarcy(model, k: int = 5) -> float:\n",
    "    \"모델을 입력받아 KFold 예측 후 accuracy score를 반환하는 함수\"\n",
    "    kfold = StratifiedKFold(k, shuffle=True, random_state=42)\n",
    "    result = []\n",
    "    for train_idx, test_idx in kfold.split(train_df[\"text\"], train_df[\"label\"]):\n",
    "        train, val = train_df.iloc[train_idx], train_df.iloc[test_idx]\n",
    "        model.fit(train[\"text\"], train[\"label\"])\n",
    "        pred = model.predict(val[\"text\"])\n",
    "        acc = accuracy_score(val[\"label\"], pred)\n",
    "        result.append(acc)\n",
    "\n",
    "    return np.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371355b3-4854-4681-b994-205b15d5e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"naive_bayes\", BernoulliNB()),\n",
    "    (\"SGD\", SGDClassifier(random_state=42, n_jobs=-1)),\n",
    "]\n",
    "\n",
    "model_pipes = [(name, get_pipe(model, name)) for name, model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783a499-fa01-49c5-8a24-3d5b414fe601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [\n",
    "#     (\"naive_bayes\", BernoulliNB()),\n",
    "#     (\"SGD\", SGDClassifier(random_state=42, n_jobs=-1)),\n",
    "#     (\"rfc\", RandomForestClassifier(random_state=42, n_jobs=-1)),\n",
    "#     (\"SVC\", SVC(random_state=42)),\n",
    "#     (\"ada\", AdaBoostClassifier(random_state=42)),\n",
    "#     (\"lgbm\", LGBMClassifier(random_state=42)),\n",
    "#     (\"lgbm2\", LGBMClassifier(n_estimators=80, random_state=42)),\n",
    "#     (\"xgb\", XGBClassifier(random_state=42)),\n",
    "#     (\"knc1\", KNeighborsClassifier()),\n",
    "#     (\"knc2\", KNeighborsClassifier(n_neighbors=4))\n",
    "# ]\n",
    "\n",
    "# model_pipes = [(name, get_pipe(model, name)) for name, model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddcf9e5-8bbf-4966-9d83-06759b9a88ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = Table(title=\"Model Comparison Table\")\n",
    "table.add_column(\"Model Name\", justify=\"left\", style=\"green\")\n",
    "table.add_column(\"Accuracy\", justify=\"right\")\n",
    "\n",
    "for model_name, model in tqdm(model_pipes, leave=False):\n",
    "    acc = return_kfold_accuarcy(model)\n",
    "    table.add_row(model_name, f\"{acc:0.3f}\")\n",
    "\n",
    "rich.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4c0fd-b292-4607-b5e1-c7f8a8677c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stack_models = [(name, get_pipe(model, name)) for name, model in models]\n",
    "\n",
    "stacking = StackingClassifier(stack_models)\n",
    "acc = return_kfold_accuarcy(stacking)\n",
    "rich.print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d306dac-36d7-41dc-a10e-e502320ba1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking.fit(train_df['text'], train_df['label'])\n",
    "submission_pred = stacking.predict(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ff0a4-8013-4652-bb09-b141bf56837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('spam_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921e0ae-ce2c-4353-8a11-fdb8788a2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label'] = submission_pred\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093783f-5cc7-4f11-937d-6f257f8b1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label']= submission['label'].replace([0,1],['ham','spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3983453-4e65-40f8-a8be-dbad1c3e8853",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"/aihub/data/M1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae25809f-a793-4a30-82cd-62e193c676eb",
   "metadata": {},
   "source": [
    "# 수어번역[CNN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09888fb4-dfc4-4eab-ac70-cf8c8fc44305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 라이브러리\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random as rd\n",
    "import cv2, os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.backend import clear_session\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd97a0-cbed-4875-bb6f-7a8638ee96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습곡선 함수\n",
    "def dl_history_plot(history):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history['loss'], label='train_err')\n",
    "    plt.plot(history['val_loss'], label='val_err')\n",
    "\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d0eb0-616b-4bcd-8882-42b266c2820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460843a8-69eb-4210-8c5d-82171e6f31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '내경로'\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "data.head()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb7a7f-d974-421b-9ca9-ffbf4618ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class name 찍어보기\n",
    "import string\n",
    "class_names = list(string.ascii_lowercase)\n",
    "len(class_names), class_names\n",
    "\n",
    "\n",
    "# 데이터 살펴보기\n",
    "# 아래 숫자를 바꿔가며 화면에 그려 봅시다.\n",
    "n = 10\n",
    "sign_fig = data.iloc[n, 1:].values\n",
    "sign_fig = sign_fig.reshape(28, 28)\n",
    "\n",
    "sign = class_names[data.iloc[n,0]]\n",
    "\n",
    "plt.title(sign)\n",
    "plt.imshow(255-sign_fig, cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b740ce-73d8-42f1-bb45-caaa2ff0109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "target = 'label'\n",
    "x = data.drop(target, axis = 1)\n",
    "y = data.loc[:, target]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, train_size = 20000, random_state = 2022)\n",
    "\n",
    "x_train.shape, x_val.shape\n",
    "\n",
    "# 모두 넘파이로 변환\n",
    "x_train2, x_val2, y_train2, y_val2 = x_train.values, x_val.values, y_train.values, y_val.values\n",
    "\n",
    "\n",
    "## CNN을 위해 shape 맞추기 n, 28,28,1\n",
    "x_train2 = x_train2.reshape(20000,28,28,1)\n",
    "x_val2 = x_val2.reshape(7455,28,28,1)\n",
    "\n",
    "x_train2.shape, x_val2.shape\n",
    "\n",
    "# min_max scaling\n",
    "x_train2 = x_train2 / 255.\n",
    "x_val2 = x_val2 / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87cd37a-6430-41c0-9c43-37aa6be47009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델링(요즘 뜨는거-> CatBoost)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "m1 = RandomForestClassifier()\n",
    "m1.fit(x_train, y_train)\n",
    "p1 = m1.predict(x_val)\n",
    "\n",
    "cn = np.array(class_names)\n",
    "\n",
    "print(accuracy_score(y_val,p1))\n",
    "print('-'*60)\n",
    "print(confusion_matrix(y_val, p1))\n",
    "print('-'*60)\n",
    "print(classification_report(cn[y_val], cn[p1]))\n",
    "\n",
    "## CNN\n",
    "clear_session()\n",
    "\n",
    "m2 = Sequential([Conv2D(32, kernel_size=3, input_shape=(28, 28, 1), padding='same', strides =1, activation='relu'),\n",
    "                    MaxPooling2D(pool_size=2, strides=2),\n",
    "                    Flatten(),\n",
    "                    Dense(128, activation = 'relu'),\n",
    "                    Dense(25, activation='softmax')\n",
    "])\n",
    "\n",
    "m2.summary()\n",
    "\n",
    "m2.compile(optimizer = 'adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "history = m2.fit(x_train2, y_train2, epochs = 10, validation_split=0.2).history\n",
    "\n",
    "dl_history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a961e-9e8d-417c-bcc1-305a9bcbcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 & 검증\n",
    "p2 = m2.predict(x_val2)\n",
    "\n",
    "p2_1 = p2.argmax(axis=1)\n",
    "\n",
    "cn = np.array(class_names)\n",
    "\n",
    "print(accuracy_score(y_val,p2_1))\n",
    "print('-'*60)\n",
    "print(confusion_matrix(y_val, p2_1))\n",
    "print('-'*60)\n",
    "print(classification_report(cn[y_val], cn[p2_1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704b7a36-6b13-4082-8aeb-de0e77c583d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장하기(3가지 방식)keras, joblib, pickle\n",
    "import joblib\n",
    "joblib.dump(m1, 'sign_model_rf.pkl')\n",
    "m2.save('sign_model.h5')\n",
    "\n",
    "# 모델 로딩\n",
    "m1_2 = joblib.load('sign_model_rf.pkl')\n",
    "from keras.models import load_model\n",
    "m2_1 = load_model('sign_model.h5')\n",
    "\n",
    "# 모델 사용\n",
    "p1_2 = m1_2.predict(x_val) # 랜덤포레스트 모델\n",
    "print(accuracy_score(y_val,p1_2))\n",
    "print('-'*60)\n",
    "print(confusion_matrix(y_val, p1_2))\n",
    "print('-'*60)\n",
    "print(classification_report(cn[y_val], cn[p1_2]))\n",
    "\n",
    "p2_1 = m2_1.predict(x_val2) # CNN모델 \n",
    "p2_2 = p2_1.argmax(axis=1)\n",
    "print(accuracy_score(y_val,p2_2))\n",
    "print('-'*60)\n",
    "print(confusion_matrix(y_val, p2_2))\n",
    "print('-'*60)\n",
    "print(classification_report(cn[y_val], cn[p2_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce92c29-b8a8-4640-9c15-45bbbf15969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프 라인 (Data Pipeline 구성) 함수로 만들기\n",
    "\n",
    "# 파이프라인에서 필요한 라이브러리/함수\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "\n",
    "def sign_pipeline(file) :\n",
    "\n",
    "    # class names 준비\n",
    "    class_names = list(string.ascii_lowercase)\n",
    "    class_names = np.array(class_names)\n",
    "\n",
    "    # 흑백으로 읽기\n",
    "    img = cv2.imread(file , cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # 크기 조정\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "\n",
    "    # input shape 맞추기\n",
    "    test_sign = img.reshape(1,28,28,1)\n",
    "\n",
    "    # 스케일링\n",
    "    test_sign = test_sign / 255.\n",
    "\n",
    "    # 모델 로딩\n",
    "    model = load_model('sign_model.h5')\n",
    "\n",
    "    # 예측\n",
    "    pred = model.predict(test_sign)\n",
    "    pred_1 = pred.argmax(axis=1)\n",
    "\n",
    "    return class_names[pred_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f800b-7a92-4486-873c-b20566a0f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/content/drive/MyDrive/dataset/test image/v.png'\n",
    "sign_pipeline(file) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
